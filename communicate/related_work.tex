
Accelerating vector operations through compression has been the subject of a great deal of research in the computer vision, information retrieval, and machine learning communities, among others. Our review will necessarily be incomplete, so we refer the reader to \cite{learningToHashSurvey, hashingSimilaritySurvey} for detailed surveys. %  and focus on methods not discussed in Section~\ref{sec:method}

Many existing approaches in the computer vision and information retrieval literature fall into one of two categories \cite{hashingSimilaritySurvey}: binary embedding and vector quantization. Binary embedding techniques seek to map vectors in $\mathbb{R}^J$ to $B$-dimensional Hamming space, typically with $B < J$. The appeal of binary embedding is that a $B$-element vector in Hamming space can be stored in $B$ bits, affording excellent compression. Moreover, the \texttt{popcount} instruction present on virtually all desktop, smart phone, and server processors can be used to compute Hamming distances between 8 byte vectors in as little as three cycles. This fast distance computation comes at the price of reduced representational accuracy for a given code length \cite{opq,hashingSimilaritySurvey} . He et al. \cite{opq} demonstrated that the popular binary embedding technique of \cite{ITQ} is a more constrained version of their vector quantization algorithm, and that the objective function of another state-of-the art binary embedding \cite{isohash}, can be understood as maximizing only one of two sufficient conditions for optimal encoding of Gaussian data.

Vector quantization approaches yield lower errors than binary embedding for a given code length, but entail slower encoding and distance computations. The simplest and most popular vector quantization method is K-means, which can be seen as encoding a vector as the centroid to which it is closest. A generalization of K-means, Product Quantization (PQ) \cite{pq}, splits the vector into $M$ disjoint subvectors and runs K-means on each. The resulting code is the concatenation of the codes for each subspace. Numerous generalizations of PQ have been published, including Cartesian K-means \cite{cartesianKmeans}, Optimized Product Quantization \cite{opq},  Generalized Residual Vector Quantization \cite{grvq}, Additive Quantization \cite{aq}, Composite Quantization \cite{cq}, Optimized Tree Quantization \cite{otq}, Stacked Quantizers \cite{stackedQuantizers}, and Local Search Quantization \cite{lsq}. The idea behind most of these generalizations is to either rotate the vectors or relax the constraint that the subvectors be disjoint. Collectively, these techniques that rely on using the concatenation of multiple codes to describe a vector are known as Multi-Codebook Quantization (MCQ) methods. % , Residual Vector Quantization [],

An interesting hybrid between binary embedding and vector quantization is the recent Polysemous Coding of Douze et al. \cite{polysemous}. This encoding uses product quantization codebooks optimized to also function as binary codes, allowing the use of Hamming distances as a fast approximation that can be refined for promising nearest neighbor candidates. % Like our own algorithm, it seeks the best of both binary embedding speed and vector quantization accuracy. Our technique obtains this by speeding up the vector quantization distance computations directly, obviating the need for multiple passes and the possibility of false dismissals from unrepresentative Hamming distances.

The most similar vector quantization-related algorithm to our own is that of \cite{simdpq}, which also vectorizes PQ distance computations. However, their method requires hundreds of thousands or millions of encodings to be sorted lexicographically and stored contiguously ahead of time, as well as scanned through serially. This is untenable when the data is rapidly changing or when using an indexing structure, which would split the data into far smaller partitions. Their approach also requires a second refinement pass of non-vectorized PQ distance computations, making their reported speedups significantly lower than our own.

In the machine learning community, accelerating vector operations has been done primarily through (non-binary) embedding, structured matrices, and model compression. Embedding yields acceleration by reducing the dimensionality of data while preserving the relevant structure of a dataset overall. There are strong theoretical gaurantees regarding the level of reduction attainable for a given level of distortion in pairwise distances \cite{jl, fastJL, jlIsTight}, as well as strong empirical results \cite{superBitLSH, compressiveMining}. However, because embedding \textit{per se} only entails reducing the number of floating-point numbers stored, without reducing the size of each, it is not usually competitive with vector quantization methods. It is possible to embed data before applying vector quantization, so the two techniques are complementary.

An alternative to embedding that reduces the cost of storing and multiplying by matrices is the use of structured matrices. This consists of repeatedly applying a linear transform, such as permutation \cite{adaptiveFastfood}, the Fast Fourier Transform \cite{orthogonalRandomFeatures}, the Discrete Cosine Transform \cite{acdc}, or the Fast Hadamard Transform \cite{crossPolytope, structuredSpinners}, possibly with learned elementwise weights, instead of performing a matrix multiply. These methods have strong theoretical grounding \cite{structuredSpinners} and sometimes outperform non-structured matrices [adaptiveFastfood]. They are orthogonal to our work in that they bypass the need for a matrix entirely, while our approach can accelerate operations in which a matrix is needed.

Another vector-quantization-like technique common in machine learning is model compression. This typically consists of some combination of 1) restricting the representation of variables, such as neural network weights, to fewer bits \cite{lossAwareDnnBinarization}; 2) reusing weights \cite{hashnet}; 3) pruning weights in a model after training \cite{diversityNets, learningWeightsConnections}; and 4) training a small model to approximate the outputs of a larger model \cite{rnnDarkKnowledge}. This has been a subject of intense research for neural networks in recent years, so we do not believe that our approach could yield smaller neural networks than the current state of the art. Instead, our focus is on accelerating operations on weights and data that would otherwise not have been compressed.% matching the form of Eq~\ref{eq:distFuncForm}.

% In addition to all of the above approaches, there is the possibility of running algorithms on GPUs [], clusters [], ASICs [], or FPGAs []. Since such alternate hardware could accelerate both our own algorithm and the most similar comparisons, we do not address this possibility further.

% % In addition to these methods, there has been a great deal of work on compressing data


% % Our algorithm is similar to above multi-codebook quantization methods,

% In addition to these methods, there has been a great deal

% Vector quantization is an extremely well-studied problem, so our review will necessarily be incomplete. We refer the reader to [learningToHash, thatOtherSurvey] for detailed surveys. The classic K-means algorithm [] is by far the most widely-used vector quantization technique.

% Great deal of work on vector quantization
%     -mostly on generating better encoding algorithms
%         -typically through complex procedures that require tons of encoding time
%             -eg, state of the art (LSQ) encoding takes over a ms *per vector*; we're like a ten thousand times faster than this
%         -sometimes through direct integration with an index, such as the IMI (LOPQ does this)
%     -our scanning algorithm can use any of these encoding schemes that can spit out 4bit codes, and our preprocessing can help with basically any of them
%     -most similar to ours is that VLDB paper that used the same machine instructions; however, it required huge sets of static vectors that could be sorted ahead of time


%     -also lots of work on compressing neural nets
%         -replacing raw mats with structured mats, such as adaptive fastfood transform or the DCT ones
%             -heuristic (?) weight-sharing like hashnets
%             -zelda diversity nets
%         -quantization, sparsity
%             -pruning post-hoc
%             -learning with only quantized reprs
%             -ASIC acceleration
%         -other stuff I don't know that well

%     -sketching / embedding
%         -pretty sure there's some sketch for euclidean distance
%         -sketching and embedding equivalent for norms

%         -sketching for stats such as number of uniques


