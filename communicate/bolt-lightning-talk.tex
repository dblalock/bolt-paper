
We describe a means of dramatically accelerating matrix-vector operations in exchange for a small amount of approximation error. Our approach can accelerate nearest neighbor search and maximum inner product search by over $100\times$ compared to floating point operations and up to $10\times$ compared to existing methods. Perhaps surprisingly, this speedup is sufficient to accelerate even matrix-matrix operations by reducing them to many matrix-vector operations. For example, using our technique to compute approximate dot products in a nested loop can multiply matrices faster than a state-of-the-art BLAS implementation. The core idea of our approach is to reduce multiplies to table lookups, and table lookups to vectorized operations in registers.
