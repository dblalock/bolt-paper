
% Get cost equation in here; ops are CRUD, and existing work focuses on extremely read-heavy workloads

% TODO how to frame this given that we actually have both 10x query speed, (almost) 10x encoding speed, and can do matmuls 5x faster than BLAS?

As datasets grow larger, so too do the costs of mining them. These costs include not only the space to store the data, but also the compute time to operate on it. These time costs can be decomposed into:
\begin{align}
    \texttt{Cost}_{\text{time}} = \texttt{Cost}_{\text{read}} + \texttt{Cost}_{\text{write}}
\end{align}
where $\texttt{Cost}_{\texttt{read}}$ is the time cost of reading data, and $\texttt{Cost}_{\texttt{write}}$ is the time cost of creating, updating, or deleting data.

Vector quantization methods presently enable significant reductions in $\texttt{Cost}_{\texttt{storage}}$ and $\texttt{Cost}_{\texttt{read}}$ on datasets of real-valued vectors. By replacing each vector with a learned approximation, these methods both decrease space usage and enable fast approximate scalar reductions, such as dot products and Euclidean distances. With as little as 8B per vector, these techniques can preserve distances and dot products with extremely high accuracy [][][].



 % but add overhead to Cost_{\texttt{write}}.


% When the dataset is fixed, vector quantization methods enable significant reductions in both of these costs. By replacing each vector with a learned approximation, these methods both reduce the space needed to store the vectors and enable fast approximate scalar reductions, such as dot products and Euclidean distances.

However, computing the approximation for a given vector can be time-consuming, adding greatly to $\texttt{Cost}_{\texttt{write}}$. The state-of-the-art method of [LSQ], for example, requires up to \textit{4ms} to encode a single $128$-dimensional vector. Other techniques are faster, but as we show experimentally, virtually all suffer from encoding times that yield significant overhead when data is being added or changed rapidly.

We describe a vector quantization method, Bolt, that greatly reduces both the time to encode vectors ($\texttt{Cost}_{\texttt{write}}$) and the time to compute scalar reductions over them ($\texttt{Cost}_{\texttt{read}}$). This makes quantization worthwhile even for fast-changing or fast-arriving data. Our key idea is to use much smaller quantization codebooks than similar techniques, which both facilitates finding the optimal encoding and allows scans over codes to be done in a vectorized manner.

Our contributions consist of:
\begin{enumerate}
\item A vector quantization algorithm that encodes vectors significantly faster than existing algorithms for a given level of representational fidelity.
\item A fast means of computing approximate similarities and distances using quantized vectors. Possible similarities and distances include dot products, cosine similarities, and distances in $L_p$ spaces, such as the Euclidean distance.
\item Theoretical and empirical evidence that the sufficient statistics used my dozens of recent vector quantization algorithms can be approximated with little or no loss of accuracy.
\item Theoretical analysis of both our approach and popular related approaches.
\end{enumerate}


% Many machine learning algorithms are built around some combination of matrix multiplications, dot products, and distance computations in vector spaces. Such algorithms include deep neural networks, k-means and hierarchical clustering, and linear regression, among many others.

% Lots of people have tried speeding up deep neural nets, mostly by pruning them post-hoc or adopting elementwise quantization / sparsity


% for ease of exposition, we will refer to computing ``similarities'' as shorthand for computing either dot products or euclidean distances throughout the remainder of this work, because typing ``dot products or distances'' is really verbose


% Contributions:

% A fast means of preprocessing vectors so as to minimize quantization loss. This method is compatible with most techniques in the rapidly-evolving area of vector quantization.

% A hardware-friendly means of computing approximate distances using quantized vectors.

% Theoretical analysis of our technique's effectiveness.

% Empirical demonstration that approximate distances suffice for many real-world applications.


% somewhere sneak in the fact that Bolt is an acronym for Based On Lookup Tables


\subsection{Problem Statement}

Formally, the problem our algorithm addresses is the following.

Let $\vec{q} \in \mathbb{R}^J$ be a \textit{query} vector and let $\mathcal{X} = \{\vec{x}_1,\ldots,\vec{x}_N\}, \vec{x}_i \in \mathbb{R}^J$ be a collection of \textit{database} vectors. Further let $d: \mathbb{R}^J \times \mathbb{R}^J \rightarrow \mathbb{R}$ be a distance or similarity function that can be written as:
\begin{align} \label{eq:distFuncForm}
        d(\vec{q}, \vec{x}) = f(\sum_{j=1}^J \delta(q_j, x_j))
\end{align}

% TODO this technically doesn't include how additive methods do ED;
%   wait, nvm, we're fine cuz they use the d_hat equation, not this one

where $f: \mathbb{R} \rightarrow \mathbb{R}$, $\delta: \mathbb{R}^J \times \mathbb{R}^J \rightarrow \mathbb{R}$. This includes both distances in $L_p$ spaces and dot products as special cases. In the former case, $\delta(q_j, x_j) = |q_j - x_j|^p$ and $f(r) = r^{(1/p)}$; in the latter case, $\delta(q_j, x_j) = q_j x_j$ and $f(r) = r$. For brevity, we will henceforth refer to $d$ as a distance function and its output as a distance, though our remarks apply to all functions of the above form unless noted otherwise.

Our task is to construct three functions $g: \mathbb{R}^J \rightarrow \mathcal{G}$, $h: \mathbb{R}^J \rightarrow \mathcal{H}$, and $\hat{d}: \mathcal{G} \times \mathcal{H} \rightarrow \mathbb{R}$ such that for a given approximation loss $\mathcal{L}$,
\begin{align}
    \mathcal{L} = E_{\vec{q},\vec{x}}[(d(\vec{q}, \vec{x}) - \hat{d}(g(\vec{q}), h(\vec{x})))^2]
    % d(\vec{q}, \vec{x}) \approx \hat{d}(g(\vec{q}), h(\vec{x}))
\end{align}
the computation time $T$,
\begin{align}
    T = T_g + T_h + T_d
\end{align}
is minimized, where $T_g$ is the time to encode received queries $\vec{q}$ using $g$\footnote{We cast the creation of query-specific lookup tables as encoding $\vec{q}$ rather than creating a new $\hat{d}$ (the typical interpretation in recent literature).}, $T_h$ the time to encode $\mathcal{X}$ using $h$, and $T_d$ the time to compute the approximate distances between the encoded queries and encoded database vectors. The relative contributions of each of these terms depends on how frequently $\mathcal{X}$ changes, so many of our experiments characterize each separately. % Because related work typically optimizes $T_g$ and $T_d$ at the expense of $T_h$, our differentiating assumption is that $T_h$ cannot be neglected. % This assumption is invalid when $\mathcal{X}$

% \footnote{We describe the creation of lookup tables for each query as encoding of the query, not creation of a query-specific $\hat{d}$ (the typical interpretation in recent literature).}

% . This is not to be confused with the \textit{symmetric} encoding used in some binary embedding methods.

There is a tradeoff between the value of the loss $\mathcal{L}$ and the time $T$, so multiple operating points are possible. In the extreme cases, $\mathcal{L}$ can be fixed at 0 by setting $g$ and $h$ to identity functions and setting $\hat{d} = d$. Similarly, $T$ can be set to $0$ by ignoring the data and estimating $d$ as a constant. The primary contribution of this work is therefore the introduction of $g$, $h$ and $\hat{d}$ functions that are significantly faster than those of existing work for a wide range of operating points.

% the loss:
% \begin{align}
%     \mathcal{L} = E_{\vec{q},\vec{x}}[(d(\vec{q}, \vec{x}) - \hat{d}(g(\vec{q}), h(\vec{x})))^2]
%     % \mathcal{L}[g, h, \hat{d}] = E_{q,x}[(d(q, x) - \hat{d}(g(q), h(x)))^2]
% \end{align}
% is minimized. Moreover, each of these functions should be as fast to compute as possible.

% Intuitively, the function $g$ encodes the query, $h$ encodes the database vectors, and $\hat{d}$ computes an approximate similarity or distance based on the encodings. In general, it is possible to drive the loss arbitrarily low by increasing the time and space usage of these functions. Indeed, the loss can be fixed at 0 by setting $g$ and $h$ to identity functions and setting $\hat{d} = d$. Consequently, our metric of interest is \textbf{computation time for a given loss}. The primary contribution of this work is the introduction of $g$, $h$ and $\hat{d}$ functions that are significantly faster than those of existing work for a given loss $\mathcal{L} > 0$.

% \begin{center}
% % \begin{tabu} to \linewidth [0]{ X[l] | X[c] | X[c] | X[c] | X[c]}
% \begin{tabu} to \textwidth [0]{ X[l] | X[c] | X[c] | X[c] | X[c]}
% % \caption{Performance of Vector Quantization Algorithms}
% \hline
%     & Data Encoding Speed & Query Encoding Speed & Search Speed & Compression \\
% \hline
%     Bolt (proposed) & Very High & Very High & Very High & Medium \\
%     % Binary Embedding & High & High & High & Very Low \\
%     PQ & High & High & High & Low \\
%     OPQ & High & High & High & Medium \\
%     RVQ & High & Medium & High & Medium \\
%     GRVQ, LSQ, CQ, AQ, OTQ & Low, Very Low & Medium & High & High \\
%     Raw Floats & N/A & N/A & Low & None \\
% \hline
% \end{tabu}
% \end{center}




\subsection{Assumptions}

Like other work [][][][], we assume that there is an initial offline phase during which the functions $g$ and $h$ may be learned. This phase contains a training dataset for $\mathcal{X}$ but not necessarily $\vec{q}$. Following this offline phase, there is an online phase wherein we are given database vectors $\vec{x}$ that must be encoded and query vectors $\vec{q}$ for which we must compute the distances to all of the database vectors received so far. Once a query is received, these distances must be computed with as little latency as possible. The vectors of $\mat{X}$ may be given all at once, or one at a time; they may also be modified or deleted, necessitating re-encoding or removal. This is in contrast to most existing work, which assumes that $\vec{x}$ vectors are all added at once before any queries are recieved [][][][].

In practice, one might require the distances between $\vec{q}$ and only some of the database vectors $\mathcal{X}$ (in particular, the $k$ closest vectors). This can be achieved using an indexing structure, such as an Inverted Multi-Index [][] or Locality-Sensitive Hashing hash tables [][], that allow inspection of only a fraction of $\mathcal{X}$. Such indexing is complementary to our work in that our approach could be used to accelerate the computation of distances to the subset of $\mathcal{X}$ that is inspected. Consequently, we assume that the task is to compute the distances to all vectors, noting that, in a production setting, ``all vectors'' for a given query might be a subset of a full database.

%     -we consider only dot products and euclidean distances throughout this work since they are by far the most common, but note that our approach could in principle be generalized to any similarity or distance of the above form.\footnote{Practically speaking, functions with a large range of values for \delta(q_j, x_j), such as distances in Lp space with p > 2, are unlikely to be approximated well.}


