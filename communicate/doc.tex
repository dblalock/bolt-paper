
% \documentclass[conference]{IEEEtran}
% \documentclass{sig-alternate} % pre 2017
\documentclass[sigconf]{acmart}  % starting in 2017
\input{setup.tex}

\begin{document}

\setcopyright{rightsretained}

%Conference
\acmConference[KDD 2017]{ACM SIGKDD}{August 2017}{Halifax, Nova Scotia Canada}
\acmYear{2017}
\copyrightyear{2017}
% \acmPrice{15.00}

% ================================================================
\title{Bolt: Accelerated Data Mining with Fast Vector Compression}
% ================================================================

\author{Davis W. Blalock}
% \orcid{1234-5678-9012}
\affiliation{%
  \institution{Computer Science and Artificial \\ Intelligence Laboratory}
  \institution{Massachusetts Institute of Technology}
  % \streetaddress{P.O. Box 1212}
  % \city{Dublin}
  % \state{Ohio}
  % \postcode{43017-6221}
}
\email{dblalock@mit.edu}

\author{John V. Guttag}
\affiliation{%
  \institution{Computer Science and Artificial \\ Intelligence Laboratory}
  \institution{Massachusetts Institute of Technology}
  % \streetaddress{P.O. Box 1212}
  % \city{Dublin}
  % \state{Ohio}
  % \postcode{43017-6221}
}
\email{guttag@mit.edu}

% ------------------------------------------------
\begin{abstract}
% ------------------------------------------------

% v0
% The need to compute dot products and distances between vectors of parameters is ubiquitous in machine learning. These computations are often responsible for most of an algorithm's running time, and storing the parameters is usually responsible for most of its space consumption.

% We describe an approximate method of computing dot products and distances that greatly reduces both time and space consumption with little or no loss in accuracy. Our approach is based on vector quantization, and entails replacing the original computation with lookups in a compact table of sufficient statistics about the vectors in question. Unlike similar techniques, our algorithm does not assume that vectors are static and is designed to be hardware-friendly.

% We show experimentally that our approach can be used to accelerate matrix multiplications, k-means clustering, nearest neighbor search, and maximum inner product search by an order of magnitude or more. Furthermore, our approach is faster than even hamming distance computation, which has explicit hardware support on the tested platforms.

% v3

Vectors of data and model weights are at the heart of data mining. Storing such vectors accounts for most of the space usage of many algorithms, and operating on them accounts for most of the compute time. Recently, vector quantization methods have shown great promise in reducing these costs when operating on fixed collections of vectors. However, the high cost of accurately encoding the vectors using these schemes makes them less practical for changing collections, such as production databases, streams of sensor data, or machine learning model weights during training.

We introduce a vector quantization technique that can encode vectors up to $10\times$ faster than existing schemes while also accelerating operations such as distance and dot product computations by over $5\times$. Moreover, because it can encode over two megabytes of vectors per millisecond (2 GB/s), it makes vector quantization cheap enough to employ as a subroutine to accelerate other algorithms.

Specifically, we show experimentally that our approach can be used to accelerate neural network training, k-means clustering, nearest neighbor search, and maximum inner product search by up to $20\times$ compared to floating point operations and $5\times$ compared to other vector quantization methods. Furthermore, our approximate Euclidean distance and dot product computations are faster not only than those of related algorithms with much slower encodings, but even faster than Hamming distance computations, which have direct hardware support on the tested platforms.


% Our approach is to quantize one of the vectors and replace the original computation with lookups in a compact table of sufficient statistics about the other.

\end{abstract}

% ------------------------------------------------
% CCS taxonomy stuff / keywords
% ------------------------------------------------

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002950.10003648</concept_id>
<concept_desc>Mathematics of computing~Probability and statistics</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002950.10003648.10003671</concept_id>
<concept_desc>Mathematics of computing~Probabilistic algorithms</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10002950.10003648.10003688.10003696</concept_id>
<concept_desc>Mathematics of computing~Dimensionality reduction</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10002950.10003705</concept_id>
<concept_desc>Mathematics of computing~Mathematical software</concept_desc>
<concept_significance>100</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Mathematics of computing~Probability and statistics}
\ccsdesc[300]{Mathematics of computing~Probabilistic algorithms}
\ccsdesc[300]{Mathematics of computing~Dimensionality reduction}
\ccsdesc[100]{Mathematics of computing~Mathematical software}

\keywords{ACM proceedings, \LaTeX, text tagging}

\maketitle

% ================================================================
\section{Introduction} \label{sec:intro}
% ================================================================

\input{intro.tex}

% % ================================================================
% \section{Definitions and Problem} \label{sec:problem}
% % ================================================================

% \input{problem.tex}

% ================================================================
\section{Related Work} \label{sec:relatedWork}
% ================================================================

\input{related_work.tex}

% ================================================================
% \vspace{-2mm}
\section{Method} \label{sec:method}
% ================================================================

\input{method.tex}

% ================================================================
\section{Results} \label{sec:results}
% ================================================================

\input{results.tex}

% ================================================================
\section{Conclusion} \label{sec:conclusion}
% ================================================================

% ================================================================
% References
% ================================================================

% \IEEEtriggeratref{27}	% trigger column break to make cols even
\bibliographystyle{ACM-Reference-Format}
\bibliography{doc}

\end{document}
