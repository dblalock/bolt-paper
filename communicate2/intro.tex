
Matrix multiplication is among the most fundamental subroutines throughout numerical computing, including machine learning. As a result, there has been a great deal of work on implementing high-speed matrix multiplication libraries [], designing custom hardware to accelerate multiplication of certain classes of matrices [], scaling matrix multiplication across many machines [], and designing efficient Approximate Matrix Multiplication (AMM) algorithms under various assumptions and problem settings [].

We focus on the AMM task under the assumptions that the matrices are tall, relatively dense, and resident in a single machine's memory. In this setting, the primary challenge is not reduction of disk or network usage [], efficient coordination between distributed workers [], maximization of a space-distortion tradeoff [], or reduction of asymptotic complexity []. Instead, it is minimization of the amount of CPU time required to approximate linear operations with a given level of fidelity.

This setting arises naturally in machine learning and data mining when one has a data matrix $\mat{A}$ whose rows are samples and a linear operator $\mat{B}$ one wishes to apply to these samples. $\mat{B}$ could be a linear classifier, linear regressor, or an embedding matrix, among other possibilities.

As a concrete example, consider the task of approximating a softmax classifier trained to predict image labels given embeddings derived from a neural network. Here, the rows of $\A$ are the embeddings for each image, and the columns of $\B$ are the weight vectors for each class. Classification is performed by computing the product $\A\B$ and taking the argmax within each row of the result.
In Figure~\ref{fig:fig1}, we see the results of approximating $\A\B$ using our method, \ours, and its best-performing rivals \cite{hashjl, sparsepca} on the CIFAR-10 and CIFAR-100 datasets. Each method yields a curve whose points are specific speedups and associated levels of classification accuracy. More speedup and more accuracy (up and to the right) is better. While we defer detailed discussion to section~\ref{sec:results}, it is clear that our proposed approach significantly outperforms alternatives.

\vspace{1mm}
\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{amm/fig1}
\caption{\oursp achieves a dramatically better speed-accuracy tradeoff than existing methods when approximating two linear classifiers.}
\label{fig:fig1}
\end{center}
\end{figure}

% In addition to achieving strong empirical performance, our method also has an

In addition to achieving strong empirical performance, our method represents a significant methodological departure from most traditional approaches to this problem. Traditional AMM methods construct matrices $\V_A, \V_B \in \R^{D \times d}, d \ll D$ such that
\begin{align}
    \A \B \approx (\A \V_A) (\V_B^\top \B).
\end{align}
Often, $\V_A$ and $\V_B$ are sparse, embody some sort of sampling scheme, or have other structure such that these projection operations are faster than a dense matrix multiply. In short, these methods use linear functions to preprocess $\A$ and $\B$ and reduce the problem to exact matrix multiplication in a lower-dimensional space
% reduce the AMM problem to exact matrix multiplication in a lower-dimensional space using linear functions to preprocess $\A$ and $\B$.

Our proposed method, \ours, instead employs a \textit{nonlinear} preprocessing function and reduces the problem to table lookups. Moreover, in the case that $\B$ is known ahead of time---which happens when applying a trained linear model to new data, among other situations---\oursp does not require any multiply-add operations. This includes binary \texttt{XNOR} and \texttt{popcount} operations [], as well as the summation of shifted and masked scalars to approximate multiplication []. I.e., we are not merely approximating scalar multiplies at the bit manipulation level, but approximating matrix products at the algorithm level.

% An extension of this method common in the information retrieval literature is to define a nonlinear function $g(\cdot)$ such that
% \begin{align}
%     \A \B \approx g(\A \V_A) (\V_B^\top \B).
% \end{align}
% The $g(\cdot)$ function typically binarizes and induces structured sparsity in the resulting matrix. This allows the product of this matrix and $(\V_B^\top \B)$ to be computed using only additions, instead of multiply-adds. Moreover, if the sparsity is structured appropriately, dot products can be reduced to table lookups, with the indices into the tables given by the indices of the nonzeros.




% Our proposed method, \ours, instead preprocesses $\A$ and $\B$ with \textit{nonlinear} functions and reduce the problem to table lookups.


% These methods may also attempt to reduce the cost of each multiply-add by reducing the number of bits used to store each scalar. This works b



% Existing approaches to this task attempt to reduce the CPU time by performing fewer multiply-add operations or by reducing the cost of each such operation. The former typically entails projecting $\A$ and $\B$ into a lower dimensional space before multiplying them, and the latter typically entails reducing the number of bits used to store each scalar element.
% % This typically entails constructing a matrix $\V \in \R^{D \times d}, d < D$ such that
% % \begin{align}
% %     \A \B \approx (\A \V) (\V^\top \B).
% % \end{align}
% % Often, $\V$ is sparse or has structure such that these projection operations are faster than a dense matrix multiply. These methods may also attempt to reduce the cost of each multiply-add by reducing the number of bits used to store each scalar.

% % Instead of performing dimensionality or bitwidth reduction,
% In contrast, we introduce \ours, a method that can eliminate the multiply-add operations entirely when given time to preprocess $\B$. It does this by precomputing certain statistics about $\B$ and replacing the multiply-adds with table lookups. Crucially, these table lookups are not merely a different implementation of multiplication---as we discuss in section~\ref{sec:method}, they implement nonlinear functions. % that cannot be characterized as an algebraic ring.

Our method is most closely related to the vector quantization methods used for similarity search []. However, instead of using an expensive quantization function that requires many multiplies, we introduce a family of quantization functions that require no multiplies. %, including binary \texttt{XNOR} and \texttt{popcount} operations (though excluding implementation-level multiplies to compute memory addresses).
% TODO something about multiplication taking a lot of transistors vs table lookups.

Our contributions can be summarized as follows:
\begin{itemize}\itemsep0em
    \item An efficient family of vector quantization functions that can encode over 100GB of data per second in a single CPU thread.
    \item A provably good procedure for learning one of these functions from training data, in addition to other theoretical analysis.
    \item A high-speed summation algorithm for low-bitwidth integers that optimally avoids loss of precision without upcasting.
    \item An algorithm based on these functions for approximate matrix multiplication. Experiments across hundreds of diverse matrices demonstrate that this algorithm significantly outperforms existing alternatives.
\end{itemize}

% ------------------------------------------------
\subsection{Problem Formulation}
% ------------------------------------------------

Let $\A \in \R^{N \times D}$ and $\B \in \R^{D \times M}$ be two matrices, with $N \gg D, M$, and $M$ not significantly larger than $D$. Given a computation time budget $\tau$, our task is to
construct three functions $g(\cdot)$, $h(\cdot)$, and $f(\cdot)$ such that
\begin{align}
    \norm{\alpha f(g(\A), h(\B)) + \mat{\beta} - \A\B}_F < \eps(\tau) \norm{\A\B}_F
\end{align}
for the smallest error $\eps(\tau)$ possible. The scalar $\alpha$ and bias matrix $\mat{\beta}$ must be constants, and are separated from $f(\cdot)$ so that it can produce low-bitwidth outputs that may not capture the range of values in $\A\B$. %Furthermore, $\alpha$ must be a power of $2$ so that multiplication can be replaced with bit shifting.\footnote{}
% to allow $f(\cdot)$ to

% . Moreover, the computation time $\tau(\eps)$ to run these three functions should be much smaller than the comuputation time $\tau_0$ to compute $\A\B$ directly.

We assume the existence of a training set $\tilde{\A}$, whose rows are drawn from the same distribution as the rows of $\A$. This is a natural assumption in the case that rows of $\A$ represent examples in training data, or structured subsets thereof (such as patches of images). This assumption is common in the information retrieval literature \cite{bolt, pairq}, but is a significant departure from most theoretical work.

% We also assume that the cost of computing $h(\B)$ negligible provided that $h(\B) \in O(MD)$. One sufficient condition for this is $\B$ being provided ahead of time (as is the case when $\B$ represents fixed model weights). A second is that $\B$ has sufficiently few columns compared to the number of rows in $\A$ (i.e., $M \ll N$).


% ================================================================

% These methods transform the two matrices such that one becomes a collection of lookup tables and the other indices into those tables. Once transformed, the matrix multiplication \textit{per se} requires only table lookups. We adopt this same pattern, but replace the multiply-intensive transform with an efficient alternative. This replacement is enabled through our introduction of a fast and trainable family of quantization functions.

%  which already compute matrix products \textit{per se} with no multiplies. However, they require many multiplies in

% our key insight is that the quantization codebooks used by these methods


% The basic insight behind our method is that vector quantization can be carried

% In machine learning specifically, it is often the bottleneck in deep neural networks, which have emerged as an indispensible class of models for many tasks.



% multiplications [] or reduces the cost of each multiplication [], with the most extreme case of the latter being multiplication of binary values [].

% Most existing approaches to this task project $\mat{A}$ and $\mat{B}$ into a lower-dimensional space and then perform an exact matrix multiplication in this reduced space []. Concretely

% Most existing approaches to this task project $\mat{A}$ and $\mat{B}$

 % They may also reduce the number of bits used to store elements of $\A$ and $\B$ or


% We formalize these notions in the following section, but this corresponds to the common scenario in machine learning wherein one has a matrix whose

% Tall and (relatively) dense arise naturally in machine learning,
% real-world assumptions, such as matrices
% We focus on approximate multiplication of tall, dense matrices that already reside in memory.
% More specifically, consider two matrices $\mat{A} \in \R^{N \times D}$ and $\mat{B} \in \R^{D \times M}$, and their product $\mat{C} \triangleq \mat{A}\mat{B}$. We consider the setting in which $N \gg D, M$, and $M$ is not significantly larger than $D$. Furthermore, we focus on the typical real-world case in which $N$, $D$, and $M$ are all small enough that asymptotic complexity is not an adequate characterization of speed.
% Such matrices arise naturally in machine learning, when $A$ is a matrix of data and $B$ is a matrix of model weights or some other sort of linear projection. % It is also common in this setting to have a matrix of training data $\mat{\tilde{A}}$ whose rows share the same distribution as those of $\mat{A}$, and we assume access to such a matrix.

% By tall, we are referring to the typical real-world case in which asymptotic complexity is a poor definition of speed.

% In contrast to most existing AMM work, we also consider various levels of prior knowledge about the distributions from which the matrices are drawn. In the least informed case, nothing is known about either matrix until the instant they are provided. In the most informed case, one matrix is known ahead of time and the other has rows drawn from a distribution from which we have numerous i.i.d. samples.

% Provided that one matrix is known ahead of time, our method requires zero multiplication operations, aside from possible pointer arithmetic at the implementation level.
%This is in contrast to existing work, which either reduces the number of multiplications [] or reduces the cost of each multiplication [], with the most extreme case of the latter being multiplication of binary values [].
