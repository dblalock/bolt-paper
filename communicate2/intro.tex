
Matrix multiplication is among the most fundamental subroutines used in machine learning and scientific computing. As a result, there has been a great deal of work on implementing high-speed matrix multiplication libraries \cite{pytorch,eigen,tensorflow}, designing custom hardware to accelerate multiplication of certain classes of matrices \cite{eie,eyeriss,scnn,tpu},
performing distributed matrix multiplication \cite{distributedCoded, shortDot, entangledPolynomial, matmulCommunicationBounds},
and designing efficient Approximate Matrix Multiplication (AMM) algorithms under various assumptions% and problem settings.
.

We focus on the AMM task under the assumptions that the matrices are tall, relatively dense, and resident in a single machine's memory. In this setting, the primary challenge is minimizing the amount of compute time required to approximate linear operations with a given level of fidelity.
%not reduction of disk or network usage [], efficient coordination between distributed workers [], maximization of a space-distortion tradeoff [], or reduction of asymptotic complexity []. Instead, it is minimization of the amount of CPU time required to approximate linear operations with a given level of fidelity.
This setting arises naturally in machine learning and data mining when one has a data matrix $\mat{A}$ whose rows are samples and a linear operator $\mat{B}$ one wishes to apply to these samples. $\mat{B}$ could be a linear classifier, linear regressor, or an embedding matrix, among other possibilities.

As a concrete example, consider the task of approximating a softmax classifier trained to predict image labels given embeddings derived from a neural network. Here, the rows of $\A$ are the embeddings for each image, and the columns of $\B$ are the weight vectors for each class. Classification is performed by computing the product $\A\B$ and taking the argmax within each row of the result.
In Figure~\ref{fig:fig1}, we see the results of approximating $\A\B$ using our method and its best-performing rivals \cite{hashjl, sparsePCA} on the CIFAR-10 and CIFAR-100 datasets.
\vspace{1mm}
\begin{figure}[h]
\begin{center}
% \includegraphics[width=\linewidth]{amm/fig1}
\includegraphics[width=.95\linewidth]{amm/fig1}
\caption{Our method achieves a dramatically better speed-accuracy tradeoff than the best existing methods when approximating two linear classifiers.}
\label{fig:fig1}
\end{center}
\end{figure}
\vspace{-1mm}
% Each method yields a curve whose points are specific speedups and associated levels of classification accuracy. More speedup and more accuracy (up and to the right) is better. While we defer detailed discussion to Section~\ref{sec:results}, it is clear that our proposed approach significantly outperforms alternatives.

% In addition to achieving strong empirical performance, our method also has an

% In addition to achieving strong empirical performance,
Our method represents a significant methodological departure from most traditional approaches to this problem. Traditional AMM methods construct matrices $\V_A, \V_B \in \R^{D \times d}, d \ll D$ such that
\begin{align}
    \A \B \approx (\A \V_A) (\V_B^\top \B).
\end{align}
Often, $\V_A$ and $\V_B$ are sparse, embody some sort of sampling scheme, or have other structure such that these projection operations are faster than a dense matrix multiply. In short, these methods use linear functions to preprocess $\A$ and $\B$ and reduce the problem to exact matrix multiplication in a lower-dimensional space.
% reduce the AMM problem to exact matrix multiplication in a lower-dimensional space using linear functions to preprocess $\A$ and $\B$.

Our proposed method, \ours\footnote{Multiply-ADDitioN-lESS}, instead employs a \textit{nonlinear} preprocessing function and reduces the problem to table lookups. Moreover, in the case that $\B$ is known ahead of time---which happens when applying a trained linear model to new data, among other situations---\oursp does not require any multiply-add operations.
Our method is most closely related to vector quantization methods used for similarity search (e.g., \cite{bolt,quickAdc,quickerAdc,pq,opq}). However, instead of using an expensive quantization function that requires many multiply-adds, we introduce a family of quantization functions that require no multiply-adds. %, including binary \texttt{XNOR} and \texttt{popcount} operations (though excluding implementation-level multiplies to compute memory addresses).
% TODO something about multiplication taking a lot of transistors vs table lookups.

Our contributions can be summarized as follows:
\vspace{-3mm}
\begin{itemize}\itemsep-1mm
    \item An efficient family of learned vector quantization functions that can encode over 100GB of data per second in a single CPU thread.
    % \item A provably good procedure for learning one of these functions from training data, in addition to other theoretical analysis.
    \item A high-speed summation algorithm for low-bitwidth integers that avoids upcasting, saturation, and overflow.
    \item An algorithm based on these functions for approximate matrix multiplication. Experiments across hundreds of diverse matrices demonstrate that this algorithm significantly outperforms existing alternatives. It also features theoretical quality guarantees.
\end{itemize}
\vspace{-3mm}

% ------------------------------------------------
\subsection{Problem Formulation} \label{sec:problemStatement}
% ------------------------------------------------

% Let $\A \in \R^{N \times D}$ and $\B \in \R^{D \times M}$ be two matrices, with $N \gg D, M$, and $M$ not significantly larger than $D$. Given a computation time budget $\tau$, our task is to
Let $\A \in \R^{N \times D}$ and $\B \in \R^{D \times M}$ be two matrices, with $N \gg D \ge M$. Given a computation time budget $\tau$, our task is to
construct three functions $g(\cdot)$, $h(\cdot)$, and $f(\cdot)$, along with constants $\alpha$ and $\mat{\beta}$, such that
\begin{align} \label{eq:objective}
    \norm{\alpha f(g(\A), h(\B)) + \mat{\beta} - \A\B}_F < \eps(\tau) \norm{\A\B}_F
\end{align}
for as small an error $\eps(\tau)$ possible. The constants $\alpha$ and $\mat{\beta}$ are separated from $f(\cdot,\cdot)$ so that $f(\cdot,\cdot)$ can produce low-bitwidth outputs (e.g., in the range $[0, 255]$) even when the entries of $\A\B$ do not fall in this range.

We assume the existence of a training set $\tilde{\A}$, whose rows are drawn from the same distribution as the rows of $\A$. This is a natural assumption in the case that rows of $\A$ represent examples in training data, or structured subsets thereof (such as patches of images). This assumption is common in the information retrieval literature \cite{bolt,pairq,quip}, though not in most theoretical work. Lastly, while our ideas are not specific to CPUs, we focus on CPU performance throughout this paper and leave implementation on GPUs, FPGAs, etc., to future work.

% While we believe that our results suggest that methods similar to our own could hold promise for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms, such extensions will require additional research---the present work only claims superiority for \textit{approximate matrix multiplication} as described in our problem formulation.
