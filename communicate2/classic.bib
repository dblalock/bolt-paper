
@inproceedings{messer_choosing_1998,
	address = {Southampton},
	title = {Choosing an {Optimal} {Neural} {Network} {Size} to {Aid} a {Search} through a {Large} {Image} {Database}},
	isbn = {978-1-901725-04-9},
	url = {http://www.bmva.org/bmvc/1998/papers/d105/h105.htm},
	doi = {10.5244/C.12.24},
	language = {en},
	urldate = {2019-07-18},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 1998},
	publisher = {British Machine Vision Association},
	author = {Messer, K. and Kittler, J.},
	year = {1998},
	pages = {24.1--24.10},
	file = {Messer and Kittler - 1998 - Choosing an Optimal Neural Network Size to Aid a S.pdf:/Users/davis/Zotero/storage/IA9TNMIE/Messer and Kittler - 1998 - Choosing an Optimal Neural Network Size to Aid a S.pdf:application/pdf}
}

@article{janowsky_pruning_1989,
	title = {Pruning versus clipping in neural networks},
	volume = {39},
	issn = {0556-2791},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.39.6600},
	doi = {10.1103/PhysRevA.39.6600},
	language = {en},
	number = {12},
	urldate = {2019-07-18},
	journal = {Physical Review A},
	author = {Janowsky, Steven A.},
	month = jun,
	year = {1989},
	pages = {6600--6603},
	file = {Janowsky - 1989 - Pruning versus clipping in neural networks.pdf:/Users/davis/Zotero/storage/B9H9NMMQ/Janowsky - 1989 - Pruning versus clipping in neural networks.pdf:application/pdf}
}

@inproceedings{hassibi_optimal_1993,
	address = {San Francisco, CA, USA},
	title = {Optimal {Brain} {Surgeon} and general network pruning},
	isbn = {978-0-7803-0999-9},
	url = {http://ieeexplore.ieee.org/document/298572/},
	doi = {10.1109/ICNN.1993.298572},
	abstract = {We investigate the use of inforination from all second order derivatives of the error function to perform network pruning (i.e., renioving unimportant weights fi*oma trained network) in order to improve generalization, siniplify networks, reduce hardware or storage requirements, increase the speed of further training. and in soiiie cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is significantly better than magnitude-based methods and Optinial Brain Damage, which often remove the wrong weights. OBS permits pruning of more weights than other methods (for the same error on the training set), and thus yields better generalizatioii on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H-' from training data and structural information of the net. OBS permits a 76\%, a 62\%, and a 90\% reduction in weights over backpropagation with weight decay on three benchmark MONK'S problems. Of OBS, Optimal Brain Damage, and a magnitude-based method, only OBS deletes the correct weights from a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg used 18,000 weights in their NETtalk network, we used OBS to prune a network to just 1,560weights, yielding better generalization.},
	language = {en},
	urldate = {2019-07-18},
	booktitle = {{IEEE} {International} {Conference} on {Neural} {Networks}},
	publisher = {IEEE},
	author = {Hassibi, B. and Stork, D.G. and Wolff, G.J.},
	year = {1993},
	pages = {293--299},
	file = {Hassibi et al. - 1993 - Optimal Brain Surgeon and general network pruning.pdf:/Users/davis/Zotero/storage/DJM6KICJ/Hassibi et al. - 1993 - Optimal Brain Surgeon and general network pruning.pdf:application/pdf}
}

@article{reed_pruning_1993,
	title = {Pruning algorithms-a survey},
	volume = {4},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/248452/},
	doi = {10.1109/72.248452},
	abstract = {A rule of thumb for obtaininggood generalizationin systems trained by examples is that one should use the smallest system that will fit the data. Unfortunately, it usually is not obvious what size is best; a system that is too small will not be able to learn the data while one that is just big enough may learn very slowly and be very sensitive to initial conditions and learning parameters. This paper is a survey of neural network pruning algorithms.The approachtaken by the methodsdescribed here is 5! to train a network that is larger than necessary and then remove Y the parts that are not needed.},
	language = {en},
	number = {5},
	urldate = {2019-07-18},
	journal = {IEEE Transactions on Neural Networks},
	author = {Reed, R.},
	month = sep,
	year = {1993},
	pages = {740--747},
	file = {Reed - 1993 - Pruning algorithms-a survey.pdf:/Users/davis/Zotero/storage/FNI865HI/Reed - 1993 - Pruning algorithms-a survey.pdf:application/pdf}
}

@article{castellano_iterative_1997,
	title = {An iterative pruning algorithm for feedforward neural networks},
	volume = {8},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/572092/},
	doi = {10.1109/72.572092},
	language = {en},
	number = {3},
	urldate = {2019-07-18},
	journal = {IEEE Transactions on Neural Networks},
	author = {Castellano, G. and Fanelli, A.M. and Pelillo, M.},
	month = may,
	year = {1997},
	pages = {519--531},
	file = {Castellano et al. - 1997 - An iterative pruning algorithm for feedforward neu.pdf:/Users/davis/Zotero/storage/D7KNMRHH/Castellano et al. - 1997 - An iterative pruning algorithm for feedforward neu.pdf:application/pdf}
}

@article{pearlmutter_fast_1994,
	title = {Fast {Exact} {Multiplication} by the {Hessian}},
	volume = {6},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1994.6.1.147},
	doi = {10.1162/neco.1994.6.1.147},
	language = {en},
	number = {1},
	urldate = {2019-07-18},
	journal = {Neural Computation},
	author = {Pearlmutter, Barak A.},
	month = jan,
	year = {1994},
	pages = {147--160},
	file = {Pearlmutter - 1994 - Fast Exact Multiplication by the Hessian.pdf:/Users/davis/Zotero/storage/426QNJ84/Pearlmutter - 1994 - Fast Exact Multiplication by the Hessian.pdf:application/pdf}
}

@article{bishop_training_1995,
	title = {Training with {Noise} is {Equivalent} to {Tikhonov} {Regularization}},
	volume = {7},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1995.7.1.108},
	doi = {10.1162/neco.1995.7.1.108},
	language = {en},
	number = {1},
	urldate = {2019-07-18},
	journal = {Neural Computation},
	author = {Bishop, Chris M.},
	month = jan,
	year = {1995},
	pages = {108--116},
	file = {Bishop - 1995 - Training with Noise is Equivalent to Tikhonov Regu.pdf:/Users/davis/Zotero/storage/RKM5XYAA/Bishop - 1995 - Training with Noise is Equivalent to Tikhonov Regu.pdf:application/pdf}
}

@article{mozer_using_1989,
	title = {Using {Relevance} to {Reduce} {Network} {Size} {Automatically}},
	volume = {1},
	issn = {0954-0091, 1360-0494},
	url = {https://www.tandfonline.com/doi/full/10.1080/09540098908915626},
	doi = {10.1080/09540098908915626},
	language = {en},
	number = {1},
	urldate = {2019-07-18},
	journal = {Connection Science},
	author = {Mozer, Michael C. and Smolensky, Paul},
	month = jan,
	year = {1989},
	pages = {3--16},
	file = {Mozer and Smolensky - 1989 - Using Relevance to Reduce Network Size Automatical.pdf:/Users/davis/Zotero/storage/4FV4V9K8/Mozer and Smolensky - 1989 - Using Relevance to Reduce Network Size Automatical.pdf:application/pdf}
}

@inproceedings{tresp_early_1997,
	title = {Early brain damage},
	booktitle = {Advances in neural information processing systems},
	author = {Tresp, Volker and Neuneier, Ralph and Zimmermann, Hans-Georg},
	year = {1997},
	pages = {669--675},
	file = {Tresp et al. - Early Brain Damage.pdf:/Users/davis/Zotero/storage/CEVR3DQN/Tresp et al. - Early Brain Damage.pdf:application/pdf}
}

@inproceedings{lecun_optimal_1990,
	title = {Optimal brain damage},
	booktitle = {Advances in neural information processing systems},
	author = {LeCun, Yann and Denker, John S and Solla, Sara A},
	year = {1990},
	pages = {598--605},
	file = {LeCun et al. - Optimal Brain Damage.pdf:/Users/davis/Zotero/storage/SZ2FTBXN/LeCun et al. - Optimal Brain Damage.pdf:application/pdf}
}

@inproceedings{mozer_skeletonization:_1989,
	title = {Skeletonization: {A} technique for trimming the fat from a network via relevance assessment},
	booktitle = {Advances in neural information processing systems},
	author = {Mozer, Michael C and Smolensky, Paul},
	year = {1989},
	pages = {107--115},
	file = {Mozer and Smolensky - Skeletonization A Technique for Trimming the Fat .pdf:/Users/davis/Zotero/storage/I4D9W8BB/Mozer and Smolensky - Skeletonization A Technique for Trimming the Fat .pdf:application/pdf}
}

@techreport{lawrence_what_1998,
	title = {What size neural network gives optimal generalization? {Convergence} properties of backpropagation},
	author = {Lawrence, Steve and Giles, C Lee and Tsoi, Ah Chung},
	year = {1998},
	file = {Lawrence et al. - What Size Neural Network Gives Optimal Generalizat.pdf:/Users/davis/Zotero/storage/7IS2WNQ3/Lawrence et al. - What Size Neural Network Gives Optimal Generalizat.pdf:application/pdf}
}

@article{karnin_simple_1990,
	title = {A simple procedure for pruning back-propagation trained neural networks},
	volume = {1},
	number = {2},
	journal = {IEEE transactions on neural networks},
	author = {Karnin, Ehud D},
	year = {1990},
	pages = {239--242},
	file = {00080236.pdf:/Users/davis/Zotero/storage/JD5E24V5/00080236.pdf:application/pdf}
}