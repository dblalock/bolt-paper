%!TEX output_directory = aux

\documentclass{article}  % sysml

\usepackage{booktabs}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
% \usepackage[accepted]{sysml2019}
% \usepackage{sysml2019}
\usepackage{icml2020}

% \newcommand{\oursp}{\textsc{HashMul}\text{ }}
% \newcommand{\ours}{\textsc{HashMul}}
% \newcommand{\oursp}{\textsc{MADDNESS}\text{ }}
% \newcommand{\ours}{\textsc{MADDNESS}}
\newcommand{\oursp}{\textsc{Maddness}\text{ }}
\newcommand{\ours}{\textsc{Maddness}}
\newcommand{\oursHash}{\textsc{MaddnessHash}}

% \newcommand{\mine}{\textsc{Sprintz}}
% \newcommand{\minesp}{\textsc{Sprintz}\text{ }}

% \newcommand{\npapers}{\text{83 }}
% \newcommand{\rawnpapers}{83}

% \usepackage{flafter}  %

% \usepackage[sorting=ynt]{biblatex}
% \usepackage[sorting=ynt]{natbib}
% \usepackage[sort]{natbib}
% \DeclareSortingScheme{noneyear}{
%  \sort{\citeorder}
%  \sort{\field{year}}
% }

% \usepackage[sorting=ynt]{natbib}

\input{setup.tex}

% optional custom title for header
% \sysmltitlerunning{Multiplying Matrices Without Multiplying}
\icmltitlerunning{Multiplying Matrices Without Multiplying}

\begin{document}

\twocolumn[
% ================================================================
\icmltitle{Multiplying Matrices Without Multiplying}
% ================================================================

% \begin{sysmlauthorlist}
\begin{icmlauthorlist}
% \sysmlauthor{Davis Blalock}{mit}
% \sysmlauthor{John Guttag}{mit}
\icmlauthor{Davis Blalock}{mit}
\icmlauthor{Tamara Broderick?}{mit}
\icmlauthor{John Guttag}{mit}
% \end{sysmlauthorlist}
\end{icmlauthorlist}

% \sysmlaffiliation{csail}{MIT CSAIL, Cambridge, MA, USA}
% \sysmlcorrespondingauthor{Davis Blalock}{dblalock@mit.edu}
\icmlaffiliation{csail}{MIT CSAIL, Cambridge, MA, USA}
\icmlcorrespondingauthor{Davis Blalock}{dblalock@mit.edu}

% apparently these only show up in pdf metadata, not document
% \sysmlkeywords{Approximate Algorithms, Matrix Multiplication}
\icmlkeywords{Vector Quantization, Approximate Algorithms, Matrix Multiplication}

\vskip 0.3in
] % end of icml 2 columnn

\printAffiliationsAndNotice{}

% ------------------------------------------------
\begin{abstract}
% ------------------------------------------------

Multiplying matrices is among the most fundamental and most computationally demanding operations in machine learning and numerical computing. Consequently, the task of efficiently approximating matrix products has received significant attention.

We introduce an approximate matrix multiplication algorithm that substantially outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it runs far faster than current approaches for a given amount of error, often obtaining more than a $10\times$ relative speedup. In the common case that one matrix is known ahead of time, our method also has the interesting property that it requires \textit{zero} multiply-add operations. The key idea behind our approach is to replace the expensive encoding step common in vector quantization methods with a learned locality-sensitive hash function.


\end{abstract}
% ]  % end of manual twocolumn for sysml

% \maketitle

% ================================================================
\section{Introduction} \label{sec:intro}
% ================================================================

\input{intro.tex}

% ================================================================
% \section{Background and Related Work}
\section{Related Work} \label{sec:relatedWork}
% ================================================================

\input{relatedWork.tex}
% \input{bg.tex}

%================================================================
\vspace{-1.5mm}
\section{Background - Product Quantization} \label{sec:background}
%================================================================

\input{background.tex}

%================================================================
% \vspace{-5mm}
\vspace{-3mm}
\section{Our Method} \label{sec:method}
\vspace{-1mm}
%================================================================

\input{method.tex}

% %================================================================
% \section{Theoretical Analysis}
% %================================================================

% \input{theory.tex}

%================================================================
\vspace{-1.5mm}
\section{Experiments} \label{sec:results}
%================================================================

\input{results.tex}


%================================================================
\vspace{-1.5mm}
\section{Conclusion}
\vspace{-.5mm}
%================================================================

We introduce an approximate matrix multiplication algorithm that achieves a significantly better speed-quality tradeoff than existing methods, as measured on various machine learning and other tasks using hundreds of real-world matrices from diverse domains. Our method's performance stems from 1) its replacement of the bottleneck in existing methods with a learned locality-sensitive hash function, 2) its optimization of a quantity that other methods cannot optimize without a large slowdown, and 3) its use of an inexact estimator for computing sums. In future work, we plan to integrate our method into neural networks and specialize it for convolutions.

% ================================================================
% References
% ================================================================

% \IEEEtriggeratref{27} % trigger column break to make cols even
% \bibliographystyle{ACM-Reference-Format}
% \bibliographystyle{abbrev}
% \bibliographystyle{sysml2019}
\bibliographystyle{icml2020}
% \bibliography{prune,architectures,misc,understandDnn,classic,datasets,compress,science,metapapers}


% TODO uncomment

\bibliography{architectures,datasets,misc,sprintz,extract,bolt,backprop-alternatives,distillation,fast-dnn-runtime-stuff,fast-optimize,nets-theory,small-fast-arch,sparseAndPrune,scalarQuantize,vectorQuantize,combo,hashing,shrink+prune,binarize,ternary,automl,zero-shot,few-shot,amm,ammMore,lsh,adversarial}
\clearpage
\newpage  % so we can cut the pdf
\appendix
\input{appendix.tex}

\end{document}
