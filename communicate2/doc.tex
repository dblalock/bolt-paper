%!TEX output_directory = aux

\documentclass{article}  % sysml

\usepackage{booktabs}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
% \usepackage[accepted]{sysml2019}
% \usepackage{sysml2019}
\usepackage{icml2020}

% \newcommand{\oursp}{\textsc{HashMul}\text{ }}
% \newcommand{\ours}{\textsc{HashMul}}
% \newcommand{\oursp}{\textsc{MADDNESS}\text{ }}
% \newcommand{\ours}{\textsc{MADDNESS}}
\newcommand{\oursp}{\textsc{Maddness}\text{ }}
\newcommand{\ours}{\textsc{Maddness}}
\newcommand{\oursHash}{\textsc{MaddnessHash}}

% \newcommand{\mine}{\textsc{Sprintz}}
% \newcommand{\minesp}{\textsc{Sprintz}\text{ }}

% \newcommand{\npapers}{\text{83 }}
% \newcommand{\rawnpapers}{83}

% \usepackage{flafter}  %

% \usepackage[sorting=ynt]{biblatex}
% \usepackage[sorting=ynt]{natbib}
% \usepackage[sort]{natbib}
% \DeclareSortingScheme{noneyear}{
%  \sort{\citeorder}
%  \sort{\field{year}}
% }

% \usepackage[sorting=ynt]{natbib}

\input{setup.tex}

% optional custom title for header
% \sysmltitlerunning{Multiplying Matrices Without Multiplying}
\icmltitlerunning{Multiplying Matrices Without Multiplying}

\begin{document}

\twocolumn[
% ================================================================
% \title{Have We Actually Learned Anything about \\ Pruning Neural Networks?}
% \sysmltitle{Multiplying Matrices Without Multiplying}
\icmltitle{Multiplying Matrices Without Multiplying}
% \title{Have We Actually Learned Anything about Pruning Neural Networks?}
% ================================================================

% \sysmltitlerunning{Submission and Formatting Instructions for SysML 2019}
% \begin{document}
% \twocolumn[
% \sysmltitle{Submission and Formatting Instructions for SysML 2019}

% \author{Davis W. Blalock}
% \affiliation{
%   \institution{Computer Science and Artificial \\ Intelligence Laboratory}
%   \institution{Massachusetts Institute of Technology}
% }
% \email{dblalock@mit.edu}

% \author{Jose Javier Gonzalez Ortiz}
% \affiliation{
%   \institution{Computer Science and Artificial \\ Intelligence Laboratory}
%   \institution{Massachusetts Institute of Technology}
% }
% \email{jjgo@mit.edu}

% \author{John V. Guttag}
% \affiliation{
%   \institution{Computer Science and Artificial \\ Intelligence Laboratory}
%   \institution{Massachusetts Institute of Technology}
% }
% \email{guttag@mit.edu}

% \begin{sysmlauthorlist}
\begin{icmlauthorlist}
% \sysmlauthor{Davis Blalock}{mit}
% \sysmlauthor{John Guttag}{mit}
\icmlauthor{Davis Blalock}{mit}
\icmlauthor{Tamara Broderick?}{mit}
\icmlauthor{John Guttag}{mit}
% \end{sysmlauthorlist}
\end{icmlauthorlist}

% \sysmlaffiliation{csail}{MIT CSAIL, Cambridge, MA, USA}
% \sysmlcorrespondingauthor{Davis Blalock}{dblalock@mit.edu}
\icmlaffiliation{csail}{MIT CSAIL, Cambridge, MA, USA}
\icmlcorrespondingauthor{Davis Blalock}{dblalock@mit.edu}

% apparently these only show up in pdf metadata, not document
% \sysmlkeywords{Approximate Algorithms, Matrix Multiplication}
\icmlkeywords{Approximate Algorithms, Matrix Multiplication}

\vskip 0.3in
] % end of icml 2 columnn

\printAffiliationsAndNotice{}

% ------------------------------------------------
\begin{abstract}
% ------------------------------------------------

Multiplying matrices is among the most fundamental and most computationally demanding operations in machine learning and numerical computing. Consequently, the task of efficiently approximating matrix products has received significant attention.

We introduce an approximate matrix multiplication algorithm that substantially outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it runs far faster than current approaches for a given amount of error, often obtaining more than a $10\times$ relative speedup. In the common case that one matrix is known ahead of time, our method also has the interesting property that it requires \textit{zero} multiply-add operations. The key idea behind our approach is to replace the expensive encoding step common in vector quantization methods with a learned locality-sensitive hash function.


\end{abstract}
% ]  % end of manual twocolumn for sysml

% \maketitle

% ================================================================
\section{Introduction} \label{sec:intro}
% ================================================================

\input{intro.tex}

% ================================================================
% \section{Background and Related Work}
\section{Related Work} \label{sec:relatedWork}
% ================================================================

\input{relatedWork.tex}
% \input{bg.tex}

%================================================================
\vspace{-1.5mm}
\section{Background - Product Quantization} \label{sec:background}
%================================================================

\input{background.tex}

%================================================================
\vspace{-7mm}
\section{Our Method} \label{sec:method}
\vspace{-1mm}
%================================================================

\input{method.tex}

% %================================================================
% \section{Theoretical Analysis}
% %================================================================

% \input{theory.tex}

%================================================================
\section{Experiments} \label{sec:results}
%================================================================

\input{results.tex}


%================================================================
\section{Conclusion}
%================================================================

We introduce an approximate matrix multiplication algorithm that achieves a significantly better speed-quality tradeoff than existing methods, as measured on hundreds of real-world matrices from various domains. Our method's performance stems from 1) its replacement of the bottleneck in existing methods with a learned locality-sensitive hash function, 2) its optimization of a quantity that other methods cannot optimize without a large slowdown, and 3) its use of an inexact estimator for computing sums. In future work, we plan to integrate our method into neural networks and specialize it for convolutions.

% ================================================================
% References
% ================================================================

% \IEEEtriggeratref{27} % trigger column break to make cols even
% \bibliographystyle{ACM-Reference-Format}
% \bibliographystyle{abbrev}
% \bibliographystyle{sysml2019}
\bibliographystyle{icml2020}
% \bibliography{prune,architectures,misc,understandDnn,classic,datasets,compress,science,metapapers}


% TODO uncomment

\bibliography{architectures,datasets,misc,sprintz,extract,bolt,backprop-alternatives,distillation,fast-dnn-runtime-stuff,fast-optimize,nets-theory,small-fast-arch,sparseAndPrune,scalarQuantize,vectorQuantize,combo,hashing,shrink+prune,binarize,ternary,automl,zero-shot,few-shot,amm,ammMore,lsh,adversarial}
\clearpage
\newpage  % so we can cut the pdf
\appendix
\input{appendix.tex}

\end{document}
