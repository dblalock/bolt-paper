%!TEX output_directory = aux

\documentclass{article}  % sysml

\usepackage{booktabs}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{icml2020}

% \newcommand{\oursp}{\textsc{HashMul}\text{ }}
% \newcommand{\ours}{\textsc{HashMul}}
% \newcommand{\oursp}{\textsc{MADDNESS}\text{ }}
% \newcommand{\ours}{\textsc{MADDNESS}}
\newcommand{\oursp}{\textsc{Maddness}\text{ }}
\newcommand{\ours}{\textsc{Maddness}}
\newcommand{\oursHash}{\textsc{MaddnessHash}}

% \usepackage{flafter}  %

% \usepackage[sorting=ynt]{biblatex}
% \usepackage[sorting=ynt]{natbib}
% \usepackage[sort]{natbib}
% \DeclareSortingScheme{noneyear}{
%  \sort{\citeorder}
%  \sort{\field{year}}
% }

% \usepackage[sorting=ynt]{natbib}

\input{setup.tex}

% optional custom title for header
\icmltitlerunning{Multiplying Matrices Without Multiplying}

\begin{document}

\twocolumn[
% ================================================================
\icmltitle{Multiplying Matrices Without Multiplying}
% ================================================================

\begin{icmlauthorlist}
\icmlauthor{Batman}{WayneEnterprises}
\end{icmlauthorlist}

\icmlaffiliation{WayneEnterprises}{Wayne Enterprises, Gotham, USA}
\icmlcorrespondingauthor{Batman}{batman@batman.batman}

\icmlkeywords{Vector Quantization, Approximate Algorithms, Matrix Multiplication}

\vskip 0.3in
] % end of icml 2 columnn

\printAffiliationsAndNotice{}

% ------------------------------------------------
\begin{abstract}
% ------------------------------------------------

Multiplying matrices is among the most fundamental and most computationally demanding operations in machine learning and scientific computing. Consequently, the task of efficiently approximating matrix products has received significant attention.

We introduce an approximate matrix multiplication algorithm that greatly outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it often runs $10\times$ faster than current methods at a given level of error, as well as $100\times$ faster than exact matrix multiplication. In the common case that one matrix is known ahead of time, our method also has the interesting property that it requires zero multiply-adds. These results suggest that a mixture of hashing, averaging, and byte shuffling—--the core operations of our method—--could be a more promising building block than the sparsified, factorized, and/or scalar quantized matrix products that have been the subject of hundreds of papers and enormous hardware investment in recent years.

\end{abstract}

% ================================================================
\section{Introduction} \label{sec:intro}
% ================================================================

\input{intro.tex}

% ================================================================
% \section{Background and Related Work}
\section{Related Work} \label{sec:relatedWork}
% ================================================================

\input{relatedWork.tex}
% \input{bg.tex}

%================================================================
\vspace{-1.5mm}
\section{Background - Product Quantization} \label{sec:background}
\vspace{-.5mm}
%================================================================

\input{background.tex}

%================================================================
% \vspace{-5mm}
\vspace{-3mm}
\section{Our Method} \label{sec:method}
\vspace{-.5mm}
%================================================================

\input{method.tex}

%================================================================
% \vspace{-2mm}
\section{Experiments} \label{sec:results}
% \vspace{-.5mm}
%================================================================

\input{results.tex}

%================================================================
% \vspace{-2.5mm}
\vspace{-.5mm}
\section{Discussion and Conclusion}
\vspace{-.5mm}
%================================================================

We introduce \ours, an algorithm that achieves up to a $10\times$ better speed-quality tradeoff than existing methods for the well-studied problem of approximate matrix multiplication, as measured on a large, diverse, and challenging set of real-world matrices.
% substantially larger than that used in any previous work.
% , as measured using a large, diverse, and challenging set of real-world matrices.
Our method also features theoretical guarantees, as well as a number of algorithmic contributions likely to be of more general interest. %While the focus of this paper is matrix multiplication, our results suggest that future methods similar to our own might hold promise for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms.

% \ours's efficacy for matrix multiplication suggests that similar methods based on vector quantization and byte shuffling may be promising for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms.

% \vspace{-.25mm}
The main limitation of our work is that we do not demonstrate (or claim) speedups on Google TPUs, recent NVIDIA GPUs, or other accelerators specialized for traditional matrix products. However, 1) such accelerators are a small minority of computational devices, and 2) our work does suggest a possible path for \textit{future} accelerators to be much more efficient. Namely, replacing the multiply-add hardware needed by current approaches with the multi\textit{plex}-add hardware needed for fast table lookups could both save many transistors and likely increase output quality.

% hardware acceleration for the hashing, averaging, and table lookup operations we employ could both increase quality of output and require fewer transistors than current multiplication-based approaches.

% than are needed by multipliers, and would (as we demonstrate) likely

% on which we rely would require fewer transistors than are needed by multipliers, and would (as we demonstrate) likely achieve better quality than current approaches.

%  simultanebe implemented with far fewer transistors than are needed by multipliers, and 2) could (as we demonstrate) approximate linear transforms at least as well traditional approaches like scalar quantization.

% \vspace{-.25mm}
Finally, while the focus of this paper is matrix multiplication and extending it beyond this will require further research, our results suggest that similar methods might hold promise for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms.

% In future work, our results suggest that methods similar to our own might hold promise for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms.

% In future work, our results

% that using methods similar to our own accelerating

 %---though realizing this potential will require a great deal of further research and engineering.

% Our results suggest that methods similar to our own could hold promise for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms, but we emphasize that such extensions will require significant further research and the present work claims superiority only for \textit{approximate matrix multiplication}.
% the present work only claims superiority for \textit{approximate matrix multiplication} as described in our problem formulation. In future work

% Our method's performance stems from 1) its replacement of the bottleneck in existing methods with a learned locality-sensitive hash function, 2) its optimization of a quantity that other methods cannot optimize without a large slowdown, and 3) its use of an inexact estimator for computing sums.
% In future work, we plan to specialize our method for convolutions, implement it on GPUs, and integrate it into neural networks.
% In future work, we plan to integrate our method into neural networks, specialize it for convolutions, and implement it on GPUs.

% ================================================================
% References
% ================================================================

% \IEEEtriggeratref{27} % trigger column break to make cols even
% \bibliographystyle{ACM-Reference-Format}
% \bibliographystyle{abbrev}
% \bibliographystyle{sysml2019}
\bibliographystyle{icml2020}
% \bibliography{prune,architectures,misc,understandDnn,classic,datasets,compress,science,metapapers}


% TODO uncomment

\bibliography{architectures,datasets,misc,sprintz,extract,bolt,backprop-alternatives,distillation,fast-dnn-runtime-stuff,fast-optimize,nets-theory,small-fast-arch,sparseAndPrune,scalarQuantize,vectorQuantize,combo,hashing,shrink+prune,binarize,ternary,automl,zero-shot,few-shot,amm,ammMore,lsh,adversarial,libs,math}
\clearpage
\newpage  % so we can cut the pdf
\appendix
\input{appendix.tex}

\end{document}
