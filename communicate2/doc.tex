%!TEX output_directory = aux

\documentclass{article}  % sysml

\usepackage{booktabs}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
% \usepackage[accepted]{sysml2019}
% \usepackage{sysml2019}
\usepackage{icml2020}

% \newcommand{\npapers}{\text{83 }}
% \newcommand{\rawnpapers}{83}

% \usepackage{flafter}  %

% \usepackage[sorting=ynt]{biblatex}
% \usepackage[sorting=ynt]{natbib}
% \usepackage[sort]{natbib}
% \DeclareSortingScheme{noneyear}{
%  \sort{\citeorder}
%  \sort{\field{year}}
% }

% \usepackage[sorting=ynt]{natbib}

\input{setup.tex}

% optional custom title for header
% \sysmltitlerunning{Multiplying Matrices Without Multiplying}
\icmltitlerunning{Multiplying Matrices Without Multiplying}

\begin{document}

\twocolumn[
% ================================================================
% \title{Have We Actually Learned Anything about \\ Pruning Neural Networks?}
% \sysmltitle{Multiplying Matrices Without Multiplying}
\icmltitle{Multiplying Matrices Without Multiplying}
% \title{Have We Actually Learned Anything about Pruning Neural Networks?}
% ================================================================

% \sysmltitlerunning{Submission and Formatting Instructions for SysML 2019}
% \begin{document}
% \twocolumn[
% \sysmltitle{Submission and Formatting Instructions for SysML 2019}

% \author{Davis W. Blalock}
% \affiliation{
%   \institution{Computer Science and Artificial \\ Intelligence Laboratory}
%   \institution{Massachusetts Institute of Technology}
% }
% \email{dblalock@mit.edu}

% \author{Jose Javier Gonzalez Ortiz}
% \affiliation{
%   \institution{Computer Science and Artificial \\ Intelligence Laboratory}
%   \institution{Massachusetts Institute of Technology}
% }
% \email{jjgo@mit.edu}

% \author{John V. Guttag}
% \affiliation{
%   \institution{Computer Science and Artificial \\ Intelligence Laboratory}
%   \institution{Massachusetts Institute of Technology}
% }
% \email{guttag@mit.edu}

% \begin{sysmlauthorlist}
\begin{icmlauthorlist}
% \sysmlauthor{Davis Blalock}{mit}
% \sysmlauthor{John Guttag}{mit}
\icmlauthor{Davis Blalock}{mit}
\icmlauthor{Tamara Broderick?}{mit}
\icmlauthor{John Guttag}{mit}
% \end{sysmlauthorlist}
\end{icmlauthorlist}

% \sysmlaffiliation{csail}{MIT CSAIL, Cambridge, MA, USA}
% \sysmlcorrespondingauthor{Davis Blalock}{dblalock@mit.edu}
\icmlaffiliation{csail}{MIT CSAIL, Cambridge, MA, USA}
\icmlcorrespondingauthor{Davis Blalock}{dblalock@mit.edu}

% apparently these only show up in pdf metadata, not document
% \sysmlkeywords{Approximate Algorithms, Matrix Multiplication}
\icmlkeywords{Approximate Algorithms, Matrix Multiplication}

\vskip 0.3in
] % end of icml 2 columnn

\printAffiliationsAndNotice{}

% ------------------------------------------------
\begin{abstract}
% ------------------------------------------------

Multiplying matrices is among the most fundamental and most computationally demanding operations in numerical computing. Consequently, the task of efficiently approximating matrix products has received a great deal of attention in recent years. Existing approximation methods work by reducing the number of multiply-add operations, reducing the bitwidth of the data, or by otherwise reducing the cost of each multiply-add.

We describe a method that, given time to preprocess one of the two matrices, \textit{eliminates} the multiply-add operations. Moreover, experiments across hundreds of matrices from diverse domains show that our method is consistently $10\times$ faster than existing approaches for a given level of approximation error. For example, we are able to approximate a softmax classifier on the CIFAR-10 dataset $91\times$ faster than brute force with a $1\%$ loss of accuracy and no retraining.

% \paragraph{Alternate Version}
% Multiplying matrices is among the most fundamental and most computationally demanding operations in machine learning and scientific computing. Consequently, the task of efficiently approximating matrix products has received a great deal of attention in recent years.

% We introduce an approximate matrix multiplication algorithm that significantly outperforms existing methods on hundreds of real-world matrices from diverse domains, often by a factor of $10\times$ or more.
% Our algorithm obtains this speedup by departing from the traditional approaches that \textit{reduce} the number of multiply-add operations, we preprocess one of the matrices such that subsequent multiplies are \textit{eliminated entirely}.

% Current methods work by reducing the number and cost of multiply-add operations through sparsity, factorization, and reduced bitwidth. We introduce a method that, given time to preprocess one of the two matrices, \textit{eliminates the multiply-add operations entirely}.

% Moreover, in contrast to current methods, our algorithm does not reduce the cost of each multiply-add or diminish their number through factorization or sparsity; instead, it \textit{eliminates the multiply-adds} entirely by intelligently preprocessing one of the matrices.


% not reduce the number of scalar multiply-adds,


% after evaluating various approaches across hundreds of matrices from dive,

% Our method also has theoretical guarantees, based on an approach to bounding sums of weakly dependent variables that may be of independent interest.

% obtains a significantly better efficiency vs. quality tradeoff than existing methods across a wide range of problems, metrics, and datasets. Furthermore, in the special case that one matrix is known ahead of time, our method computes matrix products with no multiplication operations, a property that may be of interest to hardware designers.


\end{abstract}
% ]  % end of manual twocolumn for sysml

% \maketitle

% ================================================================
\section{Introduction} \label{sec:intro}
% ================================================================

\input{intro.tex}

%================================================================
\section{Related Work}
%================================================================

\input{relatedWork.tex}

%================================================================
\section{Background - Product Quantization}
%================================================================

\input{background.tex}

%================================================================
\section{Method}
%================================================================

\input{method.tex}

% %================================================================
% \section{Theoretical Analysis}
% %================================================================

% \input{theory.tex}

% %================================================================
% \section{Experiments}
% %================================================================

% \input{results.tex}

%================================================================
% \section{Open Questions and Future Directions}
% %================================================================

% \input{questions.tex}


%================================================================
\section{Conclusion}
%================================================================

We sure did multiply those matrices. In the near future, we plan to integrate our method into the training of deep neural networks.

% sysml says to include acknowledgements in initial submission
% %================================================================
% \section{Acknowledgements}
% %================================================================

% We thank Luigi Celona for providing the data used in \cite{luigi}.
% %``Benchmark Analysis of Representative Deep Neural Network Architectures.'' []

% ================================================================
% References
% ================================================================

% \IEEEtriggeratref{27} % trigger column break to make cols even
% \bibliographystyle{ACM-Reference-Format}
% \bibliographystyle{abbrev}
\bibliographystyle{sysml2019}
% \bibliography{prune,architectures,misc,understandDnn,classic,datasets,compress,science,metapapers}
\bibliography{misc,sprintz,extract,bolt,backprop-alternatives,distillation,fast-dnn-runtime-stuff,fast-optimize,nets-theory,small-fast-arch,sparseAndPrune,scalarQuantize,vectorQuantize,combo,hashing,shrink+prune,binarize,ternary,automl,zero-shot,few-shot,amm,adversarial}

\end{document}
