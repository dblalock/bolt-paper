%!TEX output_directory = aux

\documentclass{article}  % sysml

\usepackage{booktabs}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{icml2020}

% \newcommand{\oursp}{\textsc{HashMul}\text{ }}
% \newcommand{\ours}{\textsc{HashMul}}
% \newcommand{\oursp}{\textsc{MADDNESS}\text{ }}
% \newcommand{\ours}{\textsc{MADDNESS}}
\newcommand{\oursp}{\textsc{Maddness}\text{ }}
\newcommand{\ours}{\textsc{Maddness}}
\newcommand{\oursHash}{\textsc{MaddnessHash}}

% \usepackage{flafter}  %

% \usepackage[sorting=ynt]{biblatex}
% \usepackage[sorting=ynt]{natbib}
% \usepackage[sort]{natbib}
% \DeclareSortingScheme{noneyear}{
%  \sort{\citeorder}
%  \sort{\field{year}}
% }

% \usepackage[sorting=ynt]{natbib}

\input{setup.tex}

% optional custom title for header
\icmltitlerunning{Multiplying Matrices Without Multiplying}

\begin{document}

\twocolumn[
% ================================================================
\icmltitle{Multiplying Matrices Without Multiplying}
% ================================================================

\begin{icmlauthorlist}
\icmlauthor{Batman}{WayneEnterprises}
\end{icmlauthorlist}

\icmlaffiliation{WayneEnterprises}{Wayne Enterprises, Gotham, USA}
\icmlcorrespondingauthor{Batman}{batman@batman.batman}

\icmlkeywords{Vector Quantization, Approximate Algorithms, Matrix Multiplication}

\vskip 0.3in
] % end of icml 2 columnn

\printAffiliationsAndNotice{}

% ------------------------------------------------
\begin{abstract}
% ------------------------------------------------

Multiplying matrices is among the most fundamental and most computationally demanding operations in machine learning and scientific computing. Consequently, the task of efficiently approximating matrix products has received significant attention.

We introduce an approximate matrix multiplication algorithm that greatly outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it often runs $10\times$ faster than current methods at a given level of error, as well as $100\times$ faster than exact matrix multiplication. In the common case that one matrix is known ahead of time, our method also has the interesting property that it requires zero multiply-adds. These results suggest that a mixture of hashing, averaging, and byte shuffling—--the core operations of our method—--could be a more promising building block than the sparsified, factorized, and/or scalar quantized matrix products that have been the subject of hundreds of papers and enormous hardware investment in recent years.

\end{abstract}

% ================================================================
\section{Introduction} \label{sec:intro}
% ================================================================

\input{intro.tex}

% ================================================================
% \section{Background and Related Work}
\section{Related Work} \label{sec:relatedWork}
% ================================================================

\input{relatedWork.tex}
% \input{bg.tex}

%================================================================
\vspace{-1.5mm}
\section{Background - Product Quantization} \label{sec:background}
\vspace{-.5mm}
%================================================================

\input{background.tex}

%================================================================
% \vspace{-5mm}
\vspace{-3mm}
\section{Our Method} \label{sec:method}
\vspace{-.5mm}
%================================================================

\input{method.tex}

%================================================================
% \vspace{-2mm}
\section{Experiments} \label{sec:results}
% \vspace{-.5mm}
%================================================================

\input{results.tex}

%================================================================
\vspace{-2.5mm}
\section{Conclusion}
\vspace{-.5mm}
%================================================================

We introduce an approximate matrix multiplication algorithm that achieves a significantly better speed-quality tradeoff than existing methods, as measured using hundreds of real-world matrices from diverse domains. Our method also features theoretical guarantees, as well as a number of algorithmic contributions likely to be of more general interest. Regarding future work, these results suggest that methods similar to our own might hold promise for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms. %---though realizing this potential will require a great deal of further research and engineering.

% Our results suggest that methods similar to our own could hold promise for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms, but we emphasize that such extensions will require significant further research and the present work claims superiority only for \textit{approximate matrix multiplication}.
% the present work only claims superiority for \textit{approximate matrix multiplication} as described in our problem formulation. In future work

% Our method's performance stems from 1) its replacement of the bottleneck in existing methods with a learned locality-sensitive hash function, 2) its optimization of a quantity that other methods cannot optimize without a large slowdown, and 3) its use of an inexact estimator for computing sums.
% In future work, we plan to specialize our method for convolutions, implement it on GPUs, and integrate it into neural networks.
% In future work, we plan to integrate our method into neural networks, specialize it for convolutions, and implement it on GPUs.

% ================================================================
% References
% ================================================================

% \IEEEtriggeratref{27} % trigger column break to make cols even
% \bibliographystyle{ACM-Reference-Format}
% \bibliographystyle{abbrev}
% \bibliographystyle{sysml2019}
\bibliographystyle{icml2020}
% \bibliography{prune,architectures,misc,understandDnn,classic,datasets,compress,science,metapapers}


% TODO uncomment

\bibliography{architectures,datasets,misc,sprintz,extract,bolt,backprop-alternatives,distillation,fast-dnn-runtime-stuff,fast-optimize,nets-theory,small-fast-arch,sparseAndPrune,scalarQuantize,vectorQuantize,combo,hashing,shrink+prune,binarize,ternary,automl,zero-shot,few-shot,amm,ammMore,lsh,adversarial}
\clearpage
\newpage  % so we can cut the pdf
\appendix
\input{appendix.tex}

\end{document}
