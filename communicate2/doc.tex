%!TEX output_directory = aux

\documentclass{article}  % sysml

\usepackage{booktabs}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{icml2020}

% \newcommand{\oursp}{\textsc{HashMul}\text{ }}
% \newcommand{\ours}{\textsc{HashMul}}
% \newcommand{\oursp}{\textsc{MADDNESS}\text{ }}
% \newcommand{\ours}{\textsc{MADDNESS}}
\newcommand{\oursp}{\textsc{Maddness}\text{ }}
\newcommand{\ours}{\textsc{Maddness}}
\newcommand{\oursHash}{\textsc{MaddnessHash}}

% \usepackage{flafter}  %

% \usepackage[sorting=ynt]{biblatex}
% \usepackage[sorting=ynt]{natbib}
% \usepackage[sort]{natbib}
% \DeclareSortingScheme{noneyear}{
%  \sort{\citeorder}
%  \sort{\field{year}}
% }

% \usepackage[sorting=ynt]{natbib}

\input{setup.tex}

% optional custom title for header
\icmltitlerunning{Multiplying Matrices Without Multiplying}

\begin{document}

\twocolumn[
% ================================================================
\icmltitle{Multiplying Matrices Without Multiplying}
% ================================================================

\begin{icmlauthorlist}
\icmlauthor{Batman}{WayneEnterprises}
\end{icmlauthorlist}

\icmlaffiliation{WayneEnterprises}{Wayne Enterprises, Gotham, USA}
\icmlcorrespondingauthor{Batman}{batman@batman.batman}

\icmlkeywords{Vector Quantization, Approximate Algorithms, Matrix Multiplication}

\vskip 0.3in
] % end of icml 2 columnn

\printAffiliationsAndNotice{}

% ------------------------------------------------
\begin{abstract}
% ------------------------------------------------


% We introduce an approximate matrix multiplication algorithm that greatly outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it often runs $10\times$ faster than current methods at a given level of error, as well as $100\times$ faster than exact matrix multiplication. In the common case that one matrix is known ahead of time, our method also has the interesting property that it requires zero multiply-adds.

Multiplying matrices is among the most fundamental and compute-intensive operations in machine learning. % Fortunately, for many applications, approximate matrix multiplication suffices.
% Consequently, many algorithms use approximate
% and scientific computing.
% Consequently, the task of efficiently approximating matrix products has received significant attention.
% This has led researchers to consider using approximate matrix products.
Consequently, there has been a great deal of research on approximate matrix multiplication (AMM).
% Multiplying matrices is a common

We introduce a learning-based AMM algorithm that greatly outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it often runs $100\times$ faster than exact matrix products and $10\times$ faster than current approximate methods. In the common case that one matrix is known ahead of time,
our method also has the interesting property that it requires zero multiply-adds.

These results suggest that a mixture of hashing, averaging, and byte shuffling—--the core operations of our method—--could be a more promising building block for machine learning than the sparsified, factorized, and/or scalar quantized matrix products that have been the focus of substantial research and hardware investment in recent years. They also suggest that machine learning can be applied at even the lowest level of the software stack---e.g., our matrix product kernels begin by running inference in decision trees.

% These results suggest that a mixture of hashing, averaging, and byte shuffling—--the core operations of our method—--could be a more promising building block than the sparsified, factorized, and/or scalar quantized matrix products that have been the subject of hundreds of papers and enormous hardware investment in recent years.

% Multiplying matrices is among the most fundamental and most computationally demanding operations in machine learning and scientific computing.

% We introduce a new approximate matrix multiplication algorithm that greatly outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it often runs 100× faster than exact matrix multiplication and 10× faster than current approximate methods. In the common case that one matrix is known ahead of time, which occurs when applying a linear model, our method also has the interesting property that it requires zero multiply-adds.

% Our results suggest that a mixture of hashing, averaging, and byte shuffling, the core operations of our method, could be a more promising building block than the sparsified, factorized, and/or scalar quantized matrix products that have been commonly proposed in the literature and have driven much hardware investment in recent years.

\end{abstract}

% ================================================================
\vspace*{-5mm}
\section{Introduction} \label{sec:intro}
\vspace{-.5mm}
% ================================================================

\input{intro.tex}

% ================================================================
% \section{Background and Related Work}
\vspace{-2mm}
\section{Related Work} \label{sec:relatedWork}
\vspace{-.5mm}
% ================================================================

\input{relatedWork.tex}
% \input{bg.tex}

%================================================================
\vspace{-1.5mm}
\section{Background - Product Quantization} \label{sec:background}
\vspace{-.5mm}
%================================================================

\input{background.tex}

%================================================================
% \vspace{-5mm}
\vspace{-3mm}
\section{Our Method} \label{sec:method}
\vspace{-.5mm}
%================================================================

\input{method.tex}

%================================================================
% \vspace{-2mm}
\section{Experiments} \label{sec:results}
% \vspace{-.5mm}
%================================================================

\input{results.tex}

%================================================================
% \vspace{-2.5mm}
% \vspace{-.5mm}
% \vspace*{.5mm}
\section{Discussion and Conclusion}
\vspace{-.5mm}
%================================================================

% We introduce \ours, a learning-based algorithm for the well-studied problem of approximate matrix multiplication (AMM). Experiments using a large, diverse, and challenging set of real-world matrices show that our algorithm achieves up to a $10\times$ better speed-quality tradeoff than existing methods. Our method also features theoretical guarantees regarding.

% that achieves up to a $10\times$ better speed-quality tradeoff than existing methods for the well-studied problem of approximate matrix multiplication (AMM). as measured on a large, diverse, and challenging set of real-world matrices.

We introduce \ours, an algorithm that achieves up to a $10\times$ better speed-quality tradeoff than existing methods for the well-studied problem of approximate matrix multiplication (AMM), as measured on a large, diverse, and challenging set of real-world matrices.
% substantially larger than that used in any previous work.
% , as measured using a large, diverse, and challenging set of real-world matrices.
Our method also features theoretical guarantees, as well as standalone algorithmic contributions that may be of independent interest. In particular, we introduce a family of trainable vector quantization functions that can encode data at over 100GB/s in a single thread and a fast approximate algorithm for summing low-bitwidth integers.

Our approach is a significant departure from existing AMM work in that it relies on hashing and table lookups rather than multiply-add operations. It is also unusual as an \textit{application} of machine learning, inserting learned functions and representations even within low-level kernels.

%While the focus of this paper is matrix multiplication, our results suggest that future methods similar to our own might hold promise for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms.

% \ours's efficacy for matrix multiplication suggests that similar methods based on vector quantization and byte shuffling may be promising for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms.

% \vspace{-.25mm}
The main limitation of our work is that we do not demonstrate (or claim) speedups on Google TPUs, recent NVIDIA GPUs, or other accelerators specialized for traditional matrix products. However, 1) such accelerators are a small minority of computational devices, and 2) our work does suggest a possible path for \textit{future} accelerators to be much more efficient. Namely, replacing the multiply-add hardware needed by current approaches with the multi\textit{plex}-add hardware needed for fast table lookups could both save many transistors and perhaps increase output quality.

Finally, while the focus of this paper is matrix multiplication and extending it beyond this will require further research, our results suggest that similar methods might hold promise for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms.


% % ------------------------

% We introduce MADDNESS, an algorithm that uses averaging, byte shuffling, and learned hash functions to achieve a new state-of-the-art for the well-studied problem of approximate matrix multiplication. %The algorithm is built using two novel   components: 1) a subfamily of vector quantization functions that can encode over $100$GB of data per second on a single CPU thread, and 2) A high-speed algorithm for summing low-bitwidth integers.
% % In addition to providing theoretical guarantees, we evaluate the efficiency and accuracy of our algorithm on
% Experiments using a large, diverse, and challenging set of publicly available data show that our algorithm achieves up to a $10\times$ better speed-quality tradeoff than existing methods.% for approximate matrix multiplication.
% These results are enabled by several algorithmic contributions, including trainable family of hash functions that can encode data at over 100GB/s in a single CPU thread, an efficient means of optimizing vector quantization prototypes, and a fast approximate algorithm for summing low-bitwidth integers.

% % Our method is a significant departure from most existing work on this problem, and

% The main limitation of our work is that we do not demonstrate (or claim) speedups on Google TPUs, recent NVIDIA GPUs, or other accelerators specialized for traditional matrix products. However, 1) such accelerators are a small minority of computational devices, and 2) our work does suggest a possible path for future accelerators to be much more efficient. Namely, replacing the multiply-add hardware needed by current approaches with the multi\textit{plex}-add hardware needed for fast table lookups could both save many transistors and improve output accuracy.


% Finally, while the focus of this paper is matrix multiplication and extending it beyond this will require further research, our results suggest that similar methods might hold promise for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms.

% While the method described in this paper can be applied to a wide variety to applications that use matrix multiplication, the method is particularly well suited for machine learning algorithms that use matrix multiplication. For example, the application of a linear model to test data could be greatly accelerated by exploiting the fact that one of the matrices is known in advance. And while it requires further algorithmic development, our results suggest that similar methods might hold promise for accelerating convolution and deep learning.



% hardware acceleration for the hashing, averaging, and table lookup operations we employ could both increase quality of output and require fewer transistors than current multiplication-based approaches.

% than are needed by multipliers, and would (as we demonstrate) likely

% on which we rely would require fewer transistors than are needed by multipliers, and would (as we demonstrate) likely achieve better quality than current approaches.

%  simultanebe implemented with far fewer transistors than are needed by multipliers, and 2) could (as we demonstrate) approximate linear transforms at least as well traditional approaches like scalar quantization.

% \vspace{-.25mm}

% In future work, our results suggest that methods similar to our own might hold promise for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms.

% In future work, our results

% that using methods similar to our own accelerating

 %---though realizing this potential will require a great deal of further research and engineering.

% Our results suggest that methods similar to our own could hold promise for accelerating convolution, deep learning, and other workloads bottlenecked by linear transforms, but we emphasize that such extensions will require significant further research and the present work claims superiority only for \textit{approximate matrix multiplication}.
% the present work only claims superiority for \textit{approximate matrix multiplication} as described in our problem formulation. In future work

% Our method's performance stems from 1) its replacement of the bottleneck in existing methods with a learned locality-sensitive hash function, 2) its optimization of a quantity that other methods cannot optimize without a large slowdown, and 3) its use of an inexact estimator for computing sums.
% In future work, we plan to specialize our method for convolutions, implement it on GPUs, and integrate it into neural networks.
% In future work, we plan to integrate our method into neural networks, specialize it for convolutions, and implement it on GPUs.

% ================================================================
% References
% ================================================================

% \IEEEtriggeratref{27} % trigger column break to make cols even
% \bibliographystyle{ACM-Reference-Format}
% \bibliographystyle{abbrev}
% \bibliographystyle{sysml2019}
\bibliographystyle{icml2020}
% \bibliography{prune,architectures,misc,understandDnn,classic,datasets,compress,science,metapapers}


% TODO uncomment

\bibliography{architectures,datasets,misc,sprintz,extract,bolt,backprop-alternatives,distillation,fast-dnn-runtime-stuff,fast-optimize,nets-theory,small-fast-arch,sparseAndPrune,scalarQuantize,vectorQuantize,combo,hashing,shrink+prune,binarize,ternary,automl,zero-shot,few-shot,amm,ammMore,lsh,adversarial,libs,math,metapapers}
\clearpage
\newpage  % so we can cut the pdf
\appendix
\input{appendix.tex}

\end{document}
