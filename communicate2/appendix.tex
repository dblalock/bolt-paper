
% ================================================================
\section{Quantizing Lookup Tables} \label{sec:lutQuantize}
% ===============================================================

% Existing work has shown that setting $K = 16$ and quantizing the lookup tables for a given column of $\B$ to 8 bits can offer enormous speedups compared to larger $K$ and/or floating-point tables \cite{bolt, quickAdc, quickerAdc}. This is because 16 1-byte entries can be stored in a SIMD register, allowing 16 or more table lookups to be performed in parallel in a single instruction.

Since the lookup table entries naturally occupy more than 8 bits even for 8-bit data (since products of 8-bit values require 16 bits), some means of quantizing these entries is necessary to enable vectorization. Unfortunately, existing quantization methods are not applicable to our problem setting. The scheme of \citet{bolt} requires knowledge of $\B$ at training time, while the scheme of \citet{quickAdc} and \citet{quickerAdc} is only applicable for nearest-neighbor search. We instead use the following approach, where $\mat{T} \in \R^{M \times C \times K}$ is the tensor of lookup tables for all $M$ columns of $\B$, $\mat{T}^{q}$ is the quantized version of $\mat{T}$, $\vec{\delta} \in \R^C$ is a vector of table-specific offsets, and $\alpha^{-1}$ is an overall scale factor:
\begin{align}
    \vec{\delta}_c &\triangleq \min_{m,k} \text{ } \mat{T}_{m,c,k} \\
    \alpha^{-1} \triangleq 2^l \cs l &= \max_c \floor{ \log_2 \left( \frac{255}{
            \max_{m,k} (\mat{T}_{m,c,k} - \delta_{c})
        } \right)}. \\
    \mat{T}^{q}_{m,c,k} &\triangleq \alpha^{-1}(\mat{T}_{m,c,k} - \delta_{c})
\end{align}
This is similar to equations~\ref{eq:offset} and \ref{eq:scale}, but with the scale factor pooled across all codebooks instead of unique to each input column. The $\alpha$ used here is the same as that in equation~\ref{eq:objective}, and the matrix $\beta$ in equation~\ref{eq:objective} has entries equal to $\sum_c \vec{\delta}_c$ (plus the debiasing constant from our averaging-based aggregation).

% ================================================================
\section{Quantization and \oursHash} \label{sec:hashQuantize}
% ================================================================

The use of at most 16 leaves is so that the resulting codes use 4 bits. This allows the use of these same shuffle instructions to accelerate the table lookups as in \citet{bolt}.

The only subtlety in vectorizing our hash function is that one must execute line 4 using shuffle instructions such as \texttt{vpshufb} on x86, \texttt{vtbl} on ARM, or \texttt{vtbl} on PowerPC. In order to do this, the split values and scalars $x_{j^t}$ must be 8-bit integers. We quantize them by learning for each split index $j$ a pair of scalars $(\gamma_j, \delta_j)$, where
\begin{align}
    \delta_j &\triangleq \min_i \v^j_i \label{eq:offset} \\
    % \gamma_j \triangleq 2^l, l = \left \lfloor \text{log2} \left( \frac{255}{\max_i \v^j_i - \delta_j} \right) \right \rfloor
    \gamma_j &\triangleq 2^l, l = \floor{ \log_2 \left( \frac{255}{\max_i \v^j_i - \delta_j} \right) \label{eq:scale} }
\end{align}
% and
% \begin{align}
% %     &\min_i \v^j_i - \delta_j = 0 \\
% %     &\max_i \gamma_j(\v^j_i - \delta_j) \le 255
% \end{align}
% and $\gamma_j$ is a power of 2.
This restriction of $\gamma_j$ to powers of two allows one to quantize $x_{j^t}$ values with only shifts instead of multiplies. The $\vec{v}$ values can be quantized at the end of the training phase, while the $x_{j^t}$ values must be quantized within Algorithm~\ref{algo:ourEnc} before line 5.

% ================================================================
\vfill\break  % columnbreak
\section{Subroutines for Training \oursHash} \label{sec:optimalSplitVal}
% ================================================================

The \texttt{optimal\_split\_threshold} algorithm finds the best threshold at which to split a bucket within a given dimension. To do this, it uses the \texttt{cumulative\_sse} function (\ref{algo:cumSSE}) to help evaluate the loss associated with the resulting child buckets.

These algorithms exploit the fact that the sum of squared errors can be computed using only the sum of values and sum of squared values, both of which can be updated in $O(1)$ time when a vector is moved from one side of the split to the other.

% ------------------------------------------------ optimalSplitVal

\begin{algorithm}[h]
\caption{Optimal Split Threshold Within a Bucket} \label{algo:optimalSplitVal}
\begin{algorithmic}[1]
    \STATE {\bfseries Input:} bucket $\mathcal{B}$, index $j$
    \STATE {$\mat{X} \leftarrow \texttt{as\_2d\_array}(\mathcal{B})$ }
    \STATE {$\mat{X}^{sort} = \texttt{sort\_rows\_based\_on\_col}(\mat{X} \text{, } j)$}
    \STATE {$\texttt{sses\_head} \leftarrow \texttt{cumulative\_sse}(\mat{X}^{sort}, \texttt{false}) $}
    \STATE {$\texttt{sses\_tail} \leftarrow \texttt{cumulative\_sse}(\mat{X}^{sort}, \texttt{true}) $}
    \STATE {$\texttt{losses} \leftarrow \texttt{sses\_head} $}
    \STATE {$\texttt{losses}_{1:N-1} \leftarrow \texttt{losses}_{1:N-1} + \texttt{sses\_tail}_{2:N} $}
    % \STATE {$ n^\ast \leftarrow \min_n \sum_{d=1}^D $}
    % \STATE {$ n^\ast \leftarrow \argmin_n \texttt{sses\_head}_n + \texttt{sses\_tail}_{n+1} $}
    \STATE {$ n^\ast \leftarrow \argmin_n \texttt{losses}_n $}
    \STATE{$ \textbf{return } (\mat{X}^{sort}_{n^\ast \text{, } j} + \mat{X}^{sort}_{n^\ast + 1 \text{, } j}) / 2 \text{, } \texttt{losses}_{n^\ast} $}
    % \STATE {$\texttt{sses\_tail} \leftarrow \texttt{reverse\_row\_order}(\texttt{cumsse\_cols}(\texttt{reverse\_row\_order}(\mat{X}_{sort})_) $}
    % \STATE {$\texttt{sses\_tail} \leftarrow \texttt{reverse\_row\_order}(\texttt{cumsse\_cols}(\texttt{reverse\_row\_order}(\mat{X}_{sort})_) $}

\end{algorithmic}
\end{algorithm}

% ------------------------------------------------ cumSSE

\begin{algorithm}[h]
% \caption{Cumulative SSE Within Columns} \label{algo:cumSSE}
\caption{Cumulative SSE} \label{algo:cumSSE}
\begin{algorithmic}[1]
    \STATE {\bfseries Input:} 2D array $\mat{X}$, boolean \texttt{reverse}
    \STATE {$N, D \leftarrow \texttt{shape}(\mat{X})$}
    \IF {\texttt{reverse}}
        \STATE{$ \forall_i \texttt{ swap}(\mat{X}_{i,d}, \mat{X}_{N-i+1,d}) $}
    \ENDIF

    % \STATE {$\texttt{out} \leftarrow \texttt{empty}(N \text{, } D)$}
    \STATE {$\texttt{out} \leftarrow \texttt{empty}(N)$}
    \STATE {$\texttt{cumX} \leftarrow \texttt{empty}(D)$}
    \STATE {$\texttt{cumX2} \leftarrow \texttt{empty}(D)$}

    \LINECOMMENT{Initialize first row of output and cumulative values}
    \STATE{$\texttt{out}_{1} \leftarrow 0 $}
    \FOR{$d \leftarrow 1 \textbf{ to } D $}
        \STATE{$\texttt{cumX}_d \leftarrow X_{1, d} $}
        \STATE{$\texttt{cumX2}_d \leftarrow (X_{1, d})^2 $}
        % \STATE{$\texttt{out}_{1, d} \leftarrow 0 $}
    \ENDFOR

    \LINECOMMENT{Compute remaining output rows}
    \FOR{$n \leftarrow 2 \textbf{ to } N $}
        \STATE{$\texttt{out}_{n} \leftarrow 0 $}
        \FOR{$d \leftarrow 1 \textbf{ to } D $}
            \STATE{$\texttt{cumX}_d \leftarrow \texttt{cumX}_d + X_{1, d} $}
            \STATE{$\texttt{cumX2}_d \leftarrow \texttt{cumX2}_d + (X_{1, d})^2 $}
            % \STATE{$\texttt{meanX} \leftarrow \texttt{cumX}_d / n $}
            % \STATE{$\texttt{out}_{n, d} \leftarrow \texttt{cumX2}_d - (\texttt{cumX}_d \times \texttt{cumX}_d / n)$}
            \STATE{$\texttt{out}_{n} \leftarrow \texttt{out}_{n} + \texttt{cumX2}_d - (\texttt{cumX}_d \times \texttt{cumX}_d / n)$}
        \ENDFOR
    \ENDFOR
    \STATE{\textbf{return } \texttt{out}}
\end{algorithmic}
\end{algorithm}

% ================================================================
\vfill\break  % columnbreak
\section{Analysis of Aggregation Using Pairwise Averages} \label{sec:aggregateAnalysis}
% ================================================================


\begin{theorem}[Bias and Variance of Sum Estimator]
Let $\hat{s}(\x), \x \in \R^C, C \text{ \% } U = 0$ be the sum estimator described in section~\ref{sec:aggregate} and let $s(\x) \triangleq \sum_c x_c$. Further suppose that the low bits of any two scalars averaged are drawn from a Bernoulli(.5) distribution. Then $E[s(\x) - \hat{s}(\x)] = C \log_2(U) / 4$ and $E[(s(\x) - \hat{s}(\x))^2] = C \log_2(U) / 8$.
\end{theorem}

\begin{proof}
TODO
\end{proof}


% ================================================================
\vfill\break  % columnbreak
\section{Additional experimental details} \label{sec:experimentDetails}
% ================================================================

% ------------------------------------------------
\subsection{Choice of Matrix Multiplication Tasks}
% ------------------------------------------------

Because nearly all existing work on approximate matrix multiplication either focuses on special cases that do not satisfy our problem definition \cite{quickerAdc, pq, opq} or synthetic matrices, there is not a clear set of benchmark matrix multiply tasks to use. We therefore propose a collection of tasks that we believe are both reproducible and representative of many real-world matrices. To the best of our knowledge, our experiments use over an order of magnitude more matrices than any previous study.


% ------------------------------------------------
\subsection{SparsePCA details}
% ------------------------------------------------

We took steps to ensure that SparsePCA's results were not hampered by insufficient hyperparameter tuning. First, for each matrix product, we tried a range of lambda values which we found to encompass the full gamut of nearly 0\% to nearly 100\% sparsity: $\lambda \in 2^i, i \in \{-5, -4, -3, -2, -1, 0, 1, 2, 3\}$. Second, because different sparsity patterns may yield different exeuction times, we report not times from the single matrix SparsePCA produces for a given ($d, \lambda$) pair, but the best times from any of ten random matrices of the same size and at most the same sparsity. Finally and most importantly, we plot only the Pareto frontier of (speed, quality) pairs produced for a given matrix multiply. I.e., we let SparsePCA cherrypick its best results on each individual matrix multiply.

% ------------------------------------------------
\subsection{Exact Matrix Multiplication}
% ------------------------------------------------

We also implemented our own matrix product function specialized for tall, skinny matrices. In all cases, we report the timings based on the better of these two functions for a given matrix product.

% ------------------------------------------------
\subsection{Additional Baselines}
% ------------------------------------------------

We also tested Frequent Directions / Fast frequent directions \cite{liberty_simple_2012, ghashami_frequent_2016, isvd}, many variations of the sampling method of \cite{drineas_fast_2006}, projection using orthogonalized gaussian random matrices \cite{superbitLSH}, projection using matrices of scaled i.i.d. rademacher random variables \cite{rademacherJL}, projection using orthonormalized matrices of rademacher random variables, the co-occuring directions sketch \cite{mroueh_co-occuring_2016}, OSNAP \cite{osnap}, Product Quantization \cite{pq}, and Optimized Product Quanization \cite{opq}.

The poor performance of many of these methods is unsurprising in our setting. Given that we have access to a training set on which to learn the true principal components, the Eckert-Young-Mirsky theorem \cite{eckartYoungMirskyThm} suggests that PCA should outperform any other matrix sketching method employing dense projection matrices. Also, since PQ and OPQ use 256 dense centroids (except in the Bolt / QuickerADC variations), it is also not possible for them to perform well when $\min(D, M)$ is not significantly larger than 256.

% ------------------------------------------------
\subsection{UCR Time Series Archive}
% ------------------------------------------------

We omitted datasets with fewer than 128 training examples, since it is not possible for Stochastic Neighbor Compression to draw 128 samples without replacement in this case.

In addition to being a large, public corpus of over a hundred datasets from a huge variety of different domains, UCR Time Series Archive also has the advantage that it can be used to produce matrix multiplication tasks of a fixed size. This is necessary for meaningful comparison of performance across datasets. We constructed training and test matrices $\tilde{\A}$ and $\A$ by resampling each time series in each dataset's train and test set to a length of $320$ (the closest multiple of 32 to the median length of 310). We obtained the matrix $\B$ for each dataset by running Stochastic Neighbor Compression \cite{snc} on the training set with an RBF kernel of bandwidth one. We set the number of returned neighbors to 128 (results with 64 and 256 were similar), yielding a $\B$ matrix of size $320 \times 128$. %See Appendix~\ref{sec:experimentDetails} for additional details.

Since different datasets have different test set sizes, all results are for a standardized test set size of 1000 rows. We wanted the length to be a multiple of 32 since existing methods operate best with sizes that are either powers of two or, failing that, multiples of large powers of two.



% since having a length that is a multiple of at least 8 is the best-case scenario for most existing methods, and 320 is a ``rounder'' number than 312.

% ------------------------------------------------
\subsection{Caltech101}
% ------------------------------------------------

We only extracted full windows---i.e., never past the edge of an image. We extracted the windows in CHW order, meaning that scalars from the same color channel were placed at contiguous indices. The ``first'' images are based on filename in lexicographic order.

We used pairs of filters because using a single filter would mean timing a matrix-vector product instead of a matrix-matrix product.

To allow meaningful speed comparisons across images, we resized and center cropped each image to $224 \times 224$ as commonly done in image classification pipelines \cite{resNet,resnet2,densenet}. We then extracted sliding windows of the appropriate size and used each (flattened) window as one row of $\tilde{\A}$ or $\A$. We similarly flattened the filters, with each set of coefficients forming one column of $\B$. In both cases, $\B$ has two columns---this is because using a single filter would mean timing a matrix-vector product instead of a matrix-matrix product. Moreover, because Sobel filters are often used in horizontal and vertical pairings, and Gaussian filters are often used together to perform difference-of-Gaussians transforms.

Even though the RGB values at each position are naturally unsigned 8-bit integers, we allowed all rival methods to operate on them as 32-bit floating point, without including the conversion when timing them. Because it only requires checking whether values are above a threshold, \oursp can operate on 8-bit data directly.
