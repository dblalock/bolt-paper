
@article{soulie_compression_2015,
	title = {Compression of {Deep} {Neural} {Networks} on the {Fly}},
	url = {http://arxiv.org/abs/1509.08745},
	abstract = {Thanks to their state-of-the-art performance, deep neural networks are increasingly used for object recognition. To achieve the best results, they use millions of parameters to be trained. However, when targetting embedded applications the size of these models becomes problematic. As a consequence, their usage on smartphones or other resource limited devices is prohibited. In this paper we introduce a novel compression method for deep neural networks that is performed during the learning phase. It consists in adding an extra regularization term to the cost function of fully-connected layers. We combine this method with Product Quantization (PQ) of the trained weights for higher savings in storage consumption. We evaluate our method on two data sets (MNIST and CIFAR10), on which we achieve signiﬁcantly larger compression rates than state-of-the-art methods.},
	language = {en},
	urldate = {2018-11-07},
	journal = {arXiv:1509.08745 [cs]},
	author = {Soulié, Guillaume and Gripon, Vincent and Robert, Maëlys},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.08745},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {Soulié et al. - 2015 - Compression of Deep Neural Networks on the Fly.pdf:/Users/davis/Zotero/storage/69W7RTNV/Soulié et al. - 2015 - Compression of Deep Neural Networks on the Fly.pdf:application/pdf}
}

@article{agustsson_soft--hard_nodate,
	title = {Soft-to-{Hard} {Vector} {Quantization} for {End}-to-{End} {Learning} {Compressible} {Representations}},
	abstract = {We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.},
	language = {en},
	author = {Agustsson, Eirikur and Mentzer, Fabian and Tschannen, Michael and Cavigelli, Lukas and Timofte, Radu and Benini, Luca},
	pages = {11},
	file = {Agustsson et al. - Soft-to-Hard Vector Quantization for End-to-End Le.pdf:/Users/davis/Zotero/storage/LNKX9RHQ/Agustsson et al. - Soft-to-Hard Vector Quantization for End-to-End Le.pdf:application/pdf}
}

@article{gong_compressing_2014,
	title = {Compressing {Deep} {Convolutional} {Networks} using {Vector} {Quantization}},
	url = {http://arxiv.org/abs/1412.6115},
	abstract = {Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classiﬁcation and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classiﬁcation task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1\% loss of classiﬁcation accuracy using the state-of-the-art CNN.},
	language = {en},
	urldate = {2018-11-07},
	journal = {arXiv:1412.6115 [cs]},
	author = {Gong, Yunchao and Liu, Liu and Yang, Ming and Bourdev, Lubomir},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6115},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {Gong et al. - 2014 - Compressing Deep Convolutional Networks using Vect.pdf:/Users/davis/Zotero/storage/68GN4R85/Gong et al. - 2014 - Compressing Deep Convolutional Networks using Vect.pdf:application/pdf}
}