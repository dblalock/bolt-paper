
Because our work draws on ideas from randomized algorithms, approximate matrix multiplication, vector quantization, and other fields, the body of work related to our own is vast. We therefore seek only to provide only a high-level overview, and refer the interested reader to \cite{learningToHashSurvey, hashingSimilaritySurvey, isvd} for more detailed surveys. We also defer discussion of related vector quantization methods to the following sections, since it is easier to appreciate how they differ from our own method once our method has been introduced.

\subsection{Linear Approximation}

Most AMM methods work by projecting $\A$ and $\B$ into lower-dimensional spaces and then performing an exact matrix multiply.
%  $\V_A, \V_B \in \R^{D \times d}, d \ll D$ such that
% % \begin{align}
% %     \A \B \approx (\A \V_A) (\V_B^\top \B).
% % \end{align}
% There are many options for choosing $\V_A$ and $\V_B$
%, depending on what assumptions one makes. The most common case studied is that in which $N$, $D$, and $M$ are large, but one has little or no prior knowledge about either matrix.
% One simple approach is to choose them such that $(\A \V_A)$ and $(\V_B^\top \B)$ preserve certain structure in $\A$ and $\B$---i.e. to sketch each matrix independently.

One simple option for choosing the projection matrices is to use existing matrix sketching algorithms. The most prominent deterministic matrix sketching methods are the Frequent Directions algorithm \cite{liberty_simple_2012, ghashami_frequent_2016} and its many variations \cite{teng_fast_2019, francis_practical_2018, ye_frequent_2016, huang_near_2019, luo_robust_2019, francis_improvement_2018}. There are also many randomized sketching methods \cite{sarlos_improved_2006, kyrillidis_approximate_2014, pagh_compressed_2013, hashjl,osnap} and random sampling-based methods \cite{drineas_fast_2006-1, drineas_fast_2006-2}.

A weakness of matrix sketching methods in the context of matrix multiplication is that they only consider each matrix in isolation. The first authors to formally address AMM by considering both matrices were \citet{drineas_fast_2006}, who sampled columns of $\A$ and rows of $\B$ according to a sampling distribution dependent upon both matrices. Later work by \citet{manne_fast_2014} reduces approximation of the matrices to an optimization problem, which is solved by steepest descent. \citet{mroueh_co-occuring_2016}, \citet{ye_frequent_2016}, and \citet{francis_improvement_2018} introduce variations of the Frequent Directions algorithm that take into account both matrices simultaneously.

All of the above methods differ from our own not merely in specifics, but also in problem formulation. These methods all assume that there is no training set $\tilde{\A}$ and all focus on large matrices, where provably reduced asymptotic complexity for a given level of error is the goal. % We assume smaller matrices and the presence of a training set.

\subsection{Hashing to Avoid Linear Operations}

In the neural network acceleration literature, there have been several efforts to accelerate dense linear layers using some form of hashing \cite{springScalable,slide,wtaSoftmax,googleWtaCvpr,hashnet}. These methods differ from our own in the hash functions chosen, the inability to exploit a training set, and in the overall goal of the algorithm. While we seek to approximate the entire output matrix, these methods seek to either sample outputs \cite{springScalable,slide}, approximate only the largest outputs \cite{wtaSoftmax,googleWtaCvpr}, or implement a fixed, sparse linear operator \cite{hashnet}.


