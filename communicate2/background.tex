
Recall that our task is to construct functions $g(\cdot)$, $h(\cdot)$, and $f(\cdot)$ such that
\begin{align}
    \norm{f(g(\A), h(\B)) - \A\B}_F < \eps(\tau) \norm{\A\B}_F
\end{align}
for the smallest $\eps(\tau)$ possible.

In this section, we discuss the vector quantization (VQ) approach to approximate matrix multiplication. Because these methods approximate matrix products by approximating individual dot products between rows of $\A$ and columns of $\B$, we do so by examining how they approximate a single dot product $\a^\top \b$ between one row of $\A$ and one column of $\B$. For simplicity, we will also begin by focusing on Product Quantization (PQ) \cite{pq}, the classic algorithm on which most others are based.

The basic intuition behind PQ is that $\a^\top \b \approx \hat{\a}^\top \b$, where $\norm{\hat{\a} - \a}$ is small but $\hat{\a}$ has special structure allowing the product to be computed quickly. This structure consists of $\hat{\a}$ being formed by concatenating learned prototypes in disjoint subspaces; one obtains a speedup by precomputing the dot products between $\b$ and the prototypes once, and then reusing these values across many $\a$ vectors.

 % \cite{pq}, a classic approach to this problem that serves as a conceptual foundation for our own method. There are three basic steps behind PQ:
In somewhat more detail, PQ consists of the following:
\begin{enumerate}
    % \item asdf
    % \item $g(\A)$
    % \item Replacing each row of $\A$ with a prototype chosen from a predefined set.%
    \item \textbf{Prototype Learning} - In an initial, offline training phase,  cluster the rows of $\A$ (or a training set $\tilde{\A}$) using K-means to create prototypes. A separate K-means is run in each of $C$ disjoint subspaces to produce $C$ sets of $K$ protypes. %The prototypes consist of the cartesian product of prototypes within disjoint subspaces.
    % \item $g(\A)$ - Replace each row of $\A$ with the most similar prototype.
    % \item $h(\B)$ - Precompute the dot products between each column of $\B$ and each prototype.
    \item \textbf{Prototype Assignment}, $g(\a)$ - Determine the most similar prototype to $\a$ in each subspace. Store these assignments as integer indices using $C \log_2(K)$ bits.
    \item \textbf{Table Construction}, $h(\B)$ - Precompute the dot products between $\b$ and each prototype in each subspace. Store these partial dot products in $C$ lookup tables of size $K$.
    \item \textbf{Aggregation}, $f(\cdot,\cdot)$ - Use the indices and tables to \textit{lookup} the estimated partial $\a^\top \b$ in each subspace, then sum the results across all $C$ subspaces. %$A[i]^\top B[:, j]$\footnote{Because we will frequently need to refer to slices of rank-3 tensors and above, we employ Python-style slicing notation rather than traditional superscripts and subscripts.} rather than compute it with a series of multiply-adds.
\end{enumerate}

PQ is depicted for a single pair of vectors $\a$ and $\b$ in Figure~\ref{fig:pq}. We elaborate upon each of these steps below.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{amm/pq}
\caption{Product Quantization, with prototype vectors already learned. The $g()$ function returns the index of the most similar prototype to the data vector a in each subspace. The $h(\cdot)$ function computes a lookup table of dot products between the query vector b and each protoype in each suspace. The aggregation function $f(\cdot,\cdot)$ sums the table entries in each subspace corresponding the the most similar prototype.}
% $\vec{a}$
\label{fig:pq}
\end{center}
\end{figure}

% ------------------------------------------------
\subsection{Prototype Learning}
% ------------------------------------------------

Let $\tilde{A} \in \R^{N \times D}$ be a training set, $K$ be a number of prototypes per subspace, $C$ be a number of subspaces, and $\{\mathcal{J}^{(c)}\}_{c=1}^C$ be the (mutually exclusive and collectively exhaustive) sets of indices associated with each subspace. The training-time task of PQ is to learn $C$ sets of prototypes $\mat{P}^{(c)} \in \R^{K \times |\mathcal{J}_c|}$ and assignments $\vec{z}^{(c)} \in \R^{N}$ such that:
\begin{align}
    \sum_{i=1}^N \sum_{c=1}^C \sum_{j\in \mathcal{J}_c} \tilde{\A}_{ij} - \mat{P}^{(c)}_{z^{(c)}},j
\end{align}
is minimized. It does this by running K-means separately in each subspace $\mathcal{J}$ and using the resulting centroids and assignments to populate $\mat{P}$ and $\vec{z}$.

% Perhaps the simplest means of constructing prototypes would be to run a clustering algorithm like K-means on the rows of $\A$. This would be effective, but would require using a large number of prototypes in order to have each row closely match its prototype. What would be preferable is some means of having a huge number of prototypes without having to pay so large a time and space cost.

% PQ achieves this goal by using as prototypes the cartesian product of prototypes within disjoint subspaces. Concretely, PQ runs K-means with $K$ centroids in each of $C$ disjoint (usually contiguous) sets of dimensions. This results in $K^C$ possible combinations of centroid assignments for each vector, with each unique combination corresponding to a unique overall prototype.

% vectors into disjoint ``subvectors'' (each corresponding to a unique set of dimensions) and runs k-means within each subspace.
%  If there are $K$ prototypes per subspace and $C$ subspaces, this results in $K^C$ possible combinations of prototype assignments for each vector. Viewed differently, this creates $K^C$ overall prototypes whose entries

% ------------------------------------------------
\subsection{Encoding Function - $g(\A)$}
% ------------------------------------------------

Given the learned prototypes, PQ replaces each row $\a$ of $\A$ with the concatenation of its $C$ K-means centroid assignments in each of the $C$ subspaces. Formally:
\begin{align} \label{eq:pqLoss}
    g(\a)^{(c)} \triangleq \argmin_k \sum_{j\in \mathcal{J}_c} (\a_j - \mat{P}^{(c)}_{k,j})^2
\end{align}
We will refer to the resulting sequence of indices as the \textit{encoding} of $\a$ and the set of $K$ centroids as a \textit{codebook}. This encoding function has complexity $\Theta(KD)$ for each $\a$, since there are $K$ centroids per subspace and the sum of the number of indices in all subspaces is $D$.

% ------------------------------------------------
\subsection{Table Construction - $h(\B)$}
% ------------------------------------------------

Using these same prototypes, PQ constructs a lookup table $h(\b)^{(c)} \in \R^K$ in each of the $C$ subspaces for each column $\b$ of $\B$, where
\begin{align}
    h(\b)^{(c)} \triangleq \sum_{j\in \mathcal{J}_c} \b_j \mat{P}^{(c)}_{k,j}
\end{align}
This has complexity $\Theta(KD)$ for each $\b$, since it is essentially the same as $g(\cdot)$ except without the $\argmin$ operation.

% ------------------------------------------------
\subsection{Aggregation - $f(\cdot,\cdot)$}
% ------------------------------------------------

Given the encoding of $\a$ and the lookup tables for $\b$, the product can be approximated as
\begin{align}
    \a^\top \b \approx \sum_{c=1}^C h(\b)^{(c)}_k, \text{ }k = g(\a)^{(c)}
\end{align}

This has complexity $\Theta(C)$ for each pair of vectors, or $\Theta(NMC)$ for the full matrix product $\A\B$. Since $C \ll D$, this provides a large speedup when $N$ and $M$ are large enough to amortize the costs of $g(\cdot)$ and $h(\cdot)$.

% % ------------------------------------------------
% \subsection{Complexity and Extensions}
% % ------------------------------------------------

% The total time complexity of PQ is equal to $\Theta(NDK + DKC + NMC)$. When $N$ and $M$ are large, the last term dominates and the complexity of PQ is essentially $\Theta(NMC)$.
% Assuming one does not use Strassen's or other algorithms that are never used in practice in modern numerical libraries, full matrix multiplication has complexity $\Theta(NDM)$. Thus, PQ offers a speedup of $\Theta(\frac{D}{C})$.

% To increase this ratio, many authors have attempted to construct more elaborate $g(\cdot)$ and $h(\cdot)$ functions. One line of work seeks to preprocess $\a$ and/or $\b$ with rotation matrices \cite{opq,cartesianKmeans,lopq}. Another line of work seeks to relax the restriction that the codebooks use disjoint subspaces; several authors \cite{aq,cq,otq,sq,grvq,stackedQuantizers} have observed that equation~\ref{eq:pqLoss} is a special case of
% \begin{align} \label{eq:nonOrthogonal}
%     \sum_{i=1}^N min_{\{z^{(1)},\ldots,z^{(C)}\}} \sum_{j=1}^D \left( \tilde{A}_{ij} - \sum_{c=1}^C \mat{P}^{(c)}_{z^{(c)},j} \right) ^2.
% \end{align}
% That is, all $C$ codebooks can contribute to approximating any dimension of $\a$, instead of only one codebook being responsible for it. This is strictly more expressive, but also slows down $g(\cdot)$ greatly and complicates training. There has been effort to reduce these burdens by carefully ordering update and assignment operations \cite{stackedQuantizers}, adding sparsity \cite{sq}, or structuring overlap between codebooks so that optimal assignments remain possible \cite{otq}. TODO sentence about optimizing LUTs directly.
