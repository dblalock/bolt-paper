



Recall that our task is to construct functions $g(\cdot)$, $h(\cdot)$, and $f(\cdot)$ such that
\begin{align}
    \norm{f(g(\A), h(\B)) - \A\B}_F < \eps(\tau) \norm{\A\B}_F
\end{align}
for the smallest $\eps(\tau)$ possible.

In this section, we discuss the vector quantization (VQ) approach to approximate matrix multiplication. Because these methods approximate matrix products by approximating individual dot products between rows of $\A$ and columns of $\B$, we do so by examining how they approximate a single dot product $\a^\top \b$ between one row of $\A$ and one column of $\B$. For simplicity, we will also begin by focusing on Product Quantization (PQ) \cite{pq}, the classic algorithm on which most others are based.

The basic intuition behind PQ is that $\a^\top \b \approx \hat{\a}^\top \b$, where $\norm{\hat{\a} - \a}$ is small but $\hat{\a}$ has special structure allowing the product to be computed quickly. This structure consists of $\hat{\a}$ being formed by concatenating learned prototypes in disjoint subspaces; one obtains a speedup by precomputing the dot products between $\b$ and the prototypes once, and then reusing these values across many $\a$ vectors.

 % \cite{pq}, a classic approach to this problem that serves as a conceptual foundation for our own method. There are three basic steps behind PQ:
In somewhat more detail, PQ consists of the following:
\begin{enumerate}
    % \item asdf
    % \item $g(\A)$
    % \item Replacing each row of $\A$ with a prototype chosen from a predefined set.%
    \item \textbf{Prototype Learning} - In an initial, offline training phase,  cluster the rows of $\A$ (or a training set $\tilde{\A}$) using K-means to create prototypes. A separate K-means is run in each of $C$ disjoint subspaces to produce $C$ sets of $K$ protypes. %The prototypes consist of the cartesian product of prototypes within disjoint subspaces.
    % \item $g(\A)$ - Replace each row of $\A$ with the most similar prototype.
    % \item $h(\B)$ - Precompute the dot products between each column of $\B$ and each prototype.
    \item \textbf{Prototype Assignment}, $g(\a)$ - Determine the most similar prototype to $\a$ in each subspace. Store these assignments as integer indices using $C \log_2(K)$ bits.
    \item \textbf{Table Construction}, $h(\B)$ - Precompute the dot products between $\b$ and each prototype in each subspace. Store these partial dot products in $C$ lookup tables of size $K$.
    \item \textbf{Aggregation}, $f(\cdot,\cdot)$ - Use the indices and tables to \textit{lookup} the estimated partial $\a^\top \b$ in each subspace, then sum the results across all $C$ subspaces. %$A[i]^\top B[:, j]$\footnote{Because we will frequently need to refer to slices of rank-3 tensors and above, we employ Python-style slicing notation rather than traditional superscripts and subscripts.} rather than compute it with a series of multiply-adds.
\end{enumerate}

PQ is depicted for a single pair of vectors $\a$ and $\b$ in Figure~\ref{fig:pq}. We elaborate upon each of these steps below.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{amm/pq}
\caption{Product Quantization, with prototype vectors already learned. The $g()$ function returns the index of the most similar prototype to the data vector a in each subspace. The $h(\cdot)$ function computes a lookup table of dot products between the query vector b and each protoype in each suspace. The aggregation function $f(\cdot,\cdot)$ sums the table entries in each subspace corresponding the the most similar prototype.}
% $\vec{a}$
\label{fig:pq}
\end{center}
\end{figure}

% ------------------------------------------------
\subsection{Prototype Learning}
% ------------------------------------------------

Let $\tilde{A} \in \R^{N \times D}$ be a training set, $K$ be a number of prototypes per subspace, $C$ be a number of subspaces, and $\{\mathcal{J}^{(c)}\}_{c=1}^C$ be the (mutually exclusive and collectively exhaustive) sets of indices associated with each subspace. The training-time task of PQ is to learn $C$ sets of prototypes $\mat{P}^{(c)} \in \R^{K \times |\mathcal{J}_c|}$ and assignments $\vec{z}^{(c)} \in \R^{N}$ such that:
\begin{align}
    \sum_{i=1}^N \sum_{c=1}^C \sum_{j\in \mathcal{J}_c} \tilde{\A}_{ij} - \mat{P}^{(c)}_{z^{(c)}},j
\end{align}
is minimized. It does this by running K-means separately in each subspace $\mathcal{J}$ and using the resulting centroids and assignments to populate $\mat{P}$ and $\vec{z}$.

% Perhaps the simplest means of constructing prototypes would be to run a clustering algorithm like K-means on the rows of $\A$. This would be effective, but would require using a large number of prototypes in order to have each row closely match its prototype. What would be preferable is some means of having a huge number of prototypes without having to pay so large a time and space cost.

% PQ achieves this goal by using as prototypes the cartesian product of prototypes within disjoint subspaces. Concretely, PQ runs K-means with $K$ centroids in each of $C$ disjoint (usually contiguous) sets of dimensions. This results in $K^C$ possible combinations of centroid assignments for each vector, with each unique combination corresponding to a unique overall prototype.

% vectors into disjoint ``subvectors'' (each corresponding to a unique set of dimensions) and runs k-means within each subspace.
%  If there are $K$ prototypes per subspace and $C$ subspaces, this results in $K^C$ possible combinations of prototype assignments for each vector. Viewed differently, this creates $K^C$ overall prototypes whose entries

% ------------------------------------------------
\subsection{Encoding Function - $g(\A)$}
% ------------------------------------------------

Given the learned prototypes, PQ replaces each row $\a$ of $\A$ with the concatenation of its $C$ K-means centroid assignments in each of the $C$ subspaces. Formally:
\begin{align} \label{eq:pqLoss}
    g(\a)^{(c)} \triangleq \argmin_k \sum_{j\in \mathcal{J}_c} (\a_j - \mat{P}^{(c)}_{k,j})^2
\end{align}
We will refer to the resulting sequence of indices as the \textit{encoding} of $\a$ and the set of $K$ centroids as a \textit{codebook}. This encoding function has complexity $\Theta(KD)$ for each $\a$, since there are $K$ centroids per subspace and the sum of the number of indices in all subspaces is $D$.

% ------------------------------------------------
\subsection{Table Construction - $h(\B)$}
% ------------------------------------------------

Using these same prototypes, PQ constructs a lookup table $h(\b)^{(c)} \in \R^K$ in each of the $C$ subspaces for each column $\b$ of $\B$, where
\begin{align}
    h(\b)^{(c)} \triangleq \sum_{j\in \mathcal{J}_c} \b_j \mat{P}^{(c)}_{k,j}
\end{align}
This has complexity $\Theta(KD)$ for each $\b$, since it is essentially the same as $g(\cdot)$ except without the $\argmin$ operation.

% ------------------------------------------------
\subsection{Aggregation - $f(\cdot,\cdot)$}
% ------------------------------------------------

Given the encoding of $\a$ and the lookup tables for $\b$, the product can be approximated as
\begin{align}
    \a^\top \b \approx \sum_{c=1}^C h(\b)^{(c)}_k, \text{ }k = g(\a)^{(c)}
\end{align}

This has complexity $\Theta(C)$ for each pair of vectors, or $\Theta(NMC)$ for the full matrix product $\A\B$. Since $C \ll D$, this provides a large speedup when $N$ and $M$ are large enough to amortize the costs of $g(\cdot)$ and $h(\cdot)$.

% ------------------------------------------------
\subsection{Complexity and Extensions}
% ------------------------------------------------

The total time complexity of PQ is equal to $\Theta(NDK + DKC + NMC)$. When $N$ and $M$ are large, the last term dominates and the complexity of PQ is essentially $\Theta(NMC)$.
Assuming one does not use Strassen's or other algorithms that are never used in practice in modern numerical libraries, full matrix multiplication has complexity $\Theta(NDM)$. Thus, PQ offers a speedup of $\Theta(\frac{D}{C})$.

To increase this ratio, many authors have attempted to construct more elaborate $g(\cdot)$ and $h(\cdot)$ functions. One line of work seeks to preprocess $\a$ and/or $\b$ with rotation matrices \cite{opq,cartesianKmeans,lopq}. Another line of work seeks to relax the restriction that the codebooks use disjoint subspaces; several authors \cite{aq,cq,otq,sq,grvq,stackedQuantizers} have observed that equation~\ref{eq:pqLoss} is a special case of
\begin{align} \label{eq:nonOrthogonal}
    \sum_{i=1}^N min_{\{z^{(1)},\ldots,z^{(C)}\}} \sum_{j=1}^D \left( \tilde{A}_{ij} - \sum_{c=1}^C \mat{P}^{(c)}_{z^{(c)},j} \right) ^2.
\end{align}
That is, all $C$ codebooks can contribute to approximating any dimension of $\a$, instead of only one codebook being responsible for it. This is strictly more expressive, but also slows down $g(\cdot)$ greatly and complicates training. There has been effort to reduce these burdens by carefully ordering update and assignment operations \cite{stackedQuantizers}, adding sparsity \cite{sq}, or structuring overlap between codebooks so that optimal assignments remain possible \cite{otq}. TODO sentence about optimizing LUTs directly.


%\footnote{This is assuming one does not uses Strassen's or other lower-complexity algorithms that are never used in practice in modern numerical libraries.} When $N$ and $M$ are large, the last term dominates and the complexity of PQ is essentially $\Theta(NDK + DKC + NMC)$. % We do not compare to Strassen's since we are concerned with CPU time on real-world matrices, not assymptotic complexity}.

% % ------------------------------------------------
% \subsection{Problem Formulation}
% % ------------------------------------------------

% Let $\A \in \R^{N \times D}$ and $\B \in \R^{D \times M}$ be two matrices, with $N \gg D, M$, and $M$ not significantly larger than $D$. Given a computation time budget $\tau$, our task is to
% construct three functions $g(\cdot)$, $h(\cdot)$, and $f(\cdot)$ such that
% \begin{align}
%     \norm{f(g(\A), h(\B)) - \A\B}_F < \eps(\tau) \norm{\A\B}_F
% \end{align}
% for the smallest error $\eps(\tau)$ possible.

% % . Moreover, the computation time $\tau(\eps)$ to run these three functions should be much smaller than the comuputation time $\tau_0$ to compute $\A\B$ directly.

% We assume the existence of a training set $\tilde{\A}$, whose rows are drawn from the same distribution as the rows of $\A$. This is a natural assumption in the case that rows of $\A$ represent examples in training data, or structured subsets thereof (such as patches of natural images). This assumption is common in the information retrieval literature \cite{bolt, pairq}.%, but not in theoretical work.

% % We also focus on the case in which $B$ has at most a few hundred columns--or even as few as 10. This is the range of sizes that one might obtain in a neural network, where each column of $B$ corresponds to one output neuron. It is also small enough that asymptotic complexity is a not a good characterization of how an algorithm will perform in practice.

% We also assume that the cost of computing $h(\B)$ negligible provided that $h(\B) \in O(MD)$. One sufficient condition for this is $\B$ being provided ahead of time (as is the case when $\B$ represents predefined model weights). A second is that $\B$ has sufficiently few columns compared to the number of rows in $\A$.%, which happens when $N$ is sufficiently large relative to $M * \frac{\tau(\eps)}{\tau(0)}$.

% % % ------------------------------------------------
% % \subsection{Existing approaches}
% % % ------------------------------------------------

% % Many existing approaches construct a matrix $\V \in \R^{D \times d}, d < D$ such that
% % \begin{align}
% %     \A \B \approx (\A \V) (\V^\top \B).
% % \end{align}
% % Often, $\V$ is sparse [] or has structure [] such that these projection operations are faster than a dense matrix multiply.


% % % ------------------------------------------------
% % \subsection{Problem Formulation}
% % % ------------------------------------------------

% % We consider three variations of the approximate matrix multiply problem. In all cases, let X = ...., <other variable definitions here>.

% % Maybe a table of assumptions of different methods as far as training data and preprocessing time. And/or which problem statement they try to solve.

% % Problem 1: No training data at all (most AMM work, ours with LUT computation + untrained encoding)

% % Problem 2: Training data for one matrix (ours with LUT computation)

% % Problem 3: One matrix fixed (VQ stuff with no optimizing wrt query, ours without learned quantization)

% % Problem 4: Training data for both matrices (VQ stuff that rotates using query distro, Bolt, ours with LUT computation + weighting errs)

% % Problem 5: Training data for one matrix, other matrix fixed (ours)

% % % this is not quite MECE because could have one matrix fixed and other completely unknown, in which case no LUT computation but also no trained quantizers. Probably just note that this possibility exists and our method could do it, but we ignore it for rest of the paper because

% % Note that this is maybe not a sufficient characterization because ours is also mostly just useful when one of the matrices is like really small.

% % Should probably just show ours, ours without learned quantization func, and ours with LUT computation at runtime. As a result, should only introduce these three problems.

% % % ------------------------------------------------
% % \subsection{Vector Quantization}
% % % ------------------------------------------------

% % With problem statement ourFrame stuff using the two encoding functions and one distance function.

% % First walk through basic VQ-based dot product, with img from poster.
