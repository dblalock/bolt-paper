
@article{memFewShot,
	title = {Learning to {Remember} {Rare} {Events}},
	url = {http://arxiv.org/abs/1703.03129},
	abstract = {Despite recent advances, memory-augmented deep neural networks are still limited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efﬁciency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision. It operates in a life-long manner, i.e., without the need to reset it during training.},
	language = {en},
	urldate = {2018-11-09},
	journal = {arXiv:1703.03129 [cs]},
	author = {Kaiser, Łukasz and Nachum, Ofir and Roy, Aurko and Bengio, Samy},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.03129},
	keywords = {Computer Science - Machine Learning},
	file = {Kaiser et al. - 2017 - Learning to Remember Rare Events.pdf:/Users/davis/Zotero/storage/NLIKWNRQ/Kaiser et al. - 2017 - Learning to Remember Rare Events.pdf:application/pdf}
}

@article{oneShotGesture,
	title = {One-{Shot}-{Learning} {Gesture} {Recognition} using {HOG}-{HOF} {Features}},
	url = {http://arxiv.org/abs/1312.4190},
	abstract = {The purpose of this paper is to describe one-shot-learning gesture recognition systems developed on the ChaLearn Gesture Dataset (ChaLearn). We use RGB and depth images and combine appearance (Histograms of Oriented Gradients) and motion descriptors (Histogram of Optical Flow) for parallel temporal segmentation and recognition. The Quadratic-Chi distance family is used to measure diﬀerences between histograms to capture cross-bin relationships. We also propose a new algorithm for trimming videos —to remove all the unimportant frames from videos. We present two methods that use a combination of HOG-HOF descriptors together with variants of a Dynamic Time Warping technique. Both methods outperform other published methods and help narrow the gap between human performance and algorithms on this task. The code is publicly available in the MLOSS repository.},
	language = {en},
	urldate = {2018-11-09},
	journal = {arXiv:1312.4190 [cs]},
	author = {Konečný, Jakub and Hagara, Michal},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.4190},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Konečný and Hagara - 2013 - One-Shot-Learning Gesture Recognition using HOG-HO.pdf:/Users/davis/Zotero/storage/ZLMV3PIB/Konečný and Hagara - 2013 - One-Shot-Learning Gesture Recognition using HOG-HO.pdf:application/pdf}
}

@article{recogEventsIRL,
	title = {Complex {Event} {Recognition} from {Images} with {Few} {Training} {Examples}},
	url = {http://arxiv.org/abs/1701.04769},
	abstract = {We propose to leverage concept-level representations for complex event recognition in photographs given limited training examples. We introduce a novel framework to discover event concept attributes from the web and use that to extract semantic features from images and classify them into social event categories with few training examples. Discovered concepts include a variety of objects, scenes, actions and event sub-types, leading to a discriminative and compact representation for event images. Web images are obtained for each discovered event concept and we use (pretrained) CNN features to train concept classiﬁers. Extensive experiments on challenging event datasets demonstrate that our proposed method outperforms several baselines using deep CNN features directly in classifying images into events with limited training examples. We also demonstrate that our method achieves the best overall accuracy on a dataset with unseen event categories using a single training example.},
	language = {en},
	urldate = {2018-11-09},
	journal = {arXiv:1701.04769 [cs]},
	author = {Ahsan, Unaiza and Sun, Chen and Hays, James and Essa, Irfan},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.04769},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ahsan et al. - 2017 - Complex Event Recognition from Images with Few Tra.pdf:/Users/davis/Zotero/storage/9FYTD6VY/Ahsan et al. - 2017 - Complex Event Recognition from Images with Few Tra.pdf:application/pdf}
}

@article{lcnn,
	title = {{LCNN}: {Lookup}-based {Convolutional} {Neural} {Network}},
	shorttitle = {{LCNN}},
	url = {http://arxiv.org/abs/1611.06473},
	abstract = {Porting state of the art deep learning algorithms to resource constrained compute platforms (e.g. VR, AR, wearables) is extremely challenging. We propose a fast, compact, and accurate model for convolutional neural networks that enables efﬁcient learning and inference. We introduce LCNN, a lookup-based convolutional neural network that encodes convolutions by few lookups to a dictionary that is trained to cover the space of weights in CNNs. Training LCNN involves jointly learning a dictionary and a small set of linear combinations. The size of the dictionary naturally traces a spectrum of trade-offs between efﬁciency and accuracy. Our experimental results on ImageNet challenge show that LCNN can offer 3 2⇥ speedup while achieving .},
	language = {en},
	urldate = {2018-11-09},
	journal = {arXiv:1611.06473 [cs]},
	author = {Bagherinezhad, Hessam and Rastegari, Mohammad and Farhadi, Ali},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.06473},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Bagherinezhad et al. - 2016 - LCNN Lookup-based Convolutional Neural Network.pdf:/Users/davis/Zotero/storage/5ZPHW734/Bagherinezhad et al. - 2016 - LCNN Lookup-based Convolutional Neural Network.pdf:application/pdf}
}

@article{_matchingNets,
	title = {Matching {Networks} for {One} {Shot} {Learning}},
	abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for ﬁne-tuning to adapt to new class types. We then deﬁne one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6\% to 93.2\% and from 88.0\% to 93.8\% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
	language = {en},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Tim},
	pages = {9},
	file = {Vinyals et al. - Matching Networks for One Shot Learning.pdf:/Users/davis/Zotero/storage/FAKZ7BDZ/Vinyals et al. - Matching Networks for One Shot Learning.pdf:application/pdf}
}

@article{oneShotWord,
	title = {Distributional {Modeling} on a {Diet}: {One}-shot {Word} {Learning} from {Text} {Only}},
	shorttitle = {Distributional {Modeling} on a {Diet}},
	url = {http://arxiv.org/abs/1704.04550},
	abstract = {We test whether distributional models can do one-shot learning of deﬁnitional properties from text only. Using Bayesian models, we ﬁnd that ﬁrst learning overarching structure in the known data, regularities in textual contexts and in properties, helps one-shot learning, and that individual context items can be highly informative.},
	language = {en},
	urldate = {2018-11-09},
	journal = {arXiv:1704.04550 [cs]},
	author = {Wang, Su and Roller, Stephen and Erk, Katrin},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04550},
	keywords = {Computer Science - Computation and Language},
	file = {Wang et al. - 2017 - Distributional Modeling on a Diet One-shot Word L.pdf:/Users/davis/Zotero/storage/4K8CSI3Q/Wang et al. - 2017 - Distributional Modeling on a Diet One-shot Word L.pdf:application/pdf}
}

@article{protoNets,
	title = {Prototypical {Networks} for {Few}-shot {Learning}},
	url = {http://arxiv.org/abs/1703.05175},
	abstract = {We propose prototypical networks for the problem of few-shot classiﬁcation, where a classiﬁer must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classiﬁcation can be performed by computing Euclidean distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reﬂect a simpler inductive bias that is beneﬁcial in this limited-data regime, and achieve stateof-the-art results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to the case of zero-shot learning and achieve state-of-the-art zero-shot results on the CU-Birds dataset.},
	language = {en},
	urldate = {2018-11-09},
	journal = {arXiv:1703.05175 [cs, stat]},
	author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.05175},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Snell et al. - 2017 - Prototypical Networks for Few-shot Learning.pdf:/Users/davis/Zotero/storage/IABKQ9TA/Snell et al. - 2017 - Prototypical Networks for Few-shot Learning.pdf:application/pdf}
}

@inproceedings{bayesOneShot,
	address = {Nice, France},
	title = {A {Bayesian} approach to unsupervised one-shot learning of object categories},
	isbn = {978-0-7695-1950-0},
	url = {http://ieeexplore.ieee.org/document/1238476/},
	doi = {10.1109/ICCV.2003.1238476},
	language = {en},
	urldate = {2018-11-09},
	booktitle = {Proceedings {Ninth} {IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {{Li Fe-Fei} and {Fergus} and {Perona}},
	year = {2003},
	pages = {1134--1141 vol.2},
	file = {Li Fe-Fei et al. - 2003 - A Bayesian approach to unsupervised one-shot learn.pdf:/Users/davis/Zotero/storage/WAHWC7SL/Li Fe-Fei et al. - 2003 - A Bayesian approach to unsupervised one-shot learn.pdf:application/pdf}
}

@article{invertCausal,
	title = {One-shot learning by inverting a compositional causal process},
	abstract = {People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classiﬁcation task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also tested the model on another conceptual task, generating new examples, by using a “visual Turing test” to show that our model produces human-like performance.},
	language = {en},
	author = {Lake, Brenden M and Salakhutdinov, Ruslan},
	pages = {9},
	year = {2013},
	file = {Lake and Salakhutdinov - One-shot learning by inverting a compositional cau.pdf:/Users/davis/Zotero/storage/UQD2LFLM/Lake and Salakhutdinov - One-shot learning by inverting a compositional cau.pdf:application/pdf}
}
