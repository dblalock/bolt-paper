
@article{francis_practical_2018,
	title = {A practical streaming approximate matrix multiplication algorithm},
	issn = {13191578},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1319157818306396},
	doi = {10.1016/j.jksuci.2018.09.010},
	abstract = {Approximate Matrix Multiplication (AMM) has emerged as a useful and computationally inexpensive substitute for actual multiplication of large matrices. Randomized as well as deterministic solutions to AMM were provided in the past. The latest work provides a deterministic algorithm that solves AMM more accurately than the other works. It is a streaming algorithm that is both fast and accurate. But, it is less robust to noise and is also liable to have less than optimal performance in the presence of concept drift in the input matrices. We propose an algorithm that is more accurate, robust to noise, invariant to concept drift in the data, while having almost the same running time as the state-of-the-art algorithm. We also prove that theoretical guarantees exist for the proposed algorithm. An empirical performance improvement of up to 90\% is obtained over the previous algorithm. We also propose a general framework for parallelizing the proposed algorithm. The two parallelized versions of the algorithm achieve up to 1:9x and 3:6x speedups over the original version of the proposed algorithm.},
	language = {en},
	urldate = {2019-09-12},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Francis, Deena P. and Raimond, Kumudha},
	month = sep,
	year = {2018},
	file = {Francis and Raimond - 2018 - A practical streaming approximate matrix multiplic.pdf:/Users/davis/Zotero/storage/RUZRW3QB/Francis and Raimond - 2018 - A practical streaming approximate matrix multiplic.pdf:application/pdf}
}

@inproceedings{ye_frequent_2016,
	title = {Frequent {Direction} {Algorithms} for {Approximate} {Matrix} {Multiplication} with {Applications} in {CCA}},
	abstract = {Approximate matrix multiplication (AMM) becomes increasingly popular because it makes matrix computation suitable for large-scale datasets. Most previous AMM methods are based on the idea of random selection or random projection. In this paper, we propose a deterministic algorithm FD-AMM for computing an approximation to the product of two given matrices. Moreover, the algorithm works in a streaming manner. In particular, our approach is inspired by a recently proposed matrix sketching algorithm called Frequent Directions (FD). FD-AMM has stronger error bound than both random selection and random projection algorithms with respect to the same space complexity. Our approach also leads to an algorithm for computing the Canonical Correlation Analysis (CCA) of two matrices exactly in a streaming way, which takes less space than the classical method. Experimental results validate the effectiveness of our method.},
	language = {en},
	booktitle = {{IJCAI}},
	author = {Ye, Qiaomin and Luo, Luo and Zhang, Zhihua},
	year = {2016},
	pages = {7},
	file = {Ye et al. - Frequent Direction Algorithms for Approximate Matr.pdf:/Users/davis/Zotero/storage/RS2Y7MPS/Ye et al. - Frequent Direction Algorithms for Approximate Matr.pdf:application/pdf}
}

@article{mroueh_co-occuring_2016,
	title = {Co-{Occuring} {Directions} {Sketching} for {Approximate} {Matrix} {Multiply}},
	url = {http://arxiv.org/abs/1610.07686},
	abstract = {We introduce co-occurring directions sketching, a deterministic algorithm for approximate matrix product (AMM), in the streaming model. We show that co-occuring directions achieves a better error bound for AMM than other randomized and deterministic approaches for AMM. Co-occurring directions gives a (1 + ε)-approximation of the optimal low rank approximation of a matrix product. Empirically our algorithm outperforms competing methods for AMM, for a small sketch size. We validate empirically our theoretical ﬁndings and algorithms.},
	language = {en},
	urldate = {2019-09-12},
	journal = {arXiv:1610.07686 [cs]},
	author = {Mroueh, Youssef and Marcheret, Etienne and Goel, Vaibhava},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.07686},
	keywords = {Computer Science - Machine Learning},
	file = {Mroueh et al. - 2016 - Co-Occuring Directions Sketching for Approximate M.pdf:/Users/davis/Zotero/storage/DE6PPLLH/Mroueh et al. - 2016 - Co-Occuring Directions Sketching for Approximate M.pdf:application/pdf}
}

@article{manne_fast_2014,
	title = {Fast {Approximate} {Matrix} {Multiplication} by {Solving} {Linear} {Systems}},
	url = {http://arxiv.org/abs/1408.4230},
	abstract = {In this paper, we present novel deterministic algorithms for multiplying two n × n matrices approximately. Given two matrices A, B we return a matrix C′ which is an approximation to C = AB. We consider the notion of approximate matrix multiplication in which the objective is to make the Frobenius norm of the error matrix C − C′ arbitrarily small. Our main contribution is to ﬁrst reduce the matrix multiplication problem to solving a set of linear equations and then use standard techniques to ﬁnd an approximate solution to that system in O˜(n2) time. To the best of our knowledge this the ﬁrst examination into designing quadratic time deterministic algorithms for approximate matrix multiplication which guarantee arbitrarily low absolute error w.r.t. Frobenius norm.},
	language = {en},
	urldate = {2019-09-12},
	journal = {arXiv:1408.4230 [cs]},
	author = {Manne, Shiva and Pal, Manjish},
	month = aug,
	year = {2014},
	note = {arXiv: 1408.4230},
	keywords = {Computer Science - Data Structures and Algorithms, F.2, G.1.2},
	file = {Manne and Pal - 2014 - Fast Approximate Matrix Multiplication by Solving .pdf:/Users/davis/Zotero/storage/E2H8LYNH/Manne and Pal - 2014 - Fast Approximate Matrix Multiplication by Solving .pdf:application/pdf}
}

@article{kyrillidis_approximate_2014,
	title = {Approximate {Matrix} {Multiplication} with {Application} to {Linear} {Embeddings}},
	url = {http://arxiv.org/abs/1403.7683},
	abstract = {In this paper, we study the problem of approximately computing the product of two real matrices. In particular, we analyze a dimensionality-reduction-based approximation algorithm due to Sarlos [1], introducing the notion of nuclear rank as the ratio of the nuclear norm over the spectral norm. The presented bound has improved dependence with respect to the approximation error (as compared to previous approaches), whereas the subspace – on which we project the input matrices – has dimensions proportional to the maximum of their nuclear rank and it is independent of the input dimensions.},
	language = {en},
	urldate = {2019-09-12},
	journal = {arXiv:1403.7683 [cs, math, stat]},
	author = {Kyrillidis, Anastasios and Vlachos, Michail and Zouzias, Anastasios},
	month = mar,
	year = {2014},
	note = {arXiv: 1403.7683},
	keywords = {Statistics - Machine Learning, Computer Science - Information Theory, Mathematics - Statistics Theory},
	file = {Kyrillidis et al. - 2014 - Approximate Matrix Multiplication with Application.pdf:/Users/davis/Zotero/storage/F4GSU7VY/Kyrillidis et al. - 2014 - Approximate Matrix Multiplication with Application.pdf:application/pdf}
}

@article{drineas_fast_2006,
	title = {Fast {Monte} {Carlo} {Algorithms} for {Matrices} {I}: {Approximating} {Matrix} {Multiplication}},
	volume = {36},
	issn = {0097-5397, 1095-7111},
	shorttitle = {Fast {Monte} {Carlo} {Algorithms} for {Matrices} {I}},
	url = {http://epubs.siam.org/doi/10.1137/S0097539704442684},
	doi = {10.1137/S0097539704442684},
	language = {en},
	number = {1},
	urldate = {2019-09-12},
	journal = {SIAM Journal on Computing},
	author = {Drineas, Petros and Kannan, Ravi and Mahoney, Michael W.},
	month = jan,
	year = {2006},
	pages = {132--157},
	file = {Drineas et al. - 2006 - Fast Monte Carlo Algorithms for Matrices I Approx.pdf:/Users/davis/Zotero/storage/3CHPTZPZ/Drineas et al. - 2006 - Fast Monte Carlo Algorithms for Matrices I Approx.pdf:application/pdf}
}

@article{teng_fast_2019,
	title = {A {Fast} {Frequent} {Directions} {Algorithm} for {Low} {Rank} {Approximation}},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8362693/},
	doi = {10.1109/TPAMI.2018.2839198},
	abstract = {Recently a deterministic method, frequent directions (FD) is proposed to solve the high dimensional low rank approximation problem. It works well in practice, but experiences high computational cost. In this paper, we establish a fast frequent directions algorithm for the low rank approximation problem, which implants a randomized algorithm, sparse subspace embedding (SpEmb) in FD. This new algorithm makes use of FD’s natural block structure and sends more information through SpEmb to each block in FD. We prove that our new algorithm produces a good low rank approximation with a sketch of size linear on the rank approximated. Its effectiveness and efﬁciency are demonstrated by the experimental results on both synthetic and real world datasets, as well as applications in network analysis.},
	language = {en},
	number = {6},
	urldate = {2019-09-12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Teng, Dan and Chu, Delin},
	month = jun,
	year = {2019},
	pages = {1279--1293},
	file = {Teng and Chu - 2019 - A Fast Frequent Directions Algorithm for Low Rank .pdf:/Users/davis/Zotero/storage/NJLIPLI4/Teng and Chu - 2019 - A Fast Frequent Directions Algorithm for Low Rank .pdf:application/pdf}
}

@article{gupta_oversketch:_2018,
	title = {{OverSketch}: {Approximate} {Matrix} {Multiplication} for the {Cloud}},
	shorttitle = {{OverSketch}},
	url = {http://arxiv.org/abs/1811.02653},
	doi = {10.1109/BigData.2018.8622139},
	abstract = {We propose OverSketch, an approximate algorithm for distributed matrix multiplication in serverless computing. OverSketch leverages ideas from matrix sketching and high-performance computing to enable cost-efﬁcient multiplication that is resilient to faults and straggling nodes pervasive in low-cost serverless architectures. We establish statistical guarantees on the accuracy of OverSketch and empirically validate our results by solving a large-scale linear program using interior-point methods and demonstrate a 34\% reduction in compute time on AWS Lambda.},
	language = {en},
	urldate = {2019-09-12},
	journal = {2018 IEEE International Conference on Big Data (Big Data)},
	author = {Gupta, Vipul and Wang, Shusen and Courtade, Thomas and Ramchandran, Kannan},
	month = dec,
	year = {2018},
	note = {arXiv: 1811.02653},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Information Theory},
	pages = {298--304},
	file = {Gupta et al. - 2018 - OverSketch Approximate Matrix Multiplication for .pdf:/Users/davis/Zotero/storage/426LP49K/Gupta et al. - 2018 - OverSketch Approximate Matrix Multiplication for .pdf:application/pdf}
}

@article{huang_near_2019,
	title = {Near {Optimal} {Frequent} {Directions} for {Sketching} {Dense} and {Sparse} {Matrices}},
	volume = {20},
	abstract = {Given a large matrix A ∈ Rn×d, we consider the problem of computing a sketch matrix B ∈ R ×d which is signiﬁcantly smaller than but still well approximates A. We consider the problems in the streaming model, where the algorithm can only make one pass over the input with limited working space, and we are interested in minimizing the covariance error AT A − BT B 2. The popular Frequent Directions algorithm of Liberty (2013) and its variants achieve optimal space-error tradeoﬀs. However, whether the running time can be improved remains an unanswered question. In this paper, we almost settle the question by proving that the time complexity of this problem is equivalent to that of matrix multiplication up to lower order terms. Speciﬁcally, we provide new space-optimal algorithms with faster running times and also show that the running times of our algorithms can be improved if and only if the state-of-the-art running time of matrix multiplication can be improved signiﬁcantly.},
	language = {en},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Huang, Zengfeng},
	month = feb,
	year = {2019},
	pages = {23},
	file = {Huang - Near Optimal Frequent Directions for Sketching Den.pdf:/Users/davis/Zotero/storage/UPSVU8VN/Huang - Near Optimal Frequent Directions for Sketching Den.pdf:application/pdf}
}

@article{luo_robust_2019,
	title = {Robust {Frequent} {Directions} with {Application} in {Online} {Learning}},
	volume = {20},
	abstract = {The frequent directions (FD) technique is a deterministic approach for online sketching that has many applications in machine learning. The conventional FD is a heuristic procedure that often outputs rank deﬁcient matrices. To overcome the rank deﬁciency problem, we propose a new sketching strategy called robust frequent directions (RFD) by introducing a regularization term. RFD can be derived from an optimization problem. It updates the sketch matrix and the regularization term adaptively and jointly. RFD reduces the approximation error of FD without increasing the computational cost. We also apply RFD to online learning and propose an effective hyperparameterfree online Newton algorithm. We derive a regret bound for our online Newton algorithm based on RFD, which guarantees the robustness of the algorithm. The experimental studies demonstrate that the proposed method outperforms state-of-the-art second order online learning algorithms.},
	language = {en},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Luo, Luo and Chen, Cheng and Zhang, Zhihua and Li, Wu-Jun and Zhang, Tong},
	month = feb,
	year = {2019},
	pages = {41},
	file = {Luo et al. - Robust Frequent Directions with Application in Onl.pdf:/Users/davis/Zotero/storage/TDLU7TQS/Luo et al. - Robust Frequent Directions with Application in Onl.pdf:application/pdf}
}

@article{francis_improvement_2018,
	title = {An improvement of the parameterized frequent directions algorithm},
	volume = {32},
	issn = {1384-5810, 1573-756X},
	url = {http://link.springer.com/10.1007/s10618-017-0542-x},
	doi = {10.1007/s10618-017-0542-x},
	abstract = {Matrix sketching is a technique used to create summaries of large matrices. Frequent directions (FD) and its parameterized variant, α-FD are deterministic sketching techniques that have theoretical guarantees and also work well in practice. An algorithm called the iterative singular value decomposition (iSVD) has been shown to have better performance than FD and α-FD in several datasets, despite the lack of theoretical guarantees. However, in datasets with major and sudden drift, iSVD performs poorly when compared to the other algorithms. The α-FD algorithm has better error guarantees and empirical performance when compared to FD. However, it has two limitations: the restriction on the effective values of its parameter α due to its dependence on sketch size and its constant factor reduction from selected squared singular values, both of which result in reduced empirical performance. In this paper, we present a modiﬁed parameterized FD algorithm, β-FD in order to overcome the limitations of α-FD, while maintaining similar error guarantees to that of α-FD. Empirical results on datasets with sudden and major drift and those with gradual and minor or no drift indicate that there is a trade-off between the errors in both kinds of data for different parameter values, and for β ≈ 28, our algorithm has overall better error performance than α-FD.},
	language = {en},
	number = {2},
	urldate = {2019-09-12},
	journal = {Data Mining and Knowledge Discovery},
	author = {Francis, Deena P. and Raimond, Kumudha},
	month = mar,
	year = {2018},
	pages = {453--482},
	file = {Francis and Raimond - 2018 - An improvement of the parameterized frequent direc.pdf:/Users/davis/Zotero/storage/I597KB2K/Francis and Raimond - 2018 - An improvement of the parameterized frequent direc.pdf:application/pdf}
}

@inproceedings{huang_efficient_2017,
	address = {Chicago, Illinois, USA},
	title = {Efficient {Matrix} {Sketching} over {Distributed} {Data}},
	isbn = {978-1-4503-4198-1},
	url = {http://dl.acm.org/citation.cfm?doid=3034786.3056119},
	doi = {10.1145/3034786.3056119},
	abstract = {A sketch or synopsis of a large dataset captures vital properties of the original data while typically occupying much less space. In this paper, we consider the problem of computing a sketch of a massive data matrix A ∈ Rn×d, which is distributed across a large number of s servers. Our goal is to output a matrix B ∈ R ×d which is signiﬁcantly smaller than but still approximates A well in terms of covariance error, i.e., AT A − BT B 2. Here, for a matrix A, A 2 is the spectral norm of A, which is deﬁned as the largest singular value of A. Following previous works, we call B a covariance sketch of A. We are mainly focused on minimizing the communication cost, which is arguably the most valuable resource in distributed computations. We show a gap between deterministic and randomized communication complexity for computing a covariance sketch. More speciﬁcally, we ﬁrst prove a tight deterministic lower bound, then show how to bypass this lower bound using randomization. In Principle Component Analysis (PCA), the goal is to ﬁnd a low-dimensional subspace that captures as much of the variance of a dataset as possible. Based on a well-known connection between covariance sketch and PCA, we give a new algorithm for distributed PCA with improved communication cost. Moreover, in our algorithms, each server only needs to make one pass over the data with limited working space.},
	language = {en},
	urldate = {2019-09-12},
	booktitle = {Proceedings of the 36th {ACM} {SIGMOD}-{SIGACT}-{SIGAI} {Symposium} on {Principles} of {Database} {Systems}  - {PODS} '17},
	publisher = {ACM Press},
	author = {Huang, Zengfeng and Lin, Xuemin and Zhang, Wenjie and Zhang, Ying},
	year = {2017},
	pages = {347--359},
	file = {Huang et al. - 2017 - Efficient Matrix Sketching over Distributed Data.pdf:/Users/davis/Zotero/storage/UTHUVQVS/Huang et al. - 2017 - Efficient Matrix Sketching over Distributed Data.pdf:application/pdf}
}

@article{ghashami_frequent_2016,
	title = {Frequent {Directions}: {Simple} and {Deterministic} {Matrix} {Sketching}},
	volume = {45},
	issn = {0097-5397, 1095-7111},
	shorttitle = {Frequent {Directions}},
	url = {http://epubs.siam.org/doi/10.1137/15M1009718},
	doi = {10.1137/15M1009718},
	language = {en},
	number = {5},
	urldate = {2019-09-12},
	journal = {SIAM Journal on Computing},
	author = {Ghashami, Mina and Liberty, Edo and Phillips, Jeff M. and Woodruff, David P.},
	month = jan,
	year = {2016},
	pages = {1762--1792},
	file = {Ghashami et al. - 2016 - Frequent Directions Simple and Deterministic Matr.pdf:/Users/davis/Zotero/storage/J3DXRIN9/Ghashami et al. - 2016 - Frequent Directions Simple and Deterministic Matr.pdf:application/pdf}
}

@article{desai_improved_2016,
	title = {Improved {Practical} {Matrix} {Sketching} with {Guarantees}},
	volume = {28},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/7429755/},
	doi = {10.1109/TKDE.2016.2539943},
	abstract = {Matrices have become essential data representations for many large-scale problems in data analytics, and hence matrix sketching is a critical task. Although much research has focused on improving the error/size tradeoff under various sketching paradigms, the many forms of error bounds make these approaches hard to compare in theory and in practice. This paper attempts to categorize and compare the most known methods under row-wise streaming updates with provable guarantees, and then to tweak some of these methods to gain practical improvements while retaining guarantees. For instance, we observe that a simple heuristic iSVD, with no guarantees, tends to outperform all known approaches in terms of size/error trade-off. We modify the best performing method with guarantees, FREQUENTDIRECTIONS, under the size/error trade-off to match the performance of iSVD and retain its guarantees. We also demonstrate some adversarial datasets where iSVD performs quite poorly. In comparing techniques in the time/error trade-off, techniques based on hashing or sampling tend to perform better. In this setting, we modify the most studied sampling regime to retain error guarantee but obtain dramatic improvements in the time/error trade-off. Finally, we provide easy replication of our studies on APT, a new testbed which makes available not only code and datasets, but also a computing platform with ﬁxed environmental settings.},
	language = {en},
	number = {7},
	urldate = {2019-09-12},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Desai, Amey and Ghashami, Mina and Phillips, Jeff M.},
	month = jul,
	year = {2016},
	pages = {1678--1690},
	file = {Desai et al. - 2016 - Improved Practical Matrix Sketching with Guarantee.pdf:/Users/davis/Zotero/storage/53ENPHPA/Desai et al. - 2016 - Improved Practical Matrix Sketching with Guarantee.pdf:application/pdf}
}

@article{cohen_optimal_2015,
	title = {Optimal approximate matrix product in terms of stable rank},
	url = {http://arxiv.org/abs/1507.02268},
	abstract = {We give two diﬀerent characterizations of the type of dimensionality-reducing map Π that can be used for spectral error approximate matrix multiplication (AMM). Both imply a random data-oblivious Π with m = O(r˜/ε2) rows suﬃces, where r˜ is the maximum stable rank, i.e. squared ratio of Frobenius and operator norms, of the matrices being multiplied. This answers the main open question of [MZ11, KVZ14], and is optimal for any random oblivious map.},
	language = {en},
	urldate = {2019-09-12},
	journal = {arXiv:1507.02268 [cs, stat]},
	author = {Cohen, Michael B. and Nelson, Jelani and Woodruff, David P.},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.02268},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms},
	file = {Cohen et al. - 2015 - Optimal approximate matrix product in terms of sta.pdf:/Users/davis/Zotero/storage/493I6PUF/Cohen et al. - 2015 - Optimal approximate matrix product in terms of sta.pdf:application/pdf}
}

@article{ghashami_efficient_2016,
	title = {Efficient {Frequent} {Directions} {Algorithm} for {Sparse} {Matrices}},
	url = {http://arxiv.org/abs/1602.00412},
	abstract = {This paper describes Sparse Frequent Directions, a variant of Frequent Directions for sketching sparse matrices. It resembles the original algorithm in many ways: both receive the rows of an input matrix An×d one by one in the streaming setting and compute a small sketch B ∈ R ×d. Both share the same strong (provably optimal) asymptotic guarantees with respect to the spaceaccuracy tradeoﬀ in the streaming setting. However, unlike Frequent Directions which runs in O(nd ) time regardless of the sparsity of the input matrix A, Sparse Frequent Directions runs in O˜ nnz(A) + n 2 time. Our analysis loosens the dependence on computing the Singular Value Decomposition (SVD) as a black box within the Frequent Directions algorithm. Our bounds require recent results on the properties of fast approximate SVD computations. Finally, we empirically demonstrate that these asymptotic improvements are practical and signiﬁcant on real and synthetic data.},
	language = {en},
	urldate = {2019-09-12},
	journal = {arXiv:1602.00412 [cs]},
	author = {Ghashami, Mina and Liberty, Edo and Phillips, Jeff M.},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.00412},
	keywords = {Computer Science - Data Structures and Algorithms},
	file = {Ghashami et al. - 2016 - Efficient Frequent Directions Algorithm for Sparse.pdf:/Users/davis/Zotero/storage/STST7AUV/Ghashami et al. - 2016 - Efficient Frequent Directions Algorithm for Sparse.pdf:application/pdf}
}

@article{pagh_compressed_2013,
	title = {Compressed matrix multiplication},
	volume = {5},
	issn = {19423454},
	url = {http://dl.acm.org/citation.cfm?doid=2493252.2493254},
	doi = {10.1145/2493252.2493254},
	language = {en},
	number = {3},
	urldate = {2019-09-12},
	journal = {ACM Transactions on Computation Theory},
	author = {Pagh, Rasmus},
	month = aug,
	year = {2013},
	pages = {1--17},
	file = {Pagh - 2013 - Compressed matrix multiplication.pdf:/Users/davis/Zotero/storage/WGYEWNEK/Pagh - 2013 - Compressed matrix multiplication.pdf:application/pdf}
}

@article{liberty_simple_2012,
	title = {Simple and {Deterministic} {Matrix} {Sketching}},
	url = {http://arxiv.org/abs/1206.0594},
	abstract = {We adapt a well known streaming algorithm for approximating item frequencies to the matrix sketching setting. The algorithm receives the rows of a large matrix \$A {\textbackslash}in {\textbackslash}R{\textasciicircum}\{n {\textbackslash}times m\}\$ one after the other in a streaming fashion. It maintains a sketch matrix \$B {\textbackslash}in {\textbackslash}R{\textasciicircum} \{1/{\textbackslash}eps {\textbackslash}times m\}\$ such that for any unit vector \$x\$ [{\textbackslash}{\textbar}Ax{\textbackslash}{\textbar}{\textasciicircum}2 {\textbackslash}ge {\textbackslash}{\textbar}Bx{\textbackslash}{\textbar}{\textasciicircum}2 {\textbackslash}ge {\textbackslash}{\textbar}Ax{\textbackslash}{\textbar}{\textasciicircum}2 - {\textbackslash}eps {\textbackslash}{\textbar}A{\textbackslash}{\textbar}\_\{f\}{\textasciicircum}2 {\textbackslash}.] Sketch updates per row in \$A\$ require \$O(m/{\textbackslash}eps{\textasciicircum}2)\$ operations in the worst case. A slight modification of the algorithm allows for an amortized update time of \$O(m/{\textbackslash}eps)\$ operations per row. The presented algorithm stands out in that it is: deterministic, simple to implement, and elementary to prove. It also experimentally produces more accurate sketches than widely used approaches while still being computationally competitive.},
	language = {en},
	urldate = {2019-09-12},
	journal = {arXiv:1206.0594 [cs]},
	author = {Liberty, Edo},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.0594},
	keywords = {Computer Science - Data Structures and Algorithms},
	file = {Liberty - 2012 - Simple and Deterministic Matrix Sketching.pdf:/Users/davis/Zotero/storage/9AKZJGGN/Liberty - 2012 - Simple and Deterministic Matrix Sketching.pdf:application/pdf}
}

@article{magen_low_2010,
	title = {Low {Rank} {Matrix}-{Valued} {Chernoff} {Bounds} and {Approximate} {Matrix} {Multiplication}},
	url = {http://arxiv.org/abs/1005.2724},
	abstract = {In this paper we develop algorithms for approximating matrix multiplication with respect to the spectral norm. Let A{\textbackslash}in\{{\textbackslash}RR{\textasciicircum}\{n{\textbackslash}times m\}\} and B{\textbackslash}in{\textbackslash}RR{\textasciicircum}\{n {\textbackslash}times p\} be two matrices and {\textbackslash}eps{\textgreater}0. We approximate the product A{\textasciicircum}{\textbackslash}top B using two down-sampled sketches, {\textbackslash}tilde\{A\}{\textbackslash}in{\textbackslash}RR{\textasciicircum}\{t{\textbackslash}times m\} and {\textbackslash}tilde\{B\}{\textbackslash}in{\textbackslash}RR{\textasciicircum}\{t{\textbackslash}times p\}, where t{\textbackslash}ll n such that {\textbackslash}norm\{{\textbackslash}tilde\{A\}{\textasciicircum}{\textbackslash}top {\textbackslash}tilde\{B\} - A{\textasciicircum}{\textbackslash}top B\} {\textbackslash}leq {\textbackslash}eps {\textbackslash}norm\{A\}{\textbackslash}norm\{B\} with high probability. We use two different sampling procedures for constructing {\textbackslash}tilde\{A\} and {\textbackslash}tilde\{B\}; one of them is done by i.i.d. non-uniform sampling rows from A and B and the other is done by taking random linear combinations of their rows. We prove bounds that depend only on the intrinsic dimensionality of A and B, that is their rank and their stable rank; namely the squared ratio between their Frobenius and operator norm. For achieving bounds that depend on rank we employ standard tools from high-dimensional geometry such as concentration of measure arguments combined with elaborate {\textbackslash}eps-net constructions. For bounds that depend on the smaller parameter of stable rank this technology itself seems weak. However, we show that in combination with a simple truncation argument is amenable to provide such bounds. To handle similar bounds for row sampling, we develop a novel matrix-valued Chernoff bound inequality which we call low rank matrix-valued Chernoff bound. Thanks to this inequality, we are able to give bounds that depend only on the stable rank of the input matrices...},
	language = {en},
	urldate = {2019-09-12},
	journal = {arXiv:1005.2724 [cs, math]},
	author = {Magen, Avner and Zouzias, Anastasios},
	month = may,
	year = {2010},
	note = {arXiv: 1005.2724},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Discrete Mathematics, Mathematics - Probability},
	file = {Magen and Zouzias - 2010 - Low Rank Matrix-Valued Chernoff Bounds and Approxi.pdf:/Users/davis/Zotero/storage/4TTXJSPK/Magen and Zouzias - 2010 - Low Rank Matrix-Valued Chernoff Bounds and Approxi.pdf:application/pdf}
}

@inproceedings{sarlos_improved_2006,
	address = {Berkeley, CA},
	title = {Improved {Approximation} {Algorithms} for {Large} {Matrices} via {Random} {Projections}},
	isbn = {978-0-7695-2720-8},
	url = {https://ieeexplore.ieee.org/document/4031351/},
	doi = {10.1109/FOCS.2006.37},
	abstract = {Recently several results appeared that show signiﬁcant reduction in time for matrix multiplication, singular value decomposition as well as linear ( 2) regression, all based on data dependent random sampling. Our key idea is that low dimensional embeddings can be used to eliminate data dependence and provide more versatile, linear time pass efﬁcient matrix computation. Our main contribution is summarized as follows.},
	language = {en},
	urldate = {2019-09-12},
	booktitle = {2006 47th {Annual} {IEEE} {Symposium} on {Foundations} of {Computer} {Science} ({FOCS}'06)},
	publisher = {IEEE},
	author = {Sarlos, Tamas},
	month = oct,
	year = {2006},
	pages = {143--152},
	file = {Sarlos - 2006 - Improved Approximation Algorithms for Large Matric.pdf:/Users/davis/Zotero/storage/T6FCMNKD/Sarlos - 2006 - Improved Approximation Algorithms for Large Matric.pdf:application/pdf}
}

@article{drineas_fast_2006-1,
	title = {Fast {Monte} {Carlo} {Algorithms} for {Matrices} {II}: {Computing} a {Low}-{Rank} {Approximation} to a {Matrix}},
	volume = {36},
	issn = {0097-5397, 1095-7111},
	shorttitle = {Fast {Monte} {Carlo} {Algorithms} for {Matrices} {II}},
	url = {http://epubs.siam.org/doi/10.1137/S0097539704442696},
	doi = {10.1137/S0097539704442696},
	language = {en},
	number = {1},
	urldate = {2019-09-12},
	journal = {SIAM Journal on Computing},
	author = {Drineas, Petros and Kannan, Ravi and Mahoney, Michael W.},
	month = jan,
	year = {2006},
	pages = {158--183},
	file = {Drineas et al. - 2006 - Fast Monte Carlo Algorithms for Matrices II Compu.pdf:/Users/davis/Zotero/storage/RD54T2K6/Drineas et al. - 2006 - Fast Monte Carlo Algorithms for Matrices II Compu.pdf:application/pdf}
}

@article{drineas_fast_2006-2,
	title = {Fast {Monte} {Carlo} {Algorithms} for {Matrices} {III}: {Computing} a {Compressed} {Approximate} {Matrix} {Decomposition}},
	volume = {36},
	issn = {0097-5397, 1095-7111},
	shorttitle = {Fast {Monte} {Carlo} {Algorithms} for {Matrices} {III}},
	url = {http://epubs.siam.org/doi/10.1137/S0097539704442702},
	doi = {10.1137/S0097539704442702},
	language = {en},
	number = {1},
	urldate = {2019-09-12},
	journal = {SIAM Journal on Computing},
	author = {Drineas, Petros and Kannan, Ravi and Mahoney, Michael W.},
	month = jan,
	year = {2006},
	pages = {184--206},
	file = {Drineas et al. - 2006 - Fast Monte Carlo Algorithms for Matrices III Comp.pdf:/Users/davis/Zotero/storage/H4CJ6MBL/Drineas et al. - 2006 - Fast Monte Carlo Algorithms for Matrices III Comp.pdf:application/pdf}
}

@article{clarkson_low_2012,
	title = {Low {Rank} {Approximation} and {Regression} in {Input} {Sparsity} {Time}},
	url = {http://arxiv.org/abs/1207.6365},
	abstract = {We design a new distribution over poly(rε−1) × n matrices S so that for any ﬁxed n × d matrix A of rank r, with probability at least 9/10, SAx 2 = (1 ± ε) Ax 2 simultaneously for all x ∈ Rd. Such a matrix S is called a subspace embedding. Furthermore, SA can be computed in O(nnz(A))time, where nnz(A) is the number of non-zero entries of A. This improves over all previous subspace embeddings, which required at least Ω(nd log d) time to achieve this property. We call our matrices S sparse embedding matrices.},
	language = {en},
	urldate = {2019-09-12},
	journal = {arXiv:1207.6365 [cs]},
	author = {Clarkson, Kenneth L. and Woodruff, David P.},
	month = jul,
	year = {2012},
	note = {arXiv: 1207.6365},
	keywords = {Computer Science - Data Structures and Algorithms},
	file = {Clarkson and Woodruff - 2012 - Low Rank Approximation and Regression in Input Spa.pdf:/Users/davis/Zotero/storage/S4JPHLZX/Clarkson and Woodruff - 2012 - Low Rank Approximation and Regression in Input Spa.pdf:application/pdf}
}
