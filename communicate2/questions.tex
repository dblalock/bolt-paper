
As the previous section makes clear, there do exist a few robust and interesting findings in the neural network pruning literature. However, there are still many open questions. In this section, we provide a non-exhaustive list of questions that we see as particularly interesting or promising. Such a list is necessarily subjective, so we invite the interested reader to view our list merely as a starting point for their consideration.

\begin{itemize}
\item \textbf{What type of sparsity is most effective (and under what circumstances)?}. Authors have proposed types of sparsity including unstructured (no constraints), balanced [] (same number of nonzeros for each neuron), vector [], block [] (with 2D weight submatrices block sparse) inter-channel [] (each output channel depending on a subset of input channels), and whole-channel [] (some channels removed entirely). And even this set does not cover all possibilities---a rank 4 weight tensor can be sparse along $2^4$ subsets of axes, and this is not even considering the different constraints on sparsity that might be allowed for each axis (e.g., unconstrained vs block-sparse). [han-prune-how] partially address this question by evaluating a subset of types of sparsity applied along a fixed set of axes, but this work is far from exhaustive.

\item \textbf{Should pruning seek to preserve, or disrupt, the original function computed by a trained network?} Most pruning papers implicitly assume that pruning should strive to preserve the network's original function, often framing their goal in terms of eliminating unimportant weights while preserving important ones. This interpretation is not only intuitive, but consistent with the finding that many pruning methods perform better than random pruning (at least when pruning by a large amount). It is also consistent with the the apparent truth of the relaxed version of the lottery ticket hypothesis. However, the experiments of [pruning-largest] suggest that pruning methods which disrupt the network more (as measured by post-pruning drop in validation accuracy) end up yielding better models after fine-tuning. How do we reconcile these two sets of findings?

\item \textbf{How early can we identify full-quality but smaller subnetworks?} [snip] showed that reasonably good sparse subnetworks can be obtained even before initialization, and [lottery-tix-followup] showed that one can obtain increasingly good subnetworks the longer into training one waits before applying iterative magnitude pruning. Is there another pruning method that reaches ``full'' quality earlier in the training process, or even before it begins?

\item \textbf{Is there substantial benefit to integrating pruning directly into the training process?} Several methods propose to obtain sparse models by integrating sparsification into the training processs. This is much less convenient than pruning a pretrained model, since it likely requires one to train a large model on a large dataset, instead of using one provided by existing libraries. However, it might be worth it if there is significant benefit. While these papers argue that it helps, we are not aware of an apples-to-apples comparison of two otherwise identical pruning procedures, one of which is part of the model's initial training.

% \item \textbf{What are the sources of improved efficiency in pruned networks?} As discussed before, there is evidence that, for a given number of parameters, a sparse network will outperform a dense one. Is pruning merely a means of obtaining

\item \textbf{What is the relationship between pruning and balancing the size hyperparameters of a network?} [efficientNet] showed that carefully balancing the input resolution, number of channels, and number of layers is critical for obtaining an efficient architecture. Is pruning simply a post-hoc means of obtaining this balance? How well can one still prune a model like EfficientNet [efficientNet] that already has this balance close to correct? It would also be interesting to know whether the apparent benefit of sparsity for a given number of parameters is an artifact of improved balance, or a separate phenomenon.

\item \textbf{Which saliency criteria tend to perform best?} Many pruning methods entail discarding the weights that score lowest according to some saliency criterion, such as absolute value. While many papers have proposed such criteria, these papers typically also propose other changes that make the impact of the saliency criterion hard to assess.

\item \textbf{How should one schedule pruning operations?} One way to prune an existing neural network is to remove the target fraction of weights all at once. An alternative that many papers adopt is to instead gradually prune parameters over the course of fine-tuning. How should one schedule this pruning over time? And how do factors such as original dataset size, target dataset size (in the case of transfer learning), and model size affect this answer?

\item \textbf{Under what circumstances does the relaxed lottery ticket hypothesis hold?} Existing work has shown that it appears to hold for several networks and datasets, but all use the same unstructured, iterative, magnitude-based pruning method. Does this result generalize to other sparsity structures, pruning methods, models, and datasets?

\end{itemize}
