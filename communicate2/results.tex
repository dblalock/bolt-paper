
To assess \ours's effectiveness in practice, we implemented both it and existing algorithms in C++ and Python. All of our code and raw numerical results are publicly available, along with additional experiments omitted from this section due to space constraints.\footnote{https://smarturl.it/\ours}. In order to avoid implementation bias, we built upon the source code provided by \citep{bolt}\footnote{https://github.com/dblalock/bolt}, which has the most in common with our method and includes highly tuned implementations of many algorithms to which we compare. All experiments use a single thread on a 2013 Macbook Pro with a 2.6GHz Intel Core i7-4960HQ processor. Unless stated otherwise, all timing results use five trials, with each trial reporting the lowest time it took the code to execute in 20 runs. We use the best, rather than average, since this is standard practice in performance benchmarking and is robust to the purely additive noise introduced by competing CPU tasks.

Because nearly all existing work on approximate matrix multiplication either focuses on special cases that do not satisfy our problem definition \cite{quicker-adc, pq, opq} or synthetic matrices, there is not a clear set of benchmark matrix multiply tasks to use. We therefore propose a collection of tasks that we believe are both reproducible and representative of many real-world matrices. To the best of our knowledge, our experiments use oven an order of magnitude more matrices than any previous study.% These tasks include multiplication of hundreds of real-world matrices, which is, to the best of our knowledge, orders of magnitude more than have been used in any previous study.

% ------------------------------------------------
\subsection{Methods Tested}
% ------------------------------------------------
Recall that most baselines take the form of selecting a matrix $\V \in \R^{D \times d}, d < D$ such that $\A \B \approx (\A \V) (\V^\top \B)$. Here $d$ is a free parameter that adjusts the quality vs speed tradeoff. We therefore characterize these methods by how they set $\V$.

\begin{itemize}\itemsep0em
    \item PCA. Set $\V$ equal to the top principal components of the training matrix $\tilde{\A}$.
    \item SparsePCA. Set $\V = \argmin_{\V} \frac{1}{N_{train}} FOO + \lambda \norm{\V}_1$ TODO. This is not the only dictionary learning formulation referred to as SparsePCA [], but it is reasonably representative and is the only one with support in any major Python machine learning library.\footnote{TODO sklearn link here}.

    Because SparsePCA requires the tuning of both $d$ and $\lambda$ for each matrix, we allowed it tune these parameters \textit{on the test set} to ensure that insufficient hyperparameter tuning did not hamper its performance. See Appendix~\ref{sec:experimentDetails} for more information.

    % First, for any given $d$ value and level of sparsity, we report
    % For each matrix product, we tried $\lambda \in 2^i, i \in \{-5, -4, -3, -2, -1, 0, 1, 2, 3\}$ for each matrix product. To ensure that our results err on the side of optimism (and also speed up our experiments), we report the best
    % selected the lambda \textit{post-hoc}---i.e., we cherrypicked the best results on the test set rather than cross-validating,
    % \item FastJL \cite{fastjl}. Note that we don't time the fast hadamard transform step.
    \item HashJL \cite{hashjl}. $\V$ is zero except for a $\pm 1$ in each row, with both sign and position chosen uniformly at random.
    % \item OSNAP \cite{osnap}. The OSNAP sketch is essentially a collection of $s$ HashJL sketches, each of dimensionality $d / s$. Following \cite{iSVD}, we set $s = 4$. We also tried other values of $s$ and found that other values did not perform significantly better (except $s=1$, which reduces to HashJL).
    % \item RandGauss []. The entries of $\V$ are drawn i.i.d. from $\mathcal{N}(0, D^{-\frac{1}{2}})$.
    \item RandGauss []. The entries of $\V$ are drawn i.i.d. from $\mathcal{N}(0, D^{-1})$.
    % \item OrthoGauss []. The entries of $\V$ initialized as in RandGauss, and then $\V$ is set to the $\mat{Q}$ matrix in a QR decomposition of a RandGauss matrix.
    % \item Rademacher \cite{sparseJL}. The entries of $\V$ are i.i.d. Rademacher random variables scaled by $\frac{1}{\sqrt{D}}$. TODO is that the right scale?
    \item Bolt \cite{bolt}. As discussed in Section~\ref{sec:relatedWork} Bolt is the most similar to our own method. It is not optimized for small $M$, however, effectively performing a preliminary matrix product with $M=16$ as a preprocessing step.
    \item Exact Multiplication. We simply compute the matrix product $\A\B$ using a modern BLAS implementation. We also implemented our own matrix product function specialized for tall, skinny matrices. In all cases, we report the timings based on the better of these two functions for a given matrix product.
\end{itemize}

In addition to these baselines, we test two variations of our method:
\begin{itemize}
    \item \ours. This is the algorithm described in Section~\ref{sec:methods}.
    \item \ours-PQ. This is a handicapped version of \oursp without the prototype optimization step. The disparity in performance between \oursp and \ours-PQ is the gain from optimizing the prototypes.
\end{itemize}

We also compared to many additional methods (see Appendix~\ref{sec:experimentDetails}), but omit their results since they were not competitive with the listed baselines.

% ------------------------------------------------
\subsection{How fast is \ours?}
% ------------------------------------------------

We begin by profiling the raw speed of our method and the most similar baselines. In Figure~\ref{fig:encodeSpeed}, we time the $g(\A)$ functions for $\A$ matrices with $2^15$ rows and varying number of columns $D$. Following \cite{bolt}, we also vary the size of the row encodings, profiling 8, 16, and 32 bytes per row. \oursp is not only faster than existing methods, but faster than the machine's memory bandwidth. This is possible because it only reads $O(C)$ columns, and $C$ can be lower than $D$.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{amm/encode_speed}
\caption{\oursp encodes the matrix $\A$ orders of magnitude more quickly than existing vector quantization methods.}
\label{fig:encodeSpeed}
\end{center}
\end{figure}

We also profile the speed of our aggregation function $f(\cdot, \cdot)$ using the same baselines as \citet{bolt}. Though less dramatic a speedup than for the encoding function, we see that our average-based aggregation is significantly faster than the upcasting-based method of Bolt, its nearest rival.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{amm/scan_speed}
\caption{Given the encoded matrices, \oursp computes the approximate output twice as fast as the fastest existing method.}
\label{fig:scanSpeed}
\end{center}
\end{figure}

% ------------------------------------------------
\subsection{Softmax Classifier}
% ------------------------------------------------

As described in section~\ref{sec:intro}, we approximated linear classifiers on the widely used CIFAR-10 and CIFAR-100 datasets []. The classifiers use as input features the 512-dimensional activations of an open-source, VGG-like neural networks trained on each dataset []. The matrices $\A$ are the $10000 \times 512$-dimensional floating point activations for the full test sets, and the matrices $\B$ are each original network's final dense layer. The $50000 \times 512$-dimensional activations from the training set served as the training matrices $\tilde{\A}$.

As shown in Figure~\ref{fig:cifar}, \oursp significantly outperforms all existing methods, achieving near-perfect accuracy more than an order of magnitude faster than brute force.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{amm/cifar_Speedup_Accuracy}
\caption{\oursp achieves a far better speed-accuracy tradeoff than any existing method when approximating two softmax classifiers.}
\label{fig:cifar}
\end{center}
\end{figure}

% ------------------------------------------------
\subsection{Kernel-based classification}
% ------------------------------------------------

To assess the efficacy of our method on a larger and more diverse set of datasets than simply CIFAR-10 and CIFAR-100, we trained kernel classifiers on the datasets from the UCR Time Series Archive []. In addition to being a large, public corpus of over a hundred datasets from a huge variety of different domains, it also has the advantage that it can be used to produce matrix multiplication tasks of a fixed size. This is necessary for meaningful comparison of performance across datasets. We constructed training and test matrices $\tilde{\A}$ and $\A$ by resampling each time series in each dataset's train and test set to a length of $320$ (the closest multiple of 32 to the median length of 310). We obtained the matrix $\B$ for each dataset by running Stochastic Neighbor Compression [] on the training set with an RBF kernel of bandwidth one. We set the number of returned neighbors to 128 (results with 64 and 256 were similar), yielding a $\B$ matrix of size $320 \times 128$. See Appendix~\ref{sec:experimentDetails} for additional details.

This is not the state-of-the-art means of classifying time series, but it does yield fixed-sized matrices and is representative of several modern techniques for accelerating classifiers []. This setup also complements the CIFAR results by being an extremely difficult task, since Stochastic Neighbor Compression has already optimized the classifer to avoid redundancy. % since the distances between time series are often far smaller than their euclidean lengths; this property means that even small amounts of approximation error are sufficient to change predictions. It is also difficult because the $\B$ matrix has been optimized to avoid redundancy, so any acceleration

As shown in Figure~\ref{fig:ucr}, \oursp is significantly faster at a given level of accuracy. A counter-intuitive result, however is that optimization of the prototypes is occasionally worse than not optimizing them (see the red line dipping below the blue one in the lower subplot). Since the optimization strictly increases the expressive power, we believe that this is a product of overfitting and could be corrected were we to tune the ridge regression's parameter (instead of always using $\lambda = 1$). This subplot also reveals that the most stringent degredation requirements can sometimes only be satisfied by PCA at a low speedup, since this is the only curve close to $1$.
% , at high accuracy preservation thresholds,  % However as indicated by \oursp never approaching a fraction of 1 on the bottom subplot, \oursp does not consistently preserve the full accuracy. This suggests that, for difficult classification problems in which almost no accuracy can be sacrificed, \outsp is often not the best choice. The only methods that reliably preserve nearly the full original accuracy are PCA with a small speedup and, with certain parameter settings, Bolt.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{amm/ucr2_Speedup_Relative Accuracy_rbf}
\caption{Fraction of UCR datasets for which each method preserves a given fraction of the original accuracy, versus the method's degree of speedup. \oursp enables much greater speedups for a given level of accuracy degredation}
\label{fig:ucr}
\end{center}
\end{figure}

% ------------------------------------------------
\subsection{Image Processing}
% ------------------------------------------------

To test the extreme limits of \ours, we benchmarked the various techniques' ability to apply small filters to images. As representative examples we chose $3 \times 3$ Sobel filters and $5 \times 5$ gaussian filters. The former are common high-pass filters and the latter are common low-pass filters. We took the first 10 images from the first 50 classes of the Caltech101 dataset as the training set, and the first 10 images from the remaining 51 classes as the test set. We constructed the $\A$ matrices by extracting each patch of each image in

To allow meaningful speed comparisons across images, we resized and center cropped each image to $224 \times 224$ as commonly done in image classification pipelines []. We then extracted sliding windows of the appropriate size and used each (flattened) window as one row of $\tilde{\A}$ or $\A$. We similarly flattened the filters, with each set of coefficients forming one column of $\B$. In both cases, $\B$ has two columns---this is because using a single filter would mean timing a matrix-vector product instead of a matrix-matrix product. Moreover, because Sobel filters are often used in horizontal and vertical pairings, and Gaussian filters are often used together to perform difference-of-Gaussians transforms. See Appendix~\ref{sec:experimentDetails} for additional details.

% \frac{1}{NM}
We report the normalized mean-squared error (NMSE), which is defined as $\norm{\mat{\hat{C}}_{i,j} - \A\B}^2_F / \norm{\A\B}^2_F$, where $\hat{\mat{C}}$ is the method's approximation of $\A\B$. An NMSE of 0 is perfect and an NMSE of 1 corresponds to always predicting 0.

In Figure~\ref{fig:caltech}, we see that it is only \oursp that offers any advantage over exact matrix products. This is likely due to the fact that two columns afford almost no time to amortize preprocessing costs for $\A$; indeed, rival vector quantization methods cannot logically do less worth than brute force in this setting, and dense linear methods can only save work by embedding rows of $\A$ in one-dimensional space.

\oursp performs much worse on the high-pass filters (top) than the low-pass filters (bottom). This is likely because the former produce outputs with variance that is orders of magnitude lower than that of the original image; this means that the NMSE denominator, $\norm{\A\B}^2_F$ is extremely small.% relative to $\norm{\A}_F$ and $\norm{\B}_F$.

% a method must preserve nearly all the information about the image to get $\norm{\mat{\hat{C}}_{i,j} - \A\B}^2_F$

% Notes
%   -CHW order
%   -resized and center cropped to 224x224 as is common in deep learning
%   -stride of (2, 2) on training set because

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{amm/caltech_Speedup_1 - NMSE}
\caption{Despite having only $M=2$ columns in the matrix $\B$, \oursp still achieves a significant speedup with reasonable accuracy. It is, however, much more effective for approximating a 5x5 low-pass filter (bottom) than a 3x3 high-pass filter (top). Methods that did not simultanesouly perform better than chance and provide any speedup over exact matrix multiplication are not shown.}
\label{fig:caltech}
\end{center}
\end{figure}

% % ------------------------------------------------
% \subsection{Summary}
% % ------------------------------------------------

% Our method without \textit{any} hyperparameter tuning outperforms its nearest rival with all its hyperameters tuned \textit{on the test set}.
