
@article{santurkar_how_2018,
	title = {How {Does} {Batch} {Normalization} {Help} {Optimization}?},
	url = {http://arxiv.org/abs/1805.11604},
	abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape signiﬁcantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. These ﬁndings bring us closer to a true understanding of our DNN training toolkit.},
	language = {en},
	urldate = {2018-11-05},
	journal = {arXiv:1805.11604 [cs, stat]},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
	month = may,
	year = {2018},
	note = {arXiv: 1805.11604},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{balestriero_hard_2018,
	title = {From {Hard} to {Soft}: {Understanding} {Deep} {Network} {Nonlinearities} via {Vector} {Quantization} and {Statistical} {Inference}},
	shorttitle = {From {Hard} to {Soft}},
	url = {http://arxiv.org/abs/1810.09274},
	abstract = {Nonlinearity is crucial to the performance of a deep (neural) network (DN). To date there has been little progress understanding the menagerie of available nonlinearities, but recently progress has been made on understanding the roˆle played by piecewise afﬁne and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling. In particular, DN layers constructed from these operations can be interpreted as max-afﬁne spline operators (MASOs) that have an elegant link to vector quantization (VQ) and K-means. While this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise afﬁne and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax. This paper extends the MASO framework to these and an inﬁnitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs). We show that, under a GMM, piecewise afﬁne, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural “hard” VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding “soft” VQ inference problems. We further extend the framework by hybridizing the hard and soft VQ optimizations to create a β-VQ inference that interpolates between hard, soft, and linear VQ inference. A prime example of a β-VQ DN nonlinearity is the swish nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation. Finally, we validate with experiments an important assertion of our theory, namely that DN performance can be signiﬁcantly improved by enforcing orthogonality in its linear ﬁlters.},
	language = {en},
	urldate = {2018-11-05},
	journal = {arXiv:1810.09274 [cs, stat]},
	author = {Balestriero, Randall and Baraniuk, Richard G.},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.09274},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{song_complexity_nodate,
	title = {On the {Complexity} of {Learning} {Neural} {Networks}},
	abstract = {The stunning empirical successes of neural networks currently lack rigorous theoretical explanation. What form would such an explanation take, in the face of existing complexity-theoretic lower bounds? A ﬁrst step might be to show that data generated by neural networks with a single hidden layer, smooth activation functions and benign input distributions can be learned efﬁciently. We demonstrate here a comprehensive lower bound ruling out this possibility: for a wide class of activation functions (including all currently used), and inputs drawn from any logconcave distribution, there is a family of one-hidden-layer functions whose output is a sum gate, that are hard to learn in a precise sense: any statistical query algorithm (which includes all known variants of stochastic gradient descent with any loss function) needs an exponential number of queries even using tolerance inversely proportional to the input dimensionality. Moreover, this hard family of functions is realizable with a small (sublinear in dimension) number of activation units in the single hidden layer. The lower bound is also robust to small perturbations of the true weights. Systematic experiments illustrate a phase transition in the training error as predicted by the analysis.},
	language = {en},
	author = {Song, Le and Vempala, Santosh and Wilmes, John and Xie, Bo},
	pages = {9},
}

@article{du_gradient_nodate,
	title = {Gradient {Descent} {Can} {Take} {Exponential} {Time} to {Escape} {Saddle} {Points}},
	abstract = {Although gradient descent (GD) almost always escapes saddle points asymptotically [Lee et al., 2016], this paper shows that even with fairly natural random initialization schemes and non-pathological functions, GD can be signiﬁcantly slowed down by saddle points, taking exponential time to escape. On the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et al., 2017] is not slowed down by saddle points—it can ﬁnd an approximate local minimizer in polynomial time. This result implies that GD is inherently slower than perturbed GD, and justiﬁes the importance of adding perturbations for efﬁcient non-convex optimization. While our focus is theoretical, we also present experiments that illustrate our theoretical ﬁndings.},
	language = {en},
	author = {Du, Simon S and Jin, Chi and Lee, Jason D and Jordan, Michael I and Singh, Aarti and Poczos, Barnabas},
	pages = {11},
}

@article{bietti_invariance_nodate,
	title = {Invariance and {Stability} of {Deep} {Convolutional} {Representations}},
	abstract = {In this paper, we study deep signal representations that are near-invariant to groups of transformations and stable to the action of diffeomorphisms without losing signal information. This is achieved by generalizing the multilayer kernel introduced in the context of convolutional kernel networks and by studying the geometry of the corresponding reproducing kernel Hilbert space. We show that the signal representation is stable, and that models from this functional space, such as a large class of convolutional neural networks, may enjoy the same stability.},
	language = {en},
	author = {Bietti, Alberto and Mairal, Julien},
	pages = {11},
}

@article{xiao_dynamical_nodate,
	title = {Dynamical {Isometry} and a {Mean} {Field} {Theory} of {CNNs}: {How} to {Train} 10,000-{Layer} {Vanilla} {Convolutional} {Neural} {Networks}},
	abstract = {In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers. A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging. While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs. In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. We derive this initialization scheme theoretically by developing a mean ﬁeld theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix. These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving. We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efﬁcient training of extremely deep architectures.},
	language = {en},
	author = {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S and Pennington, Jeffrey},
	pages = {10},
}

@article{patel_probabilistic_2015,
	title = {A {Probabilistic} {Theory} of {Deep} {Learning}},
	url = {http://arxiv.org/abs/1504.00641},
	abstract = {A grand challenge in machine learning is the development of computational algorithms that match or outperform humans in perceptual inference tasks that are complicated by nuisance variation. For instance, visual object recognition involves the unknown object position, orientation, and scale in object recognition while speech recognition involves the unknown voice pronunciation, pitch, and speed. Recently, a new breed of deep learning algorithms have emerged for high-nuisance inference tasks that routinely yield pattern recognition systems with near- or super-human capabilities. But a fundamental question remains: Why do they work? Intuitions abound, but a coherent framework for understanding, analyzing, and synthesizing deep learning architectures has remained elusive. We answer this question by developing a new probabilistic framework for deep learning based on the Deep Rendering Model: a generative probabilistic model that explicitly captures latent nuisance variation. By relaxing the generative model to a discriminative one, we can recover two of the current leading deep learning systems, deep convolutional neural networks and random decision forests, providing insights into their successes and shortcomings, as well as a principled route to their improvement.},
	language = {en},
	urldate = {2018-11-05},
	journal = {arXiv:1504.00641 [cs, stat]},
	author = {Patel, Ankit B. and Nguyen, Tan and Baraniuk, Richard G.},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.00641},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
}

@article{zhang_nderstanding_2017,
	title = {{NDERSTANDING} {DEEP} {LEARNING} {REQUIRES} {RE}},
	abstract = {Despite their massive size, successful deep artiﬁcial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.},
	language = {en},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz},
	year = {2017},
	pages = {15},
}

@article{nguyen_semi-supervised_2016,
	title = {Semi-{Supervised} {Learning} with the {Deep} {Rendering} {Mixture} {Model}},
	url = {http://arxiv.org/abs/1612.01942},
	abstract = {Semi-supervised learning algorithms reduce the high cost of acquiring labeled training data by using both labeled and unlabeled data during learning. Deep Convolutional Networks (DCNs) have achieved great success in supervised tasks and as such have been widely employed in the semi-supervised learning. In this paper we leverage the recently developed Deep Rendering Mixture Model (DRMM), a probabilistic generative model that models latent nuisance variation, and whose inference algorithm yields DCNs. We develop an EM algorithm for the DRMM to learn from both labeled and unlabeled data. Guided by the theory of the DRMM, we introduce a novel nonnegativity constraint and a variational inference term. We report state-of-the-art performance on MNIST and SVHN and competitive results on CIFAR10. We also probe deeper into how a DRMM trained in a semi-supervised setting represents latent nuisance variation using synthetically rendered images. Taken together, our work provides a uniﬁed framework for supervised, unsupervised, and semisupervised learning.},
	language = {en},
	urldate = {2018-11-05},
	journal = {arXiv:1612.01942 [cs, stat]},
	author = {Nguyen, Tan and Liu, Wanjia and Perez, Ethan and Baraniuk, Richard G. and Patel, Ankit B.},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.01942},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{kent_input-convex_nodate,
	title = {Input-{Convex} {Neural} {Networks} and {Posynomial} {Optimization}},
	abstract = {Fitting a model to data is a general problem that arises in many ﬁelds. Recently, deep neural networks have been successfully applied to modeling natural images, speech, and language. Posynomial models are another class of models that have been widely applied to problems in engineering design due to their tractability as an optimization problem. Both of these approaches are parametric models for function ﬁtting, though they make different assumptions about the structure of the function being modeled. Neural networks are well-known to be universal function approximators, but this expressiveness comes at the cost of being highly non-convex in their parameters, making them difﬁcult to optimize. By contrast, optimizing posynomial functions can be re-formulated as a convex optimization problem, but these models impose many restrictions on the structure of the learned function. In this paper, we explore connections between deep neural networks and posynomials. First, we explore a network model that trades off expressiveness for more principled optimization. We investigate a novel application of such a network and then we formulate such a model as a posynomial. Finally, we consider the problem of ﬁtting a posynomial function with a neural network and demonstrate fast and accurate posynomial ﬁtting using deep learning machinery.},
	language = {en},
	author = {Kent, Spencer and Mazumdar, Eric and Nagabandi, Anusha and Rakelly, Kate},
	pages = {13},
}

@article{magnani_convex_2009,
	title = {Convex piecewise-linear fitting},
	volume = {10},
	issn = {1389-4420, 1573-2924},
	url = {http://link.springer.com/10.1007/s11081-008-9045-3},
	doi = {10.1007/s11081-008-9045-3},
	abstract = {We consider the problem of ﬁtting a convex piecewise-linear function, with some speciﬁed form, to given multi-dimensional data. Except for a few special cases, this problem is hard to solve exactly, so we focus on heuristic methods that ﬁnd locally optimal ﬁts. The method we describe, which is a variation on the K-means algorithm for clustering, seems to work well in practice, at least on data that can be ﬁt well by a convex function. We focus on the simplest function form, a maximum of a ﬁxed number of afﬁne functions, and then show how the methods extend to a more general form.},
	language = {en},
	number = {1},
	urldate = {2018-11-05},
	journal = {Optimization and Engineering},
	author = {Magnani, Alessandro and Boyd, Stephen P.},
	month = mar,
	year = {2009},
	pages = {1--17},
}

@article{balestriero_mad_2018,
	title = {Mad {Max}: {Affine} {Spline} {Insights} into {Deep} {Learning}},
	shorttitle = {Mad {Max}},
	url = {http://arxiv.org/abs/1805.06576},
	abstract = {We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space that is implicitly induced by a MASO directly links DNs to the theory of vector quantization (VQ) and \$K\$-means clustering, which opens up new geometric avenue to study how DNs organize signals in a hierarchical fashion. To validate the utility of the VQ interpretation, we develop and validate a new distance metric for signals and images that quantifies the difference between their VQ encodings. (This paper is a significantly expanded version of A Spline Theory of Deep Learning from ICML 2018.)},
	language = {en},
	urldate = {2018-11-05},
	journal = {arXiv:1805.06576 [cs, stat]},
	author = {Balestriero, Randall and Baraniuk, Richard},
	month = may,
	year = {2018},
	note = {arXiv: 1805.06576},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{saxe_information_2018,
	title = {{ON} {THE} {INFORMATION} {BOTTLENECK} {THEORY} {OF} {DEEP} {LEARNING}},
	abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three speciﬁc claims: ﬁrst, that deep networks undergo two distinct phases consisting of an initial ﬁtting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case. Through a combination of analytical results and simulation, we demonstrate that the information plane trajectory is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like tanh yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we ﬁnd that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB ﬁndings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the ﬁtting process rather than during a subsequent compression period.},
	language = {en},
	author = {Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
	year = {2018},
	pages = {27},
}

@article{tishby_deep_2015,
	title = {Deep {Learning} and the {Information} {Bottleneck} {Principle}},
	url = {http://arxiv.org/abs/1503.02406},
	abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We ﬁrst show that any DNN can be quantiﬁed by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain ﬁnite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantiﬁable both by the generalization bound and by the network’s simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
	language = {en},
	urldate = {2018-11-05},
	journal = {arXiv:1503.02406 [cs]},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02406},
	keywords = {Computer Science - Machine Learning},
}

@article{shwartz-ziv_opening_2017,
	title = {Opening the {Black} {Box} of {Deep} {Neural} {Networks} via {Information}},
	url = {http://arxiv.org/abs/1703.00810},
	abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work [Tishby and Zaslavsky (2015)] proposed to analyze DNNs in the Information Plane; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer.},
	language = {en},
	urldate = {2018-11-05},
	journal = {arXiv:1703.00810 [cs]},
	author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.00810},
	keywords = {Computer Science - Machine Learning},
}

@article{tishby_information_2000,
	title = {The information bottleneck method},
	url = {http://arxiv.org/abs/physics/0004057},
	abstract = {We define the relevant information in a signal \$x{\textbackslash}in X\$ as being the information that this signal provides about another signal \$y{\textbackslash}in {\textbackslash}Y\$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal \$x\$ requires more than just predicting \$y\$, it also requires specifying which features of \${\textbackslash}X\$ play a role in the prediction. We formalize this problem as that of finding a short code for \${\textbackslash}X\$ that preserves the maximum information about \${\textbackslash}Y\$. That is, we squeeze the information that \${\textbackslash}X\$ provides about \${\textbackslash}Y\$ through a `bottleneck' formed by a limited set of codewords \${\textbackslash}tX\$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure \$d(x,{\textbackslash}x)\$ emerges from the joint statistics of \${\textbackslash}X\$ and \${\textbackslash}Y\$. This approach yields an exact set of self consistent equations for the coding rules \$X {\textbackslash}to {\textbackslash}tX\$ and \${\textbackslash}tX {\textbackslash}to {\textbackslash}Y\$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
	language = {en},
	urldate = {2018-11-05},
	journal = {arXiv:physics/0004057},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	month = apr,
	year = {2000},
	note = {arXiv: physics/0004057},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Data Analysis, Statistics and Probability},
}

@article{arora_provable_2017,
	title = {Provable benefits of representation learning},
	url = {http://arxiv.org/abs/1706.04601},
	abstract = {There is general consensus that learning representations is useful for a variety of reasons, e.g. efﬁcient use of labeled data (semi-supervised learning), transfer learning and understanding hidden structure of data. Popular techniques for representation learning include clustering, manifold learning, kernel-learning, autoencoders, Boltzmann machines, etc.},
	language = {en},
	urldate = {2018-11-05},
	journal = {arXiv:1706.04601 [cs, stat]},
	author = {Arora, Sanjeev and Risteski, Andrej},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.04601},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{raghu_expressive_2016,
	title = {On the {Expressive} {Power} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1606.05336},
	abstract = {We propose a novel approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Understanding expressivity is a classical issue in the study of neural networks, but it has remained challenging at both a conceptual and a practical level. Our approach is based on an interrelated set of measures of expressivity, uniﬁed by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. We show how our framework provides insight both into randomly initialized networks (the starting point for most standard optimization methods) and for trained networks.},
	language = {en},
	urldate = {2018-11-05},
	journal = {arXiv:1606.05336 [cs, stat]},
	author = {Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.05336},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
}
