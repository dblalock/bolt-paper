
@article{melis_state_2017,
	title = {On the {State} of the {Art} of {Evaluation} in {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/1707.05589},
	abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady inﬂux of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.},
	language = {en},
	urldate = {2019-07-25},
	journal = {arXiv:1707.05589 [cs]},
	author = {Melis, Gábor and Dyer, Chris and Blunsom, Phil},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.05589},
	keywords = {Computer Science - Computation and Language},
}

@article{lipton_troubling_2018,
	title = {Troubling {Trends} in {Machine} {Learning} {Scholarship}},
	url = {http://arxiv.org/abs/1807.03341},
	abstract = {Collectively, machine learning (ML) researchers are engaged in the creation and dissemination of knowledge about data-driven algorithms. In a given paper, researchers might aspire to any subset of the following goals, among others: to theoretically characterize what is learnable, to obtain understanding through empirically rigorous experiments, or to build a working system that has high predictive accuracy. While determining which knowledge warrants inquiry may be subjective, once the topic is fixed, papers are most valuable to the community when they act in service of the reader, creating foundational knowledge and communicating as clearly as possible. Recent progress in machine learning comes despite frequent departures from these ideals. In this paper, we focus on the following four patterns that appear to us to be trending in ML scholarship: (i) failure to distinguish between explanation and speculation; (ii) failure to identify the sources of empirical gains, e.g., emphasizing unnecessary modifications to neural architectures when gains actually stem from hyper-parameter tuning; (iii) mathiness: the use of mathematics that obfuscates or impresses rather than clarifies, e.g., by confusing technical and non-technical concepts; and (iv) misuse of language, e.g., by choosing terms of art with colloquial connotations or by overloading established technical terms. While the causes behind these patterns are uncertain, possibilities include the rapid expansion of the community, the consequent thinness of the reviewer pool, and the often-misaligned incentives between scholarship and short-term measures of success (e.g., bibliometrics, attention, and entrepreneurial opportunity). While each pattern offers a corresponding remedy (don't do it), we also discuss some speculative suggestions for how the community might combat these trends.},
	language = {en},
	urldate = {2019-07-25},
	journal = {arXiv:1807.03341 [cs, stat]},
	author = {Lipton, Zachary C. and Steinhardt, Jacob},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.03341},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{sculley_winners_2018,
	title = {Winner's {Curse}? {On} {Pace}, {Progression}, {Progress}, and {Empirical} {Rigor}},
	abstract = {The ﬁeld of ML is distinguished both by rapid innovation and rapid dissemination of results. While the pace of progress has been extraordinary by any measure, in this paper we explore potential issues that we believe to be arising as a result. In particular, we observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the ﬁeld as a whole. This short position paper highlights examples where progress has actually been slowed as a result, offers thoughts on incentive structures currently at play, and gives suggestions as seeds for discussions on productive change.},
	language = {en},
	author = {Sculley, D and Snoek, Jasper and Rahimi, Ali and Wiltschko, Alex},
	year = {2018},
	pages = {4},
}

@article{dacrema_are_2019,
	title = {Are {We} {Really} {Making} {Much} {Progress}? {A} {Worrying} {Analysis} of {Recent} {Neural} {Recommendation} {Approaches}},
	shorttitle = {Are {We} {Really} {Making} {Much} {Progress}?},
	url = {http://arxiv.org/abs/1907.06902},
	doi = {10.1145/3298689.3347058},
	abstract = {Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area. Source code of our experiments and full results are available at: https://github.com/MaurizioFD/RecSys2019\_DeepLearning\_Evaluation.},
	language = {en},
	urldate = {2019-07-25},
	journal = {arXiv:1907.06902 [cs]},
	author = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.06902},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Information Retrieval},
}

@inproceedings{armstrong_improvements_2009,
	address = {Hong Kong, China},
	title = {Improvements that don't add up: ad-hoc retrieval results since 1998},
	isbn = {978-1-60558-512-3},
	shorttitle = {Improvements that don't add up},
	url = {http://portal.acm.org/citation.cfm?doid=1645953.1646031},
	doi = {10.1145/1645953.1646031},
	abstract = {The existence and use of standard test collections in information retrieval experimentation allows results to be compared between research groups and over time. Such comparisons, however, are rarely made. Most researchers only report results from their own experiments, a practice that allows lack of overall improvement to go unnoticed. In this paper, we analyze results achieved on the TREC Ad-Hoc, Web, Terabyte, and Robust collections as reported in SIGIR (1998–2008) and CIKM (2004–2008). Dozens of individual published experiments report effectiveness improvements, and often claim statistical signiﬁcance. However, there is little evidence of improvement in ad-hoc retrieval technology over the past decade. Baselines are generally weak, often being below the median original TREC system. And in only a handful of experiments is the score of the best TREC automatic run exceeded. Given this ﬁnding, we question the value of achieving even a statistically signiﬁcant result over a weak baseline. We propose that the community adopt a practice of regular longitudinal comparison to ensure measurable progress, or at least prevent the lack of it from going unnoticed. We describe an online database of retrieval runs that facilitates such a practice.},
	language = {en},
	urldate = {2019-07-25},
	booktitle = {Proceeding of the 18th {ACM} conference on {Information} and knowledge management - {CIKM} '09},
	publisher = {ACM Press},
	author = {Armstrong, Timothy G. and Moffat, Alistair and Webber, William and Zobel, Justin},
	year = {2009},
	pages = {601},
}

@article{keogh_need_2003,
	title = {On the need for time series data mining benchmarks: a survey and empirical demonstration},
	volume = {7},
	number = {4},
	journal = {Data Mining and knowledge discovery},
	author = {Keogh, Eamonn and Kasetty, Shruti},
	year = {2003},
	pages = {349--371},
}

@article{ding_querying_2008,
	title = {Querying and mining of time series data: experimental comparison of representations and distance measures},
	volume = {1},
	number = {2},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ding, Hui and Trajcevski, Goce and Scheuermann, Peter and Wang, Xiaoyue and Keogh, Eamonn},
	year = {2008},
	pages = {1542--1552},
}

@inproceedings{blalock2020,
  author    = {Davis W. Blalock and
               Jose Javier Gonzalez Ortiz and
               Jonathan Frankle and
               John V. Guttag},
  editor    = {Inderjit S. Dhillon and
               Dimitris S. Papailiopoulos and
               Vivienne Sze},
  title     = {What is the State of Neural Network Pruning?},
  booktitle = {Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin,
               TX, USA, March 2-4, 2020},
  publisher = {mlsys.org},
  year      = {2020},
  url       = {https://proceedings.mlsys.org/book/296.pdf},
  biburl    = {https://dblp.org/rec/conf/mlsys/BlalockOFG20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
