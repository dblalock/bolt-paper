
@inproceedings{li_semi-supervised_2015,
	address = {Santiago, Chile},
	title = {Semi-{Supervised} {Zero}-{Shot} {Classification} with {Label} {Representation} {Learning}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410836/},
	doi = {10.1109/ICCV.2015.479},
	abstract = {Given the challenge of gathering labeled training data, zero-shot classiﬁcation, which transfers information from observed classes to recognize unseen classes, has become increasingly popular in the computer vision community. Most existing zero-shot learning methods require a user to ﬁrst provide a set of semantic visual attributes for each class as side information before applying a two-step prediction procedure that introduces an intermediate attribute prediction problem. In this paper, we propose a novel zeroshot classiﬁcation approach that automatically learns label embeddings from the input data in a semi-supervised large-margin learning framework. The proposed framework jointly considers multi-class classiﬁcation over all classes (observed and unseen) and tackles the target prediction problem directly without introducing intermediate prediction problems. It also has the capacity to incorporate semantic label information from different sources when available. To evaluate the proposed approach, we conduct experiments on standard zero-shot data sets. The empirical results show the proposed approach outperforms existing state-of-the-art zero-shot learning methods.},
	language = {en},
	urldate = {2018-11-09},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Li, Xin and Guo, Yuhong and Schuurmans, Dale},
	month = dec,
	year = {2015},
	pages = {4211--4219},
}

@article{jetley_prototypical_2015,
	title = {Prototypical {Priors}: {From} {Improving} {Classification} to {Zero}-{Shot} {Learning}},
	shorttitle = {Prototypical {Priors}},
	url = {http://arxiv.org/abs/1512.01192},
	abstract = {Recent works on zero-shot learning make use of side information such as visual attributes or natural language semantics to deﬁne the relations between output visual classes and then use these relationships to draw inference on new unseen classes at test time. In a novel extension to this idea, we propose the use of visual prototypical concepts as side information. For most real-world visual object categories, it may be difﬁcult to establish a unique prototype. However, in cases such as trafﬁc signs, brand logos, ﬂags, and even natural language characters, these prototypical templates are available and can be leveraged for an improved recognition performance.},
	language = {en},
	urldate = {2018-11-09},
	journal = {arXiv:1512.01192 [cs]},
	author = {Jetley, Saumya and Romera-Paredes, Bernardino and Jayasumana, Sadeep and Torr, Philip},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.01192},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{socher_zero-shot_2013,
	title = {Zero-{Shot} {Learning} {Through} {Cross}-{Modal} {Transfer}},
	url = {http://arxiv.org/abs/1301.3666},
	abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by ﬁrst using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually deﬁned semantic features for either words or images.},
	language = {en},
	urldate = {2018-11-09},
	journal = {arXiv:1301.3666 [cs]},
	author = {Socher, Richard and Ganjoo, Milind and Sridhar, Hamsa and Bastani, Osbert and Manning, Christopher D. and Ng, Andrew Y.},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3666},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}

@article{fu_transductive_2015,
	title = {Transductive {Multi}-label {Zero}-shot {Learning}},
	url = {http://arxiv.org/abs/1503.07790},
	abstract = {Zero-shot learning has received increasing interest as a means to alleviate the often prohibitive expense of annotating training data for large scale recognition problems. These methods have achieved great success via learning intermediate semantic representations in the form of attributes and more recently, semantic word vectors. However, they have thus far been constrained to the single-label case, in contrast to the growing popularity and importance of more realistic multi-label data. In this paper, for the ﬁrst time, we investigate and formalise a general framework for multi-label zero-shot learning, addressing the unique challenge therein: how to exploit multi-label correlation at test time with no training data for those classes? In particular, we propose (1) a multi-output deep regression model to project an image into a semantic word space, which explicitly exploits the correlations in the intermediate semantic layer of word vectors; (2) a novel zero-shot learning algorithm for multi-label data that exploits the unique compositionality property of semantic word vector representations; and (3) a transductive learning strategy to enable the regression model learned from seen classes to generalise well to unseen classes. Our zero-shot learning experiments on a number of standard multi-label datasets demonstrate that our method outperforms a variety of baselines.},
	language = {en},
	urldate = {2018-11-09},
	journal = {arXiv:1503.07790 [cs]},
	author = {Fu, Yanwei and Yang, Yongxin and Hospedales, Tim and Xiang, Tao and Gong, Shaogang},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.07790},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}

@article{chang_semantic_nodate,
	title = {Semantic {Concept} {Discovery} for {Large}-{Scale} {Zero}-{Shot} {Event} {Detection}},
	abstract = {We focus on detecting complex events in unconstrained Internet videos. While most existing works rely on the abundance of labeled training data, we consider a more difﬁcult zero-shot setting where no training data is supplied. We ﬁrst pre-train a number of concept classiﬁers using data from other sources. Then we evaluate the semantic correlation of each concept w.r.t. the event of interest. After further reﬁnement to take prediction inaccuracy and discriminative power into account, we apply the discovered concept classiﬁers on all test videos and obtain multiple score vectors. These distinct score vectors are converted into pairwise comparison matrices and the nuclear norm rank aggregation framework is adopted to seek consensus. To address the challenging optimization formulation, we propose an efﬁcient, highly scalable algorithm that is an order of magnitude faster than existing alternatives. Experiments on recent TRECVID datasets verify the superiority of the proposed approach.},
	language = {en},
	author = {Chang, Xiaojun and Yang, Yi and Hauptmann, Alexander G and Xing, Eric P and Yu, Yao-Liang},
	pages = {7},
}

@incollection{simpleZSL,
	address = {Cham},
	title = {An {Embarrassingly} {Simple} {Approach} to {Zero}-{Shot} {Learning}},
	isbn = {978-3-319-50075-1 978-3-319-50077-5},
	url = {http://link.springer.com/10.1007/978-3-319-50077-5_2},
	abstract = {Zero-shot learning consists in learning how to recognise new concepts by just having a description of them. Many sophisticated approaches have been proposed to address the challenges this problem comprises. In this paper we describe a zero-shot learning approach that can be implemented in just one line of code, yet it is able to outperform state of the art approaches on standard datasets. The approach is based on a more general framework which models the relationships between features, attributes, and classes as a two linear layers network, where the weights of the top layer are not learned but are given by the environment. We further provide a learning bound on the generalisation error of this kind of approaches, by casting them as domain adaptation methods. In experiments carried out on three standard real datasets, we found that our approach is able to perform signiﬁcantly better than the state of art on all of them, obtaining a ratio of improvement up to 17\%.},
	language = {en},
	urldate = {2018-11-09},
	booktitle = {Visual {Attributes}},
	publisher = {Springer International Publishing},
	author = {Romera-Paredes, Bernardino and Torr, Philip H. S.},
	editor = {Feris, Rogerio Schmidt and Lampert, Christoph and Parikh, Devi},
	year = {2017},
	doi = {10.1007/978-3-319-50077-5_2},
	pages = {11--30},
}

@inproceedings{writeAClassifier,
	address = {Sydney, Australia},
	title = {Write a {Classifier}: {Zero}-{Shot} {Learning} {Using} {Purely} {Textual} {Descriptions}},
	isbn = {978-1-4799-2840-8},
	shorttitle = {Write a {Classifier}},
	url = {http://ieeexplore.ieee.org/document/6751432/},
	doi = {10.1109/ICCV.2013.321},
	abstract = {The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classiﬁers for these categories. We propose an approach for zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry, without the need to explicitly deﬁned attributes. We propose and investigate two baseline formulations, based on regression and domain adaptation. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the classiﬁer parameters for new classes. We applied the proposed approach on two ﬁne-grained categorization datasets, and the results indicate successful classiﬁer prediction.},
	language = {en},
	urldate = {2018-11-09},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Elhoseiny, Mohamed and Saleh, Babak and Elgammal, Ahmed},
	month = dec,
	year = {2013},
	pages = {2584--2591},
}
