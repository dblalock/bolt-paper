10c10
< To lay the groundwork for our own method, we begin by reviewing Product Quantization (PQ) \cite{pq}. PQ is a classic vector quantization algorithm for approximating inner products and Euclidean distances and serves as the basis for nearly all vector quantization methods similar to our own. % This not only lays necessary groundwork for explaining our own method, but also affords the opportunity to discuss
---
> To lay the groundwork for our own method, we begin by reviewing Product Quantization (PQ) \cite{pq}. PQ is a classic vector quantization algorithm for approximating inner products and Euclidean distances and serves as the basis for nearly all vector quantization methods similar to our own. Readers who already understand PQ or have read Chapter~\ref{chap:bolt} can safely skip this section. The only significant notation introduced is the use of $C$ to denote the number of codebooks, $K$ to denote the number of prototypes per codebook, and $\mathcal{J}^{(c)}$ to denote the set of indices associated with a given codebook $c$. % However, it will be necessary to know that we will use $P^{(c)} \in \R^{K \times D}$ to denote the centroids for a given subspace and $\mathcal{J}^{(c)}$ to denote the codebook's associated indices.% and denote the matrix of centroids $\P \in \R^{KC \times D}$, with .% This not only lays necessary groundwork for explaining our own method, but also affords the opportunity to discuss
14c14,20
< The basic intuition behind PQ is that $\a^\top \b \approx \hat{\a}^\top \b$, where $\norm{\hat{\a} - \a}$ is small but $\hat{\a}$ has special structure allowing the product to be computed quickly. This structure consists of $\hat{\a}$ being formed by concatenating learned prototypes in disjoint subspaces; one obtains a speedup by precomputing the dot products between $\b$ and the prototypes once, and then reusing these values across many $\a$ vectors. The $\a$ vectors here are the (transposed) rows of $\A$ and the $\b$ vectors are the columns of $\B$.
---
> The basic intuition behind PQ is that
> % $\a^\top \b$,
> % $\approx$,
> % $\hat{\a}^\top \b$,
> $\a^\top \b \approx \hat{\a}^\top \b$,
> where $\norm{\hat{\a} - \a}$
> is small but $\hat{\a}$ has special structure allowing the product to be computed quickly. This structure consists of $\hat{\a}$ being formed by concatenating learned prototypes in disjoint subspaces; one obtains a speedup by precomputing the dot products between $\b$ and the prototypes once, and then reusing these values across many $\a$ vectors. The $\a$ vectors here are the transposed rows of $\A$ and the $\b$ vectors are the columns of $\B$.
23c29
<     \item \textbf{Prototype Learning} - In an initial, offline training phase,  cluster the rows of $\A$ (or a training set $\tilde{\A}$) using K-means to create prototypes. A separate K-means is run in each of $C$ disjoint subspaces to produce $C$ sets of $K$ protoypes. %The prototypes consist of the cartesian product of prototypes within disjoint subspaces.
---
>     \item \textbf{Prototype Learning} - In an initial, offline training phase,  cluster the rows of $\A$ (or a training set $\tilde{\A}$) using $k$-means to create prototypes. A separate $k$-means is run in each of $C$ disjoint subspaces to produce $C$ sets of $K$ prototypes. %The prototypes consist of the cartesian product of prototypes within disjoint subspaces.
26c32
<     \item \textbf{Encoding Function} $g(\a)$ - Determine the most similar prototype to $\a$ in each subspace. Store these assignments as integer indices using $C \log_2(K)$ bits.
---
>     \item \textbf{Encoding Function}, $g(\a)$ - Determine the most similar prototype to $\a$ in each subspace. Store these assignments as integer indices using $C \log_2(K)$ bits.
28c34
<     \item \textbf{Aggregation}, $f(\cdot,\cdot)$ - Use the indices and tables to \textit{lookup} the estimated partial $\a^\top \b$ in each subspace, then sum the results across all $C$ subspaces. %$A[i]^\top B[:, j]$\footnote{Because we will frequently need to refer to slices of rank-3 tensors and above, we employ Python-style slicing notation rather than traditional superscripts and subscripts.} rather than compute it with a series of multiply-adds.
---
>     \item \textbf{Aggregation}, $f(\cdot,\cdot)$ - Use the indices and tables to lookup the estimated partial $\a^\top \b$ in each subspace, then sum the results across all $C$ subspaces. %$A[i]^\top B[:, j]$\footnote{Because we will frequently need to refer to slices of rank-3 tensors and above, we employ Python-style slicing notation rather than traditional superscripts and subscripts.} rather than compute it with a series of multiply-adds.
31c37,38
< PQ is depicted for a single pair of vectors $\a$ and $\b$ in Figure~\ref{fig:pq}. We elaborate upon each of these steps below.
---
> We elaborate upon each of these steps below and refer the reader to Figure~\ref{fig:pq} for a graphical depiction of PQ approximating the product of two vectors.
> % PQ is depicted for a single pair of vectors $\a$ and $\b$ in Figure~\ref{fig:pq}. We elaborate upon each of these steps below.
40c47
< Let $\tilde{A} \in \R^{N \times D}$ be a training set, $K$ be a number of prototypes per subspace, $C$ be a number of subspaces, and $\{\mathcal{J}^{(c)}\}_{c=1}^C$ be the (mutually exclusive and collectively exhaustive) sets of indices associated with each subspace. The training-time task of PQ is to learn $C$ sets of prototypes $\mat{P}^{(c)} \in \R^{K \times |\mathcal{J}^{(c)}|}$ and assignments $\vec{z}^{(c)} \in \R^{N}$ such that:
---
> Let $\tilde{A} \in \R^{N \times D}$ be a training set, $K$ be a number of prototypes per subspace, $C$ be a number of subspaces, and $\{\mathcal{J}^{(c)}\}_{c=1}^C$ be the mutually exclusive and collectively exhaustive sets of indices associated with each subspace. The training-time task of PQ is to learn $C$ sets of prototypes $\mat{P}^{(c)} \in \R^{K \times |\mathcal{J}^{(c)}|}$ and assignments $\vec{z}^{(c)} \in \R^{N}$ such that
46c53,62
< is minimized. It does this by running K-means separately in each subspace $\mathcal{J}^{(c)}$ and using the resulting centroids and assignments to populate $\mat{P}^{(c)}$ and $\vec{z}^{(c)}$.
---
> is minimized. It does this by running $k$-means separately in each subspace $\mathcal{J}^{(c)}$ and using the resulting centroids and assignments to populate $\mat{P}^{(c)}$ and $\vec{z}^{(c)}$.
> % \vspace{4mm}
> % \begin{figure}[t]
> % \begin{center}
> % \includegraphics[width=.9\linewidth]{amm/pq}
> % \caption{Product Quantization. The $g(\cdot)$ function returns the index of the most similar prototype to the data vector $\vec{a}$ in each subspace. The $h(\cdot)$ function computes a lookup table of dot products between the query vector $\b$ and each protoype in each subspace. The aggregation function $f(\cdot,\cdot)$ sums the table entries corresponding to each index.}
> % % $\vec{a}$
> % \label{fig:pq}
> % \end{center}
> % \end{figure}
48,63d63
< \begin{figure}[t]
< \begin{center}
< \includegraphics[width=\linewidth]{amm/pq}
< \caption{Product Quantization. The $g(\cdot)$ function returns the index of the most similar prototype to the data vector a in each subspace. The $h(\cdot)$ function computes a lookup table of dot products between the query vector b and each protoype in each subspace. The aggregation function $f(\cdot,\cdot)$ sums the table entries corresponding to each index.}
< % $\vec{a}$
< \label{fig:pq}
< \end{center}
< \end{figure}
< 
< 
< % Perhaps the simplest means of constructing prototypes would be to run a clustering algorithm like K-means on the rows of $\A$. This would be effective, but would require using a large number of prototypes in order to have each row closely match its prototype. What would be preferable is some means of having a huge number of prototypes without having to pay so large a time and space cost.
< 
< % PQ achieves this goal by using as prototypes the cartesian product of prototypes within disjoint subspaces. Concretely, PQ runs K-means with $K$ centroids in each of $C$ disjoint (usually contiguous) sets of dimensions. This results in $K^C$ possible combinations of centroid assignments for each vector, with each unique combination corresponding to a unique overall prototype.
< 
< % vectors into disjoint ``subvectors'' (each corresponding to a unique set of dimensions) and runs k-means within each subspace.
< %  If there are $K$ prototypes per subspace and $C$ subspaces, this results in $K^C$ possible combinations of prototype assignments for each vector. Viewed differently, this creates $K^C$ overall prototypes whose entries
71c71
< Given the learned prototypes, PQ replaces each row $\a$ of $\A$ with the concatenation of its $C$ K-means centroid assignments in each of the $C$ subspaces. Formally:
---
> Given the learned prototypes, PQ replaces each row $\a$ of $\A$ with the concatenation of its $C$ $k$-means centroid assignments in each of the $C$ subspaces. Formally:
90c90
< Existing work has shown that setting $K = 16$ and quantizing the lookup tables to 8 bits can offer enormous speedups compared to larger $K$ and/or floating-point tables \cite{bolt, quickAdc, quickerAdc}. This is because 16 1-byte entries can be stored in a SIMD register, allowing 16 or more table lookups to be performed in parallel in a single instruction. Since the table entries naturally occupy more than 8 bits even for 8-bit data, some means of quantizing these entries is necessary. This can easily be done by subtracting off the minimum entry in each table and linearly rescaling such that the maximum entry in any table is at most 255. Ignoring rounding error, this affine transform is invertible, and is reflected by the constants $\alpha$ and $\beta$ in equation~\ref{eq:objective}. See Appendix~\ref{sec:lutQuantize} for additional details.
---
> Existing work has shown that setting $K = 16$ and quantizing the lookup tables to 8 bits can offer enormous speedups compared to larger $K$ and/or floating point tables \cite{bolt, quickAdc, quickerAdc}. This is because 16 1-byte entries can be stored in a SIMD register, allowing 16 or more table lookups to be performed in parallel in a single instruction. Since the table entries naturally occupy more than 8 bits even for 8-bit data, some means of quantizing these entries is necessary. This can easily be done by subtracting off the minimum entry in each table and linearly rescaling such that the maximum entry in any table is at most 255. Ignoring rounding error, this affine transform is invertible, and is reflected by the constants $\alpha$ and $\beta$ in Equation~\ref{eq:objective}. See Section~\ref{sec:lutQuantize} for additional details.
