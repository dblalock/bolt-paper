
@article{lottery-transfer,
	title = {One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
	shorttitle = {One ticket to win them all},
	url = {http://arxiv.org/abs/1906.02773},
	abstract = {The success of lottery ticket initializations [7] suggests that small, sparsiﬁed networks can be trained so long as the network is initialized appropriately. Unfortunately, ﬁnding these “winning ticket” initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training conﬁguration (optimizer and dataset) and evaluating their performance on another conﬁguration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods.},
	language = {en},
	urldate = {2019-07-19},
	journal = {arXiv:1906.02773 [cs, stat]},
	author = {Morcos, Ari S. and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.02773},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{snip-followup,
	title = {A {Signal} {Propagation} {Perspective} for {Pruning} {Neural} {Networks} at {Initialization}},
	url = {http://arxiv.org/abs/1906.06307},
	abstract = {Network pruning is a promising avenue for compressing deep neural networks. A typical approach to pruning starts by training a model and removing unnecessary parameters while minimizing the impact on what is learned. Alternatively, a recent approach shows that pruning can be done at initialization prior to training. However, it remains unclear exactly why pruning an untrained, randomly initialized neural network is effective. In this work, we consider the pruning problem from a signal propagation perspective, formally characterizing initialization conditions that ensure faithful signal propagation throughout a network. Based on singular values of a network’s input-output Jacobian, we ﬁnd that orthogonal initialization enables more faithful signal propagation compared to other initialization schemes, thereby enhancing pruning results on a range of modern architectures and datasets. Also, we empirically study the effect of supervision for pruning at initialization, and show that often unsupervised pruning can be as effective as the supervised pruning. Furthermore, we demonstrate that our signal propagation perspective, combined with unsupervised pruning, can indeed be useful in various scenarios where pruning is applied to non-standard arbitrarily-designed architectures.},
	language = {en},
	urldate = {2019-07-19},
	journal = {arXiv:1906.06307 [cs, stat]},
	author = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Gould, Stephen and Torr, Philip H. S.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.06307},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition}
}

@article{prune-largest,
	title = {The {Generalization}-{Stability} {Tradeoff} in {Neural} {Network} {Pruning}},
	url = {http://arxiv.org/abs/1906.03728},
	abstract = {Pruning neural network parameters to reduce model size is an area of much interest, but the original motivation for pruning was the prevention of overﬁtting rather than the improvement of computational efﬁciency. This motivation is particularly relevant given the perhaps surprising observation that a wide variety of pruning approaches confer increases in test accuracy, even when parameter counts are drastically reduced. To better understand this phenomenon, we analyze the behavior of pruning over the course of training, ﬁnding that pruning’s effect on generalization relies more on the instability generated by pruning than the ﬁnal size of the pruned model. We demonstrate that even pruning of seemingly unimportant parameters can lead to such instability, allowing our ﬁnding to account for the generalization beneﬁts of modern pruning techniques. Our results ultimately suggest that, counterintuitively, pruning regularizes through instability and mechanisms unrelated to parameter counts.},
	language = {en},
	urldate = {2019-07-19},
	journal = {arXiv:1906.03728 [cs, stat]},
	author = {Bartoldson, Brian R. and Morcos, Ari S. and Barbu, Adrian and Erlebacher, Gordon},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.03728},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{stabilizing-lottery-tix,
	title = {Stabilizing the {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1903.01611},
	abstract = {Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar beneﬁts during training. In particular, the lottery ticket hypothesis conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably ﬁnds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations.},
	language = {en},
	urldate = {2019-07-19},
	journal = {arXiv:1903.01611 [cs, stat]},
	author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.01611},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition}
}


@article{han-sparse-how,
	title = {Exploring the {Regularity} of {Sparse} {Structure} in {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1705.08922},
	abstract = {Sparsity helps reduce the computational complexity of deep neural networks by skipping zeros. Taking advantage of sparsity is listed as a high priority in the next generation DNN accelerators such as TPU[1]. The structure of sparsity, i.e., the granularity of pruning, affects the efﬁciency of hardware accelerator design as well as the prediction accuracy. Coarse-grained pruning brings more regular sparsity patterns, making it more amenable for hardware acceleration, but more challenging to maintain the same accuracy. In this paper we quantitatively measure the tradeoff between sparsity regularity and the prediction accuracy, providing insights in how to maintain the accuracy while having more structured sparsity pattern. Our experimental results show that coarse-grained pruning can achieve similar sparsity ratio as unstructured pruning given no loss of accuracy. Moreover, due to the index saving effect, coarse-grained pruning is able to obtain better compression ratio than ﬁne-grained sparsity at the same accuracy threshold. Based on the recent sparse convolutional neural network accelerator (SCNN), our experiments further demonstrate that coarse-grained sparsity saves ∼ 2× of the memory references compared with ﬁne-grained sparsity. Since memory reference is more than two orders of magnitude more expensive than arithmetic operations, the regularity of sparse structure leads to more efﬁcient hardware design.},
	language = {en},
	urldate = {2019-07-19},
	journal = {arXiv:1705.08922 [cs, stat]},
	author = {Mao, Huizi and Han, Song and Pool, Jeff and Li, Wenshuo and Liu, Xingyu and Wang, Yu and Dally, William J.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08922},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}
