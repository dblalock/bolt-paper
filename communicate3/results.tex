
To assess \ours's effectiveness, we implemented both it and existing algorithms in C++ and Python. All of our code and raw numerical results are publicly available at \texttt{https://smarturl.it/Maddness}. All experiments use a single thread on a Macbook Pro with a 2.6GHz Intel Core i7-4960HQ processor. Unless stated otherwise, all timing results use five trials, with each trial reporting the fastest among 20 executions. We use the best, rather than average, since this is standard practice in performance benchmarking and is robust to the purely additive noise introduced by competing CPU tasks. Standard deviations are shown for all curves as shaded areas, but are often too small to see. Since training can be performed offline and all methods except SparsePCA \cite{sparsePCA} train in at most a few minutes, we omit profiling of training times. We also do not profile the time to preprocess $\B$, since 1) this time is inconsequential in most cases, and 2) $\B$ is fixed and could be processed offline in all the problems we consider.

% In order to avoid implementation bias, we built upon the source code provided by \citep{bolt}\footnote{https://github.com/dblalock/bolt}, which has the most in common with our method and includes highly tuned implementations of many algorithms to which we compare.

% The metrics and baselines shown below are only a subset of those evaluated. In particular, we also measured squared errors, correlations, cosine similarities, etc, in addition to classification accuracies, but omit these since they are similar. See Appendix~\ref{sec:experimentDetails} for more results.

% Additional details about data cleaning, hyperparameter tuning, and other minutiae can be found in Appendix~\ref{sec:experimentDetails}. We do not need to tune any hyperparameters for our own method, but to ensure that other methods are not hindered by insufficient hyperparameter tuning, we allow them to tune their parameters on the test data and select the best results at a given speed \textit{post-hoc}.
We do not need to tune any hyperparameters for \ours, but we do take steps to ensure that other methods are not hindered by insufficient hyperparameter tuning. Concretely, we sweep a wide range of hyperparameter values and allow them to cherrypick their best hyperparameters on each test matrix.
Additional details about data cleaning, hyperparameter tuning, and other minutiae can be found in Section~\ref{sec:experimentDetails}.

% Also note that we compared to many methods not shown here, such as numerous randomized methods and variations of the Frequent Directions method \cite{liberty_simple_2012, ghashami_frequent_2016}. We report only those shown here since they performed the best.

Because nearly all existing work on approximate matrix multiplication either focuses on special cases that do not satisfy our problem definition \cite{quickerAdc, pq, opq} or synthetic matrices, there is not a clear set of benchmark matrix multiply tasks to use. We therefore use a collection of tasks that we believe are both reproducible and representative of many real-world matrices. To the best of our knowledge, our experiments use over an order of magnitude more matrices than any previous study.

% ------------------------------------------------
\vspace{-1mm}
\subsection{Methods Tested}
\vspace{-.5mm}
% ------------------------------------------------
% Recall that most baselines take the form of selecting a matrix $\V \in \R^{D \times d}, d < D$ such that $\A \B \approx (\A \V) (\V^\top \B)$. Here $d$ is a free parameter that adjusts the quality vs speed tradeoff. We therefore characterize these methods by how they set $\V$.
Recall that most baselines take the form of selecting a matrix $\V \in \R^{D \times d}, d < D$ such that $\A \B \approx (\A \V) (\V^\top \B)$. Here $d$ is a free parameter that adjusts the quality versus speed tradeoff. We therefore characterize most of these methods by how they set $\V$.
\vspace{-2.5mm}
% \begin{itemize}\itemsep-.5mm
\begin{itemize}\itemsep0mm
% \begin{itemize}[nosep]
    \item \textbf{PCA}. Set $\V$ equal to the top principal components of $\tilde{\A}$.
    \item \textbf{SparsePCA} \cite{sparsePCA}. Set $\V = \argmin_{\V} \min_{\mat{U}} \frac{1}{2 N_{train}} \norm{ \tilde{\A} - \mat{U}\mat{V}^\top }^2_F + \lambda \norm{\V}_1$, where $\mat{U}^\top \mat{U} = \mat{I}$. This is not the only dictionary learning formulation referred to as SparsePCA \cite{spcaSurvey1,spcaSurvey2}, but it is a good representative and is the only one with support in a major Python library.%\footnote{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html}.

    % Because SparsePCA requires the tuning of both $d$ and $\lambda$ for each matrix, we allowed it tune these parameters \textit{on the test set} to ensure that insufficient hyperparameter tuning did not hamper its performance. See Appendix~\ref{sec:experimentDetails} for more information.

    % First, for any given $d$ value and level of sparsity, we report
    % For each matrix product, we tried $\lambda \in 2^i, i \in \{-5, -4, -3, -2, -1, 0, 1, 2, 3\}$ for each matrix product. To ensure that our results err on the side of optimism (and also speed up our experiments), we report the best
    % selected the lambda \textit{post-hoc}---i.e., we cherrypicked the best results on the test set rather than cross-validating,
    \item \textbf{FastJL} \cite{fastjl}. $\V$ is set to Rademacher random variables composed with a Fast Hadamard Transform (FHT). For simplicity, we don't include the FHT in the timing.
    \item \textbf{HashJL} \cite{hashjl}. $\V$ is zero except for a $\pm 1$ in each row, with both sign and position chosen uniformly at random.
    % \item OSNAP \cite{osnap}. The OSNAP sketch is essentially a collection of $s$ HashJL sketches, each of dimensionality $d / s$. Following \cite{iSVD}, we set $s = 4$. We also tried other values of $s$ and found that other values did not perform significantly better (except $s=1$, which reduces to HashJL).
    % \item \textbf{RandGauss} \cite{lshOrig,E2LSH}. The entries of $\V$ are drawn i.i.d. from $\mathcal{N}(0, \frac{1}{D}\mat{I})$.
    \item \textbf{ScalarQuantize}. The matrices are not projected, but instead linearly quantized to eight bits such that the smallest and largest entries map to
    % the smallest and largest values expressible with eight bit integers.
    either 0 and 255 or -128 and 127, as appropriate.
    We use FBGEMM \cite{fbgemm} to perform the quantized matrix multiplies. We neglect the time required to convert from other formats to eight bits, reflecting the optimistic scenario in which the matrices are already of the appropriate types.
    % \item OrthoGauss \cite{superbitLSH}. The entries of $\V$ initialized as in RandGauss, and then $\V$ is set to the $\mat{Q}$ matrix in a QR decomposition of a RandGauss matrix.
    % \item Rademacher \cite{rademacherJL}. The entries of $\V$ are i.i.d. Rademacher random variables scaled by $\frac{1}{\sqrt{D}}$. TODO is that the right scale?
    \item \textbf{Bolt} \cite{bolt}. Bolt is an extension of PQ that uses quantized lookup tables and a reduced number of codebooks to obtain $10\times$ speedups over traditional PQ. It is the most similar method to our own, differing only in the encoding function, the use of averaging instead of upcasting, and the optimization of centroids. % It is not optimized for small $M$, however, effectively performing a preliminary matrix product with $M=16$ as a preprocessing step.
    % As discussed in Section~\ref{sec:relatedWork},
    \item \textbf{Exact Multiplication}. We simply compute the matrix product $\A\B$ using a modern BLAS implementation.
\end{itemize}
\vspace{-2mm}
% In addition to these baselines, we test two variations of our method:
We also test two variations of our method:
\vspace{-2.5mm}
% \begin{itemize}\itemsep-.5mm
\begin{itemize}\itemsep0mm
% \begin{itemize}[nosep]
    \item \textbf{\ours}. The algorithm described in Section~\ref{sec:method}.
    \item \textbf{\ours-PQ}. A handicapped version of \oursp without the prototype optimization step. The gap between \oursp and \ours-PQ is the gain from optimizing the prototypes.
\end{itemize}
\vspace{-2.5mm}
% compared to many methods not shown here, such as numerous randomized methods and variations of the Frequent Directions method \cite{liberty_simple_2012, ghashami_frequent_2016}. We report only those shown here since they performed the best.
We also compared to many additional methods (see Appendix~\ref{sec:experimentDetails}), but omit their results since they were not competitive with those listed here.

% ------------------------------------------------
\vspace{-1mm}
\subsection{How fast is \ours?}
\vspace{-.5mm}
% ------------------------------------------------

We begin by profiling the raw speed of our method. In Figure~\ref{fig:encodeSpeed}, we time the $g(\A)$ functions for various vector quantization methods. The $\A$ matrices have $2^{14}$ rows and varying numbers of columns $D$. Following \citet{bolt}, we also vary the number of codebooks $C$, profiling 8-, 16-, and 32-byte encodings. We measure in bytes rather than codebooks since PQ and OPQ use eight bits per codebook while Bolt and \oursp use four.
\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{amm/encode_speed}
\caption{\oursp encodes the $\A$ matrix orders of magnitude more quickly than existing vector quantization methods.}
\label{fig:encodeSpeed}
\end{center}
\end{figure}
\oursp is up to two orders of magnitude faster than existing methods, and its throughput increases with row length. This latter property is because its encoding cost per row is $O(C)$ rather than $O(D)$.
%Because its encoding cost is $O(C)$ rather than $O(D)$ \ours's throughput grows with row length.


% \oursp is up to two orders of magnitude faster than existing methods. \ours's throughput grows with row length because its encoding cost is $O(C)$ rather than $O(D)$. % \oursp is not only faster than existing methods, but faster than the machine's memory bandwidth of 15GB/s. This is possible because it only reads $O(C)$ columns, and $C$ can be lower than $D$.
% \vspace{-2mm}
\vspace{-1mm}
We also profile the speed of our aggregation function $f(\cdot, \cdot)$ using the same baselines as \citet{bolt}. As Figure~\ref{fig:scanSpeed} shows,
% our average-based aggregation is significantly faster than the upcasting-based method of Bolt, its nearest rival.
our average-based, matrix-aware aggregation is significantly faster than the upcasting-based (and one-vector-at-a-time) method of Bolt, its nearest rival.
\vspace{2mm}
%Though less dramatic a speedup than for the encoding function, we see that our average-based aggregation is significantly faster than the upcasting-based method of Bolt, its nearest rival.
\begin{figure}[h!]
\begin{center}
\includegraphics[width=\linewidth]{amm/scan_speed}
\caption{Given the preprocessed matrices, \oursp computes the approximate output twice as fast as the fastest existing method.}
\label{fig:scanSpeed}
\end{center}
\end{figure}

% ------------------------------------------------
\vspace{-1.5mm}
\subsection{Softmax Classifier}
\vspace{-.5mm}
% \vspace{-1mm}
% ------------------------------------------------

As described in Section~\ref{sec:intro}, we approximated linear classifiers on the widely used CIFAR-10 and CIFAR-100 datasets \cite{cifarDsets}. The classifiers use as input features the 512-dimensional activations of open-source, VGG-like neural networks trained on each dataset \cite{cifarVgg}. The matrices $\A$ are the $10000 \times 512$-dimensional floating point activations for the full test sets, and the matrices $\B$ are each network's final dense layer. The $50000 \times 512$-dimensional activations from the training set serve as the training matrices $\tilde{\A}$.
As shown in Figure~\ref{fig:cifar}, \oursp significantly outperforms all existing methods, achieving virtually the same accuracy as exact multiplication more than an order of magnitude faster.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{amm/cifar_Speedup_Accuracy}
\caption{\oursp achieves a far better speed-accuracy tradeoff than any existing method when approximating two softmax classifiers.}
\label{fig:cifar}
\end{center}
\end{figure}

Moreover, our method achieves this performance despite having worse support from the hardware. Specifically, it obtains speedups much smaller than the level of compression it provides. For example, the third points from the right correspond to speedups of roughly $100\times$. However, they are compressing each $512 \times 4\text{B} = 2048\text{B}$ row of the input down to a mere 4B, a savings of $512\times$ (sizes not shown in figure). In other words, if the hardware could lookup-accumulate as many bytes per cycle as it can multiply-accumulate, our method would be roughly four times as fast. Combined with the fact that multiplexers require many fewer transistors than multipliers, this suggests that a hardware implementation of our method could offer large speedups and power savings compared to existing accelerators.
% Similar results hold in other experiments throughout this section.

% ------------------------------------------------
\vspace{-2mm}
\subsection{Kernel-based classification}
\vspace{-.5mm}
% \vspace{-1mm}
% ------------------------------------------------

To assess the efficacy of our method on a larger and more diverse set of datasets than CIFAR-10 and CIFAR-100, we trained kernel classifiers on the datasets from the UCR Time Series Archive \cite{UCRArchive2018}. To enable meaningful speed comparison across datasets, we resampled the time series in all datasets to the median length and obtained the matrix $\B$ for each dataset by running Stochastic Neighbor Compression \cite{snc} on the training set with an RBF kernel of bandwidth one. We set the number of returned neighbors to 128 (results with 64 and 256 were similar). We approximated the Euclidean distances used by the kernel via the identity $\norm{\vec{x} - \vec{y}}_2^2 = \norm{\vec{x}}_2^2 - 2\vec{x}^\top \vec{y} + \norm{\vec{y}}_2^2$, which consists only of dot products.

This is not the state-of-the-art means of classifying time series, but it does yield fixed-sized matrices and is representative of several modern techniques for constructing highly efficient classifiers \cite{snc,dsnc,bnc,protonn}. This setup also complements the CIFAR results by being an extremely difficult task, since Stochastic Neighbor Compression has already optimized the classifiers to avoid redundancy.
%. This is because 1) Stochastic Neighbor Compression has already optimized the classifer to avoid redundancy, and 2) the distances between time series are often far smaller than their $L_2$ norms, allowing even small approximation errors to change predictions. % since the distances between time series are often far smaller than their euclidean lengths; this property means that even small amounts of approximation error are sufficient to change predictions. It is also difficult because the $\B$ matrix has been optimized to avoid redundancy, so any acceleration

As shown in Figure~\ref{fig:ucr}, \oursp is significantly faster at a given level of accuracy. A counter-intuitive result, however, is that optimization of the prototypes occasionally reduces accuracy---see the red line dipping below the blue one in the lowest subplot. Since the optimization strictly increases the expressive power, we believe that this is a product of overfitting and could be corrected were we to tune the ridge regression's parameter (instead of always using $\lambda = 1$).
% This subplot also shows that even \oursp cannot satisfy the most stringent accuracy requirements on many datasets, since the \oursp curves never approach 1.
This subplot also reveals that the most stringent degredation requirements can sometimes only be met using PCA at a low speedup (and not \ours), since this is the only curve approaching $1$.
% , at high accuracy preservation thresholds,  % However as indicated by \oursp never approaching a fraction of 1 on the bottom subplot, \oursp does not consistently preserve the full accuracy. This suggests that, for difficult classification problems in which almost no accuracy can be sacrificed, \outsp is often not the best choice. The only methods that reliably preserve nearly the full original accuracy are PCA with a small speedup and, with certain parameter settings, Bolt.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{amm/ucr2_Speedup_Relative Accuracy_rbf}
\caption{Fraction of UCR datasets for which each method preserves a given fraction of the original accuracy, versus the method's degree of speedup. \oursp enables much greater speedups for a given level of accuracy degredation.}
\label{fig:ucr}
\end{center}
\end{figure}

% ------------------------------------------------
% \vspace{-2mm}
\subsection{Image Filtering}
% \vspace{-.5mm}
% ------------------------------------------------

To test the extreme limits of \ours, we benchmarked the various techniques' ability to apply small filters to images. This task is extreme in that $D$ and $M$ are tiny, affording almost no opportunity to amortize preprocessing costs. As representative example filters, we chose $3 \times 3$ Sobel kernels and $5 \times 5$ Gaussian kernels. These are common high-pass and low-pass filters, respectively. We took the first 10 images from the first 50 classes of the Caltech101 dataset \cite{caltech} as a single training set, and the first 10 images from the remaining 51 classes as 510 test sets. We constructed the $\A$ matrices by extracting each patch of each image as one row. The $\B$ matrices have two columns, corresponding to one pair of Sobel or Gaussian filters (since using these filters in pairs is common). %This is not the most efficient way to perform image filtering since it does not exploit the structure of the convolution operation or the filters, but it does serve as a useful benchmark.

 %See Appendix~\ref{sec:experimentDetails} for additional details.

% We report the normalized mean-squared error (NMSE), defined as $\norm{\mat{\hat{C}}_{i,j} - \A\B}^2_F / \norm{\A\B}^2_F$, where $\hat{\mat{C}}$ is the method's estimate of $\A\B$. An NMSE of 0 is perfect and an NMSE of 1 corresponds to always predicting 0.
We report the normalized mean-squared error (NMSE), defined as
\begin{align}
    \norm{\mat{\hat{C}}_{i,j} - \A\B}^2_F / \norm{\A\B}^2_F
\end{align}
where $\hat{\mat{C}}$ is the method's estimate of $\A\B$. An NMSE of 0 is perfect and an NMSE of 1 corresponds to predicting all zeros.

In Figure~\ref{fig:caltech}, we see that it is only \oursp that offers any advantage over exact matrix products. This is likely because two columns afford almost no time to preprocess $\A$; indeed, rival vector quantization methods cannot logically do less work than brute force in this setting, and dense linear methods can only save work by embedding rows of $\A$ in one-dimensional space.

\oursp performs much worse on the high-pass filters (top) than the low-pass filters (bottom). This is likely because the former produce outputs with variance that is orders of magnitude lower than that of the original image; this means that the NMSE denominator, $\norm{\A\B}^2_F$ is tiny.  % relative to $\norm{\A}_F$ and $\norm{\B}_F$.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{amm/caltech_Speedup_1 - NMSE}
\caption{Despite there being only two columns in the matrix $\B$, \oursp still achieves a significant speedup with reasonable accuracy. Methods that are Pareto dominated by exact matrix multiplication on both tasks are not shown; this includes all methods but \oursp and SparsePCA.} % It is, however, much more effective for approximating a 5x5 low-pass filter (bottom) than a 3x3 high-pass filter (top).
%Only \oursp is not Pareto dominated by exact matrix multiplication (.}
\label{fig:caltech}
\end{center}
\end{figure}

% % ------------------------------------------------
% \subsection{Summary}
% % ------------------------------------------------

% Our method without \textit{any} hyperparameter tuning outperforms its nearest rival with all its hyperameters tuned \textit{on the test set}.
