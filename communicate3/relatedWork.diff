2c2
< Because our work draws on ideas from randomized algorithms, approximate matrix multiplication, vector quantization, and other fields, the body of work related to our own is vast. Here, we provide only a high-level overview, and refer the interested reader to \cite{learningToHashSurvey, hashingSimilaritySurvey, isvd} for more detailed surveys. We also defer discussion of related vector quantization methods to the following sections, since it is easier to appreciate how they differ from our own method once our method has been introduced.
---
> Because our work draws on ideas from randomized algorithms, approximate matrix multiplication, vector quantization, and other fields, the body of work related to our own is vast. Here, we provide only a high-level overview, and refer the interested reader to surveys of this area \cite{learningToHashSurvey, hashingSimilaritySurvey, isvd} for more information. We also defer discussion of related vector quantization methods to the following sections, since it is easier to appreciate how they differ from our own method once our method has been introduced.
17c17
< A weakness of matrix sketching methods in the context of matrix multiplication is that they consider each matrix in isolation. To exploit information about both matrices simultaneously, \citet{drineas_fast_2006} sample columns of $\A$ and rows of $\B$ according to a sampling distribution dependent upon both matrices. Later work by \citet{manne_fast_2014} reduces approximation of the matrices to an optimization problem, which is solved by steepest descent. \citet{mroueh_co-occuring_2016}, \citet{ye_frequent_2016}, and \citet{francis_improvement_2018} introduce variations of the Frequent Directions algorithm that take into account both matrices.
---
> A weakness of matrix sketching methods in the context of matrix multiplication is that they consider each matrix in isolation. To exploit information about both matrices simultaneously, Drineas et al. \cite{drineas_fast_2006} sample columns of $\A$ and rows of $\B$ according to a sampling distribution dependent upon both matrices. Later work by Manne et al. \cite{manne_fast_2014} reduce approximation of the matrices to an optimization problem, which is solved by steepest descent. More recently, several authors have introduced variations of the Frequent Directions algorithm that take into account both matrices \cite{mroueh_co-occuring_2016, ye_frequent_2016, francis_improvement_2018}.
24c24
< In the neural network acceleration literature, there have been several efforts to accelerate dense linear layers using some form of hashing \cite{springScalable,slide,wtaSoftmax,googleWtaCvpr,hashnet}. These methods differ from our own in the hash functions chosen, not exploiting a training set, and in the overall goal of the algorithm. While we seek to approximate the entire output matrix, these methods seek to either sample outputs \cite{springScalable,slide}, approximate only the largest outputs \cite{wtaSoftmax,googleWtaCvpr}, or implement a fixed, sparse linear operator \cite{hashnet}.
---
> In the neural network acceleration literature, there have been several efforts to accelerate dense linear layers using some form of hashing \cite{springScalable,slide,wtaSoftmax,googleWtaCvpr,hashnet}. These methods differ from our own in the hash functions chosen, in not exploiting a training set, and in the overall goal of the algorithm. While we seek to approximate the entire output matrix, these methods seek to either sample outputs \cite{springScalable,slide}, approximate only the largest outputs \cite{wtaSoftmax,googleWtaCvpr}, or implement a fixed, sparse linear operator \cite{hashnet}.
